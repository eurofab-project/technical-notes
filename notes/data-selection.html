<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krasen Samardzhiev">
<meta name="author" content="Barbara Metzler">
<meta name="author" content="Martin Fleischmann">
<meta name="author" content="Daniel Arribas-Bel">

<title>Reference Data Selection – Technical notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-4d1042b5f0b8d93047f4f930119f21bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive summary</a></li>
  <li><a href="#morphological-data" id="toc-morphological-data" class="nav-link" data-scroll-target="#morphological-data">Morphological data</a>
  <ul class="collapse">
  <li><a href="#ground-truth-classification" id="toc-ground-truth-classification" class="nav-link" data-scroll-target="#ground-truth-classification">Ground truth classification</a>
  <ul class="collapse">
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  <li><a href="#building-footprints" id="toc-building-footprints" class="nav-link" data-scroll-target="#building-footprints">Building footprints</a>
  <ul class="collapse">
  <li><a href="#available-sources" id="toc-available-sources" class="nav-link" data-scroll-target="#available-sources">Available sources</a></li>
  <li><a href="#microsoft-building-footprints" id="toc-microsoft-building-footprints" class="nav-link" data-scroll-target="#microsoft-building-footprints">Microsoft building footprints</a></li>
  </ul></li>
  <li><a href="#street-data" id="toc-street-data" class="nav-link" data-scroll-target="#street-data">Street data</a>
  <ul class="collapse">
  <li><a href="#overture-transportation-layer" id="toc-overture-transportation-layer" class="nav-link" data-scroll-target="#overture-transportation-layer">Overture transportation layer</a></li>
  </ul></li>
  <li><a href="#limitations-1" id="toc-limitations-1" class="nav-link" data-scroll-target="#limitations-1">Limitations</a></li>
  </ul></li>
  <li><a href="#ai-data" id="toc-ai-data" class="nav-link" data-scroll-target="#ai-data">AI data</a>
  <ul class="collapse">
  <li><a href="#satellite-imagery" id="toc-satellite-imagery" class="nav-link" data-scroll-target="#satellite-imagery">Satellite imagery</a></li>
  <li><a href="#urban-fabric-classes" id="toc-urban-fabric-classes" class="nav-link" data-scroll-target="#urban-fabric-classes">Urban fabric classes</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="data-selection.pdf"><i class="bi bi-file-pdf"></i>PDF (titlepage)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Reference Data Selection</h1>
<p class="subtitle lead">Technical Note D3</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Krasen Samardzhiev </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Faculty of Science, Charles University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Barbara Metzler </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            The Alan Turing Institute
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Martin Fleischmann <a href="mailto:martin.fleischmann@natur.cuni.cz" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Faculty of Science, Charles University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Daniel Arribas-Bel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            The Alan Turing Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="executive-summary" class="level1">
<h1>Executive summary</h1>
<p>This technical notes offers an overview of the data used within the EuroFab project and the reasoning behind its selection. The project requires data falling into two categories - morphological data and AI data.</p>
<p>The data in the morphological category consists of two layers - one, reflecting raw urban fabric in suboptimal quality, but available for large parts of the globe; and two, a layer capturing the target classification the models developed in the project should predict based on the suboptimal data input. Therefore, we rely on Microsoft Open Buildings project for building footprints and Overture Maps Transportation layer for street networks, capturing the raw morphological elements, and the upcoming morphotope-based hierarchical classification of urban fabric developed at the Charles University, reflecting the target for the prediction <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span>. Both of these layers cover Central Europe - Poland, Austria, Czechia, Slovakia and Germany.</p>
<p>The second category of data consists of openly available satellite imagery based on the GHS-composite-S2 R2020A dataset <span class="citation" data-cites="corbane2020global">(<a href="#ref-corbane2020global" role="doc-biblioref">Corbane et al. 2020</a>)</span> providing a cloud-free mosaic of visible RGB bands coming from the Sentinel 2 mission at 10m per pixel resolution. The satellite imagery will be used to predict another classification of urban fabric, paving the way for development of time-based classification in the future. The classification that is used as an input is using the open data product of British spatial signatures, covering the extent of Great Britain <span class="citation" data-cites="fleischmann2022geographical">(<a href="#ref-fleischmann2022geographical" role="doc-biblioref">Fleischmann and Arribas-Bel 2022</a>)</span>.</p>
<p>We use different input classifications for the morphological and the AI components of the project to not block the development of the AI model. This is due to the limited duration of the project, and since the the morphotope-based classification became available only in Q4 2024. The expectation is that the two classifications are conceptually close enough and the model architecture developed for signatures can be later used to build equally or more precise predictive model for the morphotope-based classification.</p>
</section>
<section id="morphological-data" class="level1">
<h1>Morphological data</h1>
<section id="ground-truth-classification" class="level2">
<h2 class="anchored" data-anchor-id="ground-truth-classification">Ground truth classification</h2>
<p>The main aim of this project is to create a predictive model that is capable of classifying urban fabric. To achieve this we need to use existing urban fabric classifications as ground truth data for model training. We have chosen the <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> classification to act as the ground truth, since it a classification that is detailed (using the resolution of individual buildings), consistent and scalable, while being fully data-driven, meaning there is no set of conceptual classes involved, providing closer representation of the nature of urban fabric in various diverse places around Central Europe. At the same time, it is based on the highest quality data available - building cadastre data from official sources. Since <span class="citation" data-cites="primus">Fleischmann and Samardzhiev (<a href="#ref-primus" role="doc-biblioref">Forthcoming</a>)</span> is upcoming work we describe its methodology and results in detail in this and in the Technical Note D2: Algorithm Design and Theoretical Basis Description.</p>
<p>The new classification <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> extends the work of <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span>, which has been used in numerous subsequent studies and official planning documents <span class="citation" data-cites="calafiore2023inequalities arribas2022spatial samardzhiev2022functional">(<a href="#ref-calafiore2023inequalities" role="doc-biblioref">Calafiore et al. 2023</a>; <a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>; <a href="#ref-samardzhiev2022functional" role="doc-biblioref">K. Samardzhiev et al. 2022</a>)</span> . It uses a combination of buildings and streets to generate Enclosed Tessellation Cells (ETCs), which are spatial units that are centred around buildings, and encapsulate the surrounding space that is nearest to them, rather than other buildings. ETCs boundaries are also limited by streets or a preset buffer distance from the centre of the building, which acts as the centre of the ETC. It is from this combination of buildings, streets and space, that the ETC can combine morphological characteristics of both streets, street intersections, buildings and blocks. The next step in <span class="citation" data-cites="primus">Fleischmann and Samardzhiev (<a href="#ref-primus" role="doc-biblioref">Forthcoming</a>)</span> is to measure different characteristics of the ETCs. A literature review carried out by <span class="citation" data-cites="fleischmann2021measuring">Fleischmann, Romice, and Porta (<a href="#ref-fleischmann2021measuring" role="doc-biblioref">2021</a>)</span> outlines a large number of possible variable that can be measured, of which <span class="citation" data-cites="fleischmann2022methodological">Fleischmann et al. (<a href="#ref-fleischmann2022methodological" role="doc-biblioref">2022</a>)</span> empirically suggests that there are around 74 unique ones that take into account the two dimensional nature of the data. <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> use the full array of characters since they don’t know a priori which will be the important variables. <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> uses a subset of 54 and the detailed list of characters is presented in the Algorithm design technical note, since both our method and <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> share the same characters. Similar to <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> the new clustering approach is split into two parts - defining fixed clusters and generating their taxonomy of based on their similarity, where the new approach uses regionalisation to capture the concept of a morphotope.</p>
<p>Conzen (1988), as reported in <span class="citation" data-cites="larkham1991glossary">Larkham and Jones (<a href="#ref-larkham1991glossary" role="doc-biblioref">1991</a>)</span>, defines morphotopes as the ‘smallest urban localities obtaining distinctive character among their neighbours from their particular combination of characteristics of plan type, building type and land use’.The classification follows this defining and operationalises a morphotope as a set of spatially contiguous ETCs with a minimum of 75 elements, where the internal distance calculated based on the morphological characters is on average lower within the set, than to other sets. The morphotopes for every region are calculated in isolation and in parallel. The definition of the subregions guarantees that all elements that are spatially contiguous, or have any spatial leakage effects, i.e.&nbsp;adjacency or being part of the same street segment, are processed together in the same region. Once the morphotopes are delineated, the mean of the characters of their constituent ETCs are assigned them. Therefore, every morphotope has the same dimensionality as the ETCs that form it.</p>
<p>The second step is the creation of a taxonomy of morphotopes for the whole of Central Europe. This is achieved through the application of non-spatially restrained ward clustering at two stages - one regional and one global. The resulting hierarchy can be cut at different levels depending on the application. For example at low values, there is some regional information still preserved in the hierarchy - morphotopes from one city are merged to identical morphotopes from the same city. Whereas at higher levels, different types of urban fabric, emerge across countries.</p>
<p>The figure below shows a specific cutoff value, which separates different types of houses; from heterogenous historical urbanised areas; from recent modern urban developments such as apartment blocks and commercial areas; from large industrial areas.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/prague_600.png" class="nostretch figure-img" height="400"></p>
<figcaption>High-level urban fabrics</figcaption>
</figure>
</div>
<p>More detailed information about the methodology is available in the Technical Note D2, as we aim to follow it as closely as possible in this project.</p>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>Even though all the data comes from official cadastre sources there is regional heterogeneity.</p>
<p>One limitation of the classification’s underlying data is that building unit boundaries are not recorded in a consistent manner. For example, a modernist housing block is sometimes represented as a single polygon, other times as multiple adjacent polygons, each of which representing a physical entrance to the unified building. This discrepancy adverse affects the calculation of some morphotopes related to adjacency, shape and size. In many cases this means that modernist housing estates are split based on these polygon boundaries delineations. In other cases, some modernist housing estates are treated by the algorithm as industrial areas, due to the sheer size of its units and the layout of the street network. To account for this, two preprocessing steps are carried out in the classification. First, auxiliary data is used in Czechia to combine all adjacent units into single, unified polygons for consistency. Second, an extra variable - indicating elongation and a low area to perimeter ratio is added to the hierarchy calculations. The motivation behind this variable is that non-industrial buildings are typically thin and elongated, laid out in complex patterns to maximise sunlight exposure. No extra information was used to calculate this indicator, only the morphometric characteristics already available.</p>
<p>Another limitation is that the definition of buildings is different in different regions in Central Europe. Hamburg and Berlin include train and tram lines, parking lots and parts of motorways as buildings in their official cadastre data. To limit the adverse effects of these additions, we drop any buildings that intersect overture streets in these two regions.</p>
</section>
</section>
<section id="building-footprints" class="level2">
<h2 class="anchored" data-anchor-id="building-footprints">Building footprints</h2>
<p>Building footprints are one of the two core elements needed to generate morphometric classifications. Measurements of aspects of buildings provide the bulk of the information about the morphological character of a place. Many characters in the literature <span class="citation" data-cites="fleischmann2021measuring">(<a href="#ref-fleischmann2021measuring" role="doc-biblioref">Fleischmann, Romice, and Porta 2021</a>)</span>, for example the ratio of perimeter shared with other connected buildings, require very high quality building footprints to accurately reflect the reality of the underlying topology. There are numerous available source of building footprints - official cadastre polygons; volunteer made OpenStreetMap data; Satellite-derived building footprints; or combinations of all of these such as Overture Maps or EUBUCCO. Each of these data sources come with their own drawbacks and advantages.</p>
<section id="available-sources" class="level3">
<h3 class="anchored" data-anchor-id="available-sources">Available sources</h3>
<p>The highest quality available data is from official cadastre building footprints. These provide the ground truth information for official planning and administrative applications in many countries. However, the data is not always readily available and even when it is, it may require a high degree of preprocessing. For example, there is no readily available cadastre data to download for EU countries such as Hungary and Bulgaria. Even though data for Germany is available, it is split across 16 states, each of which has its own interface and data format. Furthermore, the data is not always comparable between countries reflective of the physical reality - for example in Czechia the official cadastre reflects ownership situation in some modernist apartment blocks, rather than the physical structure of the buildings, leading to the morphological inconsistency within the same data source, not mentioning the issue across the sources.</p>
<p>Another common source is OpenStreetMap (OSM) map, which is free and open geographical database generated by volunteers. In many cases the cadastre data is processed and put into OpenStreetMap, alongside volunteer contributions where official data is missing. The issues with OSM data are similar to cadastre data where the cadastre data is used as the basis and is even harder to process since the schema used for buildings is more generic. Furthermore, the quality is unequal - for example the data in Germany is very high quality, but in not in other countries like Italy or Bulgaria. The similar heterogeneity can be observed not only between countries but often within individual cities, with some districts being of a very high quality, others being more generalised and other completely missing.</p>
<p>Another available data source is satellite-derived building footprints, which are becoming more widely adopted due to advances in satellite technology and computer vision algorithms. They have successfully been used in applications for city boundary delineation, desegregating population and others <span class="citation" data-cites="samardzhiev2023characterising">(<a href="#ref-samardzhiev2023characterising" role="doc-biblioref">K. P. Samardzhiev 2023</a>)</span>. There are multiple sources of global building footprints available - Google building footprints, Microsoft building footprints, ESRI community data, Overture maps, as well as other proprietary data. The advantage of satellite-derived building footprints data is that it is all derived in a consistent manner and covers large geographical areas. However, the quality is generally worse than other sources and the geometric topology capturing adjacent buildings is typically broken.</p>
<p>The newest available data, such as EUBUCCO <span class="citation" data-cites="milojevic-dupont2023EUBUCCO">(<a href="#ref-milojevic-dupont2023EUBUCCO" role="doc-biblioref">Milojevic-Dupont et al. 2023</a>)</span> or Overture Maps combine different types of other sources. Overture is based off of OSM, but adds a hierarchical structure of sources on top of it. The buildings from OSM are considered the ground truth and buildings from additional sources such as satellite data are added where there is no OSM data. This means that in general it has the highest number of available buildings. However, it comes with the same issues that OpenStreetMap data has, and it even further exasperates the data heterogeneity issues - in different places different data sources with different characteristics are merged. Thus, working with the data requires more preprocessing than other resources.</p>
<p>Similarly, the EUBUCCO is another data source that combines multiple datasets - official cadastre data and OpenStreetMap. Therefore it has the same advantages and disadvantages of its core sources. Furthermore, it has missing data issues that stem from the specific fusion approach of these sources. For example, there is a whole missing neighbourhood in central Prague and, in general, it has less buildings than the official cadastre data for the Central European study area.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/bubenec.png" class="nostretch figure-img" height="400"></p>
<figcaption>Missing data in central Prague</figcaption>
</figure>
</div>
</section>
<section id="microsoft-building-footprints" class="level3">
<h3 class="anchored" data-anchor-id="microsoft-building-footprints">Microsoft building footprints</h3>
<p>In this project, we use the Microsoft building footprints since they fully cover our study area - Poland, Czechia, Germany, Austria, Slovakia (i.e Central Europe). The dataset also has good worldwide coverage - they contain around 1.5 billion building polygons - and the footprints are derived in a consistent way, which ensures scalability of our method to different countries. It should be noted that our approach can also work for other consistently derived building polygons, Google Footprints for example, so long as all the data is generated from the same source.</p>
<p>However, the data does not come without issues. For example, in dense urban centres entire blocks can be delineated as individual buildings. Given that morphology calculations rely on precise local topological relations between neighbours, such as two buildings touching, this problem renders a whole number of possible measurements described in <span class="citation" data-cites="fleischmann2021measuring">(<a href="#ref-fleischmann2021measuring" role="doc-biblioref">Fleischmann, Romice, and Porta 2021</a>)</span> meaningless. Furthermore, this issue affects even simpler calculations such as counting the number of buildings within a radius or topological neighbourhood. Other issues are that computer vision techniques sometimes miss entire buildings or misidentify building boundaries. Therefore, any approach that uses satellite-derived building footprints should be able to account for these three and potentially other problems.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/building_comparison.png" class="nostretch figure-img" height="400"></p>
<figcaption>Comparison between MS buildings and cadastre level buildings in central Prague</figcaption>
</figure>
</div>
</section>
</section>
<section id="street-data" class="level2">
<h2 class="anchored" data-anchor-id="street-data">Street data</h2>
<p>Street network data is the other major data source used in numerical morphometric analysis. It is used to identify spatial relationships, delineate boundaries for other elements and calculate morphological features of the streets themselves, such as length or width. There are numerous sources for street network data, however the major one is OpenStreetMap and its derivate - Overture Maps.</p>
<section id="overture-transportation-layer" class="level3">
<h3 class="anchored" data-anchor-id="overture-transportation-layer">Overture transportation layer</h3>
<p>The street network used in this study is a direct download from Overture maps, a processed subset of data from OpenStreetMap with some additions. OpenStreetMap has global coverage and high quality data, especially for vehicular roads, on actual physical layouts of the roads, as well as additional information on street type. We use Overture directly due to the fact that it is more accessible and easier to process, however the underlying geometry and information of the two data sources is identical.</p>
<p>The types of street segments used in the analysis are limited to the ones listed in Table 1.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><em>Types of streets</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>living_street</td>
</tr>
<tr class="even">
<td>motorway</td>
</tr>
<tr class="odd">
<td>motorway_link</td>
</tr>
<tr class="even">
<td>pedestrian</td>
</tr>
<tr class="odd">
<td>primary</td>
</tr>
<tr class="even">
<td>primary_link</td>
</tr>
<tr class="odd">
<td>residential</td>
</tr>
<tr class="even">
<td>secondary</td>
</tr>
<tr class="odd">
<td>secondary_link</td>
</tr>
<tr class="even">
<td>tertiary</td>
</tr>
<tr class="odd">
<td>tertiary_link</td>
</tr>
<tr class="even">
<td>trunk</td>
</tr>
<tr class="odd">
<td>trunk_link</td>
</tr>
<tr class="even">
<td>unclassified</td>
</tr>
</tbody>
</table>
<p>Another type of segment that is filtered out are tunnels , since the analysis strictly focuses on two dimensions and therefore undergrounds structures adversely affect the calculation of boundaries and characters.</p>
</section>
</section>
<section id="limitations-1" class="level2">
<h2 class="anchored" data-anchor-id="limitations-1">Limitations</h2>
<p>There are two limitations, related to the alignment of the satellite-derived and cadastral building footprints - one temporal and one spatial. First, most cadastral data comes from a single year, whereas the satellite-derived building footprints come from images from multiple years. Second, the cadastral data has a larger spatial coverage and therefore there are areas with cadastral data, which as missing in the satellite-derived data. The second issue has a limited impact on the model training, since areas present in the cadastral data but not in satellite-derived polygons are ignored due to the spatial inner join. Nevertheless, both of these problems affect model evaluation and are promising areas of future research.</p>
</section>
</section>
<section id="ai-data" class="level1">
<h1>AI data</h1>
<section id="satellite-imagery" class="level2">
<h2 class="anchored" data-anchor-id="satellite-imagery">Satellite imagery</h2>
<p>We use satellite images as input into the AI model to predict urban fabric. The satellite image data is sourced from the GHS-composite-S2 R2020A dataset <span class="citation" data-cites="corbane2020global">(<a href="#ref-corbane2020global" role="doc-biblioref">Corbane et al. 2020</a>)</span>. This dataset is a global, cloud-free composite derived from Sentinel-2 L1C data, covering the period from January 2017 to December 2018. The composite images have a resolution of 10 meters per pixel and include three visible bands (red, green, and blue), which are well-suited for urban analysis.</p>
<p>Sentinel-2 data is particularly suitable for this application due to its consistent and openly available global coverage, which is ideal for projects requiring cross-regional comparisons. Its temporal flexibility, enabled by composites from multiple time points, allows for the analysis of urban patterns over time. With a resolution of 10 meters per pixel, Sentinel-2 provides the highest level of detail among openly available datasets, making it sufficient to distinguish urban features at the neighborhood scale. Additionally, its compatibility with many geospatial AI models pre-trained on Sentinel-2 data supports the use of existing methods and facilitates transfer learning.</p>
</section>
<section id="urban-fabric-classes" class="level2">
<h2 class="anchored" data-anchor-id="urban-fabric-classes">Urban fabric classes</h2>
<p>The urban fabric predictions from our AI model are based on labels from the Spatial Signatures Framework <span class="citation" data-cites="arribas2022spatial fleischmann2022geographical">(<a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>; <a href="#ref-fleischmann2022geographical" role="doc-biblioref">Fleischmann and Arribas-Bel 2022</a>)</span>, which provides a typology of British urban environments characterized by both form (physical structure) and function (usage). This framework captures the complexity of urban areas, offering insights into how different spaces look and operate. While our primary focus is on urban fabric classification centered on form—an approach that may be simpler since form is visible in imagery—we currently use the Spatial Signatures Framework as a proxy due to its conceptual alignment with our goals for urban characterization. This decision is also influenced by the limited duration of the project and the fact that the morphotope-based classification only became available in Q4 2024. To ensure progress in developing the AI model, we use separate input classifications for the morphological and AI components of the project. The expectation is that the two classifications are conceptually similar, allowing the model architecture developed for Spatial Signatures to later support building an equally or more precise predictive model for the morphotope-based classification.</p>
</section>
</section>
<section id="references" class="level1 unnumbered">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-arribas2022spatial" class="csl-entry" role="listitem">
Arribas-Bel, Daniel, and Martin Fleischmann. 2022. <span>“Spatial Signatures-Understanding (Urban) Spaces Through Form and Function.”</span> <em>Habitat International</em> 128: 102641.
</div>
<div id="ref-calafiore2023inequalities" class="csl-entry" role="listitem">
Calafiore, Alessia, Krasen Samardzhiev, Francisco Rowe, Martin Fleischmann, and Daniel Arribas-Bel. 2023. <span>“Inequalities in Experiencing Urban Functions. An Exploration of Human Digital (Geo-) Footprints.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em>, 23998083231208507.
</div>
<div id="ref-corbane2020global" class="csl-entry" role="listitem">
Corbane, C., P. Politis, P. Kempeneers, D. Simonetti, P. Soille, A. Burger, M. Pesaresi, F. Sabo, V. Syrris, and T. Kemper. 2020. <span>“A Global Cloud Free Pixel- Based Image Composite from <span>Sentinel</span>-2 Data.”</span> <em>Data in Brief</em> 31 (August): 105737. <a href="https://doi.org/10.1016/j.dib.2020.105737">https://doi.org/10.1016/j.dib.2020.105737</a>.
</div>
<div id="ref-fleischmann2022geographical" class="csl-entry" role="listitem">
Fleischmann, Martin, and Daniel Arribas-Bel. 2022. <span>“Geographical Characterisation of British Urban Form and Function Using the Spatial Signatures Framework.”</span> <em>Scientific Data</em> 9 (1): 546.
</div>
<div id="ref-fleischmann2022methodological" class="csl-entry" role="listitem">
Fleischmann, Martin, Alessandra Feliciotti, Ombretta Romice, and Sergio Porta. 2022. <span>“Methodological Foundation of a Numerical Taxonomy of Urban Form.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 49 (4): 1283–99.
</div>
<div id="ref-fleischmann2021measuring" class="csl-entry" role="listitem">
Fleischmann, Martin, Ombretta Romice, and Sergio Porta. 2021. <span>“Measuring Urban Form: Overcoming Terminological Inconsistencies for a Quantitative and Comprehensive Morphologic Analysis of Cities.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 48 (8): 2133–50.
</div>
<div id="ref-primus" class="csl-entry" role="listitem">
Fleischmann, Martin, and Krasen Samardzhiev. Forthcoming. <span>“Numerical Taxonomy of Urban Fabric in Central Europe,”</span> Forthcoming.
</div>
<div id="ref-larkham1991glossary" class="csl-entry" role="listitem">
Larkham, Peter J, and Andrew N Jones. 1991. <em>A Glossary of Urban Form</em>. Urban Morphology Research Group, School of Geography, University of Birmingham.
</div>
<div id="ref-milojevic-dupont2023EUBUCCO" class="csl-entry" role="listitem">
Milojevic-Dupont, Nikola, Felix Wagner, Florian Nachtigall, Jiawei Hu, Geza Boi Brüser, Marius Zumwald, Filip Biljecki, et al. 2023. <span>“<span>EUBUCCO</span> V0.1: <span>European</span> Building Stock Characteristics in a Common and Open Database for 200+ Million Individual Buildings.”</span> <em>Scientific Data</em> 10 (1): 147. <a href="https://doi.org/10.1038/s41597-023-02040-2">https://doi.org/10.1038/s41597-023-02040-2</a>.
</div>
<div id="ref-samardzhiev2023characterising" class="csl-entry" role="listitem">
Samardzhiev, Krasen Petrov. 2023. <span>“Characterising Urban Processes Using New Forms of Data and Analysis.”</span> PhD thesis, University of Liverpool United Kingdom.
</div>
<div id="ref-samardzhiev2022functional" class="csl-entry" role="listitem">
Samardzhiev, Krasen, Martin Fleischmann, Daniel Arribas-Bel, Alessia Calafiore, and Francisco Rowe. 2022. <span>“Functional Signatures in Great Britain: A Dataset.”</span> <em>Data in Brief</em> 43: 108335.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>