<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ondřej Maxian, Kamil Kovář, Martin Metzler, Martin Fleischmann, Dani Arribas-Bel">

<title>AI-Based Urban Fabric Classification using Satellite Imagery – Technical notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ad12125f43b1cd0365d2f9404fbc2628.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#ai-based-urban-fabric-classification-using-satellite-imagery" id="toc-ai-based-urban-fabric-classification-using-satellite-imagery" class="nav-link active" data-scroll-target="#ai-based-urban-fabric-classification-using-satellite-imagery">AI-Based Urban Fabric Classification using Satellite Imagery</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a></li>
  <li><a href="#data-and-methods" id="toc-data-and-methods" class="nav-link" data-scroll-target="#data-and-methods">Data and Methods</a>
  <ul class="collapse">
  <li><a href="#satellite-imagery" id="toc-satellite-imagery" class="nav-link" data-scroll-target="#satellite-imagery">Satellite Imagery</a></li>
  <li><a href="#urban-fabric-classes" id="toc-urban-fabric-classes" class="nav-link" data-scroll-target="#urban-fabric-classes">Urban Fabric Classes</a></li>
  </ul></li>
  <li><a href="#model-design" id="toc-model-design" class="nav-link" data-scroll-target="#model-design">Model Design</a>
  <ul class="collapse">
  <li><a href="#classification-vs.-segmentation" id="toc-classification-vs.-segmentation" class="nav-link" data-scroll-target="#classification-vs.-segmentation">Classification vs.&nbsp;Segmentation</a></li>
  <li><a href="#scale" id="toc-scale" class="nav-link" data-scroll-target="#scale">Scale</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a></li>
  </ul></li>
  <li><a href="#experimental-approaches" id="toc-experimental-approaches" class="nav-link" data-scroll-target="#experimental-approaches">Experimental Approaches</a></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation">Model Evaluation</a></li>
  <li><a href="#ai-model-results" id="toc-ai-model-results" class="nav-link" data-scroll-target="#ai-model-results">AI Model Results</a></li>
  <li><a href="#final-model" id="toc-final-model" class="nav-link" data-scroll-target="#final-model">Final Model</a>
  <ul class="collapse">
  <li><a href="#addressing-class-imbalance" id="toc-addressing-class-imbalance" class="nav-link" data-scroll-target="#addressing-class-imbalance">Addressing Class Imbalance</a></li>
  <li><a href="#classification-schemes" id="toc-classification-schemes" class="nav-link" data-scroll-target="#classification-schemes">Classification Schemes</a></li>
  <li><a href="#model-performance" id="toc-model-performance" class="nav-link" data-scroll-target="#model-performance">Model Performance</a></li>
  </ul></li>
  <li><a href="#temporal-analysis" id="toc-temporal-analysis" class="nav-link" data-scroll-target="#temporal-analysis">Temporal Analysis</a>
  <ul class="collapse">
  <li><a href="#diversity-analysis-shannon-index" id="toc-diversity-analysis-shannon-index" class="nav-link" data-scroll-target="#diversity-analysis-shannon-index">Diversity Analysis (Shannon Index)</a></li>
  <li><a href="#spatial-patterns-of-change" id="toc-spatial-patterns-of-change" class="nav-link" data-scroll-target="#spatial-patterns-of-change">Spatial Patterns of Change</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link" data-scroll-target="#key-findings">Key Findings</a></li>
  <li><a href="#lessons-learned" id="toc-lessons-learned" class="nav-link" data-scroll-target="#lessons-learned">Lessons Learned</a></li>
  <li><a href="#potential-research-directions" id="toc-potential-research-directions" class="nav-link" data-scroll-target="#potential-research-directions">Potential Research Directions</a></li>
  </ul></li>
  <li><a href="#software-and-example-datasets" id="toc-software-and-example-datasets" class="nav-link" data-scroll-target="#software-and-example-datasets">Software and Example Datasets</a>
  <ul class="collapse">
  <li><a href="#software-ai-method-for-urban-fabric-classification-and-morphometric-characterization" id="toc-software-ai-method-for-urban-fabric-classification-and-morphometric-characterization" class="nav-link" data-scroll-target="#software-ai-method-for-urban-fabric-classification-and-morphometric-characterization">Software: AI Method for Urban Fabric Classification and Morphometric Characterization</a></li>
  <li><a href="#example-datasets-generated-during-verification-exercises" id="toc-example-datasets-generated-during-verification-exercises" class="nav-link" data-scroll-target="#example-datasets-generated-during-verification-exercises">Example Datasets Generated During Verification Exercises</a></li>
  <li><a href="#london" id="toc-london" class="nav-link" data-scroll-target="#london">London</a></li>
  <li><a href="#liverpool" id="toc-liverpool" class="nav-link" data-scroll-target="#liverpool">Liverpool</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="final_report_turing.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AI-Based Urban Fabric Classification using Satellite Imagery</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Ondřej Maxian, Kamil Kovář, Martin Metzler, Martin Fleischmann, Dani Arribas-Bel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Charles University; The Alan Turing Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="ai-based-urban-fabric-classification-using-satellite-imagery" class="level1">
<h1>AI-Based Urban Fabric Classification using Satellite Imagery</h1>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>Urban areas represent complex and dynamic environments, making traditional analysis challenging due to their diverse characteristics. Remote sensing technologies, particularly satellite imagery, provide powerful tools for understanding and characterizing urban areas at scale. As part of the EuroFab project, we employed AI modeling techniques using Sentinel-2 satellite imagery to classify and analyze urban fabric across Great Britain. The aim was to generate detailed insights into urban spatial structures and track temporal changes spanning from 2016 to 2021.</p>
</section>
<section id="data-and-methods" class="level2">
<h2 class="anchored" data-anchor-id="data-and-methods">Data and Methods</h2>
<section id="satellite-imagery" class="level3">
<h3 class="anchored" data-anchor-id="satellite-imagery">Satellite Imagery</h3>
<p>We utilized two different sets of satellite images derived from Sentinel-2 imagery:</p>
<ol type="1">
<li><p><strong>GHS-composite-S2 R2020A dataset</strong> <span class="citation" data-cites="corbane2020global">(<a href="#ref-corbane2020global" role="doc-biblioref">Corbane et al. 2020</a>)</span>: Covering January 2017 to December 2018, this dataset offers cloud-free, consistent, high-quality RGB (red, green, blue band) composite imagery at a resolution of 10 meters per pixel. We used these images of Great Britain (GB) to train our prediction model.</p></li>
<li><p><strong>Annual Sentinel-2 composites</strong>: Created for each year from 2016 to 2021, these composites include only cloud-free RGB images from their specific year. We acquired these using the Google Earth Engine API through automated Python scripts (as described in <a href="https://github.com/urbangrammarai/gee_pipeline">GEE pipeline</a>).</p></li>
</ol>
<p>Sentinel-2 data is particularly suitable for our application due to several advantages:</p>
<ul>
<li>Consistent and openly available global coverage, ideal for cross-regional comparisons</li>
<li>Temporal flexibility through composites from multiple time points, allowing analysis of urban patterns over time</li>
<li>Resolution of 10 meters per pixel, providing sufficient detail to distinguish urban features at the neighborhood scale</li>
<li>Compatibility with many pre-trained geospatial AI models, facilitating transfer learning</li>
</ul>
</section>
<section id="urban-fabric-classes" class="level3">
<h3 class="anchored" data-anchor-id="urban-fabric-classes">Urban Fabric Classes</h3>
<p>The urban fabric predictions from our AI model are based on labels from the Spatial Signatures Framework <span class="citation" data-cites="arribas2022spatial fleischmann2022geographical">(<a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>; <a href="#ref-fleischmann2022geographical" role="doc-biblioref">Fleischmann and Arribas-Bel 2022</a>)</span>, which provides a typology of British urban environments characterized by both form (physical structure) and function (usage). This framework captures the complexity of urban areas, offering insights into how different spaces look and operate.</p>
<p>While our primary focus is on urban fabric classification centered on form—an approach that may be simpler since form is visible in imagery—we currently use the Spatial Signatures Framework as a proxy due to its conceptual alignment with our goals for urban characterization. This decision was influenced by the limited project duration and the fact that the morphotope-based classification only became available in Q4 2024.</p>
<p>To ensure progress in developing the AI model, we use separate input classifications for the morphological and AI components of the project. The expectation is that the two classifications are conceptually similar, allowing the model architecture developed for Spatial Signatures to later support building an equally or more precise predictive model for the morphotope-based classification.</p>
</section>
</section>
<section id="model-design" class="level2">
<h2 class="anchored" data-anchor-id="model-design">Model Design</h2>
<section id="classification-vs.-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="classification-vs.-segmentation">Classification vs.&nbsp;Segmentation</h3>
<p>In satellite image analysis, classification and segmentation address spatial labeling at different levels of granularity:</p>
<ul>
<li><strong>Classification</strong>: Assigns a single urban fabric type to an image tile, offering generalized insights into dominant urban characteristics without the need for pixel-perfect alignment.</li>
<li><strong>Segmentation</strong>: Assigns urban fabric types at a pixel level, providing detailed boundary-specific mapping but facing challenges due to ambiguous class boundaries in urban environments.</li>
</ul>
<p>In our study, the label dataset does not always correspond directly to identifiable features in the imagery, making classification a potentially more suitable approach as it generalizes each tile’s dominant land cover type without requiring exact pixel alignment. However, we explored both approaches to evaluate how each method performs given the scale and nature of the dataset.</p>
</section>
<section id="scale" class="level3">
<h3 class="anchored" data-anchor-id="scale">Scale</h3>
<p>We tested multiple scales for our analysis: - Segmentation at the pixel level (preprocessed into 224×224 pixel tiles for compatibility with downstream models) - Classification at scales of 56×56 pixels (560×560 meters on the ground) and 25×25 pixels (250×250 meters on the ground)</p>
</section>
<section id="sampling" class="level3">
<h3 class="anchored" data-anchor-id="sampling">Sampling</h3>
<p>We evaluated two sampling approaches to assess their impact on spatial generalization and F1-score performance:</p>
<ol type="1">
<li><strong>Random sampling</strong>: Ensures diversity and captures localized patterns, but risks spatial leakage, potentially inflating performance metrics.</li>
<li><strong>H3 resolution 3 regional sampling</strong>: Reduces spatial leakage and offers a more realistic evaluation of generalization but can suffer from unfair penalization due to the heterogeneity of regions.</li>
</ol>
</section>
</section>
<section id="experimental-approaches" class="level2">
<h2 class="anchored" data-anchor-id="experimental-approaches">Experimental Approaches</h2>
<p>We structured experiments around three distinct modeling approaches:</p>
<ul>
<li><strong>Approach A</strong>: Baseline model employing SatlasPretrain embeddings coupled with an XGBoost classifier.</li>
<li><strong>Approach B</strong>: Fine-tuned three geospatial foundation models (Satlas, Clay, Prithvi) specifically for segmentation tasks.</li>
<li><strong>Approach C</strong>: Fine-tuned the Clay model for classification tasks based on its superior segmentation performance.</li>
</ul>
</section>
<section id="model-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation">Model Evaluation</h2>
<p>Multiple metrics were utilized to comprehensively evaluate model performance:</p>
<ul>
<li><strong>Weighted Accuracy (Macro)</strong>: Reflects overall predictive accuracy adjusted for class frequency.</li>
<li><strong>Weighted F1 Score</strong>: Balances precision and recall, particularly crucial for handling imbalanced datasets.</li>
</ul>
</section>
<section id="ai-model-results" class="level2">
<h2 class="anchored" data-anchor-id="ai-model-results">AI Model Results</h2>
<p>Our experimental results demonstrated that classification approaches outperformed segmentation methods. This was not only in terms of accuracy but also in the shapes that were predicted. While the 56×56 pixel scale (560×560m) yielded marginally better statistical performance, we determined that the 25×25 pixel scale (250×250m) offered a more appropriate resolution for policy applications and urban planning contexts. A key finding was that incorporating regional context through H3 spatial indexing significantly enhanced predictive performance across all configurations. Models combining embeddings with H3 contextual information provided the most balanced performance, substantially outperforming other approaches on all metrics. Notably, when comparing sampling strategies, we observed that H3 regional sampling showed slightly lower performance metrics than random sampling, suggesting potential spatial leakage in the random sampling approach that may artificially inflate performance metrics.</p>
<p>The results in the table below represent either pixel-level values or tile level values in brackets.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 6%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Approach</strong></th>
<th><strong>Scale</strong></th>
<th><strong>Sampling</strong></th>
<th><strong>Clas/seg</strong></th>
<th><strong>Regional info</strong></th>
<th><strong>Global Acc</strong></th>
<th><strong>Macro Acc</strong></th>
<th><strong>F1 (macro)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>C (Clay)</td>
<td>1x1</td>
<td>random</td>
<td>segmentation</td>
<td></td>
<td><strong>0.73</strong></td>
<td><strong>0.31</strong></td>
<td><strong>0.30 (0.58)</strong></td>
</tr>
<tr class="even">
<td>C (Satlas)</td>
<td>1x1</td>
<td>random</td>
<td>segmentation</td>
<td></td>
<td>0.57</td>
<td></td>
<td>(0.41)</td>
</tr>
<tr class="odd">
<td>C (Prithvi)</td>
<td>1x1</td>
<td>random</td>
<td>segmentation</td>
<td></td>
<td>0.62</td>
<td></td>
<td>(0.42)</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>random</td>
<td>classification</td>
<td></td>
<td>(0.73)</td>
<td>(0.31)</td>
<td>(0.30)</td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>25x25</td>
<td>random</td>
<td>classification</td>
<td></td>
<td><strong>(0.81)</strong></td>
<td><strong>(0.46)</strong></td>
<td><strong>(0.53)</strong></td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>random</td>
<td>classification</td>
<td>lat/lon</td>
<td>(0.89)</td>
<td>(0.71)</td>
<td>(0.78)</td>
</tr>
<tr class="odd">
<td></td>
<td>25x25</td>
<td>random</td>
<td></td>
<td>lat/lon</td>
<td>(0.91)</td>
<td>(0.78)</td>
<td>(0.83)</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>H3 res 3 (55,743)</td>
<td>classification</td>
<td>H3 res 5 (lat/lon)</td>
<td>(0.58)</td>
<td>(0.15)</td>
<td>(0.15)</td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>25x25</td>
<td>H3 res 5 (2,125)</td>
<td>classification</td>
<td>H3 res 5 (lat/lon)</td>
<td>(0.65)</td>
<td>(0.20)</td>
<td>(0.21)</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>H3 res 6 (335)</td>
<td>classification</td>
<td>H3 res 5 (lat/lon)</td>
<td><strong>(0.72)</strong></td>
<td><strong>(0.29)</strong></td>
<td><strong>(0.32)</strong></td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td></td>
<td>0.76 (0.66)</td>
<td>0.22 (0.13)</td>
<td>0.23</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td>H3 res 5 (cat)</td>
<td><strong>0.87 (0.82)</strong></td>
<td><strong>0.42 (0.35)</strong></td>
<td><strong>0.45</strong></td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td>H3 res 5 (lat/lon)</td>
<td><strong>0.87 (0.81)</strong></td>
<td>0.39 (0.31)</td>
<td>0.42</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>regression (ordinal)</td>
<td>H3 res 5 (lat/lon)</td>
<td>0.80 (0.80)</td>
<td>0.26 (0.26)</td>
<td>0.26</td>
</tr>
<tr class="odd">
<td>B (Clay)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td></td>
<td>0.59 (0.68)</td>
<td>0.09</td>
<td>0.12</td>
</tr>
</tbody>
</table>
<p>Based on these insights, our final model implements a Satlas embedding architecture operating at the 25×25 pixel scale (250×250m resolution) with integrated H3 resolution 5 regional context, optimizing the balance between spatial detail and classification accuracy while ensuring robust generalization to diverse urban environments.</p>
</section>
<section id="final-model" class="level2">
<h2 class="anchored" data-anchor-id="final-model">Final Model</h2>
<section id="addressing-class-imbalance" class="level3">
<h3 class="anchored" data-anchor-id="addressing-class-imbalance">Addressing Class Imbalance</h3>
<p>Significant class imbalance, particularly for urban classes, posed a considerable challenge in our modeling. To mitigate this, we implemented a sliding-window augmentation strategy for classes comprising less than 10% of the dataset (all classes except <em>Countryside agriculture</em> and <em>Wild countryside</em>).</p>
<p>This sliding-window approach systematically shifted the sampling window horizontally and vertically by increments of 50, 100, 150, and 200 meters, significantly increasing the volume of available training data for underrepresented classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/img_aug.jpg" class="nostretch figure-img" height="420"></p>
<figcaption>Sliding Window Augmentation</figcaption>
</figure>
</div>
<p>The images below show class distributions before (left) and after (right) augmentation:</p>
<p><img src="../figures/results/type_clean_encode.png" class="nostretch quarto-figure quarto-figure-left" height="200" alt="Before Augmentation"> <img src="../figures/results/type_clean_encode_augmented.png" class="nostretch quarto-figure quarto-figure-right" height="200" alt="After Augmentation"></p>
<p>The following table summarizes augmentation results, highlighting the substantial increase in training samples for previously underrepresented classes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 48%">
<col style="width: 26%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Class</th>
<th>Before Augmentation</th>
<th>After Augmentation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accessible suburbia</td>
<td>15,054</td>
<td>129,620</td>
</tr>
<tr class="even">
<td>Connected residential neighbourhoods</td>
<td>2,567</td>
<td>21,021</td>
</tr>
<tr class="odd">
<td>Countryside agriculture</td>
<td>1,367,999</td>
<td>1,367,999</td>
</tr>
<tr class="even">
<td>Dense residential neighbourhoods</td>
<td>4,299</td>
<td>34,507</td>
</tr>
<tr class="odd">
<td>Dense urban neighbourhoods</td>
<td>3,636</td>
<td>31,657</td>
</tr>
<tr class="even">
<td>Disconnected suburbia</td>
<td>2,644</td>
<td>20,113</td>
</tr>
<tr class="odd">
<td>Gridded residential quarters</td>
<td>1,518</td>
<td>12,849</td>
</tr>
<tr class="even">
<td>Open sprawl</td>
<td>33,910</td>
<td>292,884</td>
</tr>
<tr class="odd">
<td>Urban buffer</td>
<td>381,283</td>
<td>381,283</td>
</tr>
<tr class="even">
<td>Urbanity</td>
<td>2,495</td>
<td>21,929</td>
</tr>
<tr class="odd">
<td>Warehouse/Park land</td>
<td>21,282</td>
<td>195,105</td>
</tr>
<tr class="even">
<td>Wild countryside</td>
<td>1,395,048</td>
<td>1,395,048</td>
</tr>
</tbody>
</table>
</section>
<section id="classification-schemes" class="level3">
<h3 class="anchored" data-anchor-id="classification-schemes">Classification Schemes</h3>
<p>We used two versions of our dataset:</p>
<ol type="1">
<li><strong>12-class scheme</strong>: Maintained all original Spatial Signatures classes:</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>class_labels <span class="op">=</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Accessible suburbia'</span>: <span class="dv">0</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Connected residential neighbourhoods'</span>: <span class="dv">1</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Countryside agriculture'</span>: <span class="dv">2</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Dense residential neighbourhoods'</span>: <span class="dv">3</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Dense urban neighbourhoods'</span>: <span class="dv">4</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Disconnected suburbia'</span>: <span class="dv">5</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Gridded residential quarters'</span>: <span class="dv">6</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Open sprawl'</span>: <span class="dv">7</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urban buffer'</span>: <span class="dv">8</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urbanity'</span>: <span class="dv">9</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Warehouse/Park land'</span>: <span class="dv">10</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Wild countryside'</span>: <span class="dv">11</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li><strong>Simplified 7-class scheme</strong>: Created by reclustering underlying data from the Spatial Signatures Framework using K-means clustering (K=7):</li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>class_labels_k7 <span class="op">=</span> {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Countryside agriculture'</span>: <span class="dv">0</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Open sprawl'</span>: <span class="dv">1</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Compact suburbia'</span>: <span class="dv">2</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urban'</span>: <span class="dv">3</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urban buffer'</span>: <span class="dv">4</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Warehouse/Park land'</span>: <span class="dv">5</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Wild countryside'</span>: <span class="dv">6</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="model-performance" class="level3">
<h3 class="anchored" data-anchor-id="model-performance">Model Performance</h3>
<p>The final XGBoost classifier was trained using the augmented dataset and evaluated using three metrics: micro accuracy, macro accuracy (where every class has the same weighting), and macro F1 score. We validated the model using 5-fold spatial cross-validation at H3 resolution 6, ensuring an 80/20 training-testing split.</p>
<p>The table below summarizes classifier performance for both classification schemes (7 and 12 classes) and two spatial context scenarios (with and without H3 resolution):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 23%">
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Classes (K)</th>
<th>Spatial Context</th>
<th>Accuracy</th>
<th>Macro Accuracy</th>
<th>Macro F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>7</td>
<td>None</td>
<td>0.4924</td>
<td>0.3856</td>
<td>0.3389</td>
</tr>
<tr class="even">
<td>7</td>
<td>H3 (res 5)</td>
<td>0.6959</td>
<td>0.5713</td>
<td><strong>0.5221</strong></td>
</tr>
<tr class="odd">
<td>12</td>
<td>None</td>
<td>0.4617</td>
<td>0.2666</td>
<td>0.2127</td>
</tr>
<tr class="even">
<td>12</td>
<td>H3 (res 5)</td>
<td>0.6654</td>
<td>0.4328</td>
<td>0.3654</td>
</tr>
</tbody>
</table>
<p>Including spatial context (H3 resolution) notably improved classification accuracy and F1 scores, demonstrating the importance of spatial context in predicting urban fabric classes. As anticipated, the model with fewer classes (7) performed better compared to the one with 12 classes.</p>
</section>
</section>
<section id="temporal-analysis" class="level2">
<h2 class="anchored" data-anchor-id="temporal-analysis">Temporal Analysis</h2>
<p>We used the trained XGBoost classifier to make predictions across the years 2016 to 2021. The overall overlap between the initial year (2016) and final year (2021) remained high at 88%, confirming that Spatial Signatures classes remained relatively stable across the study period. However, minor variations may indicate either genuine change or model uncertainty.</p>
<p>Year-to-year class stability rates:</p>
<ul>
<li>2016 → 2017: 88%</li>
<li>2017 → 2018: 88%</li>
<li>2018 → 2019: 86%</li>
<li>2019 → 2020: 86%</li>
<li>2020 → 2021: 88%</li>
</ul>
<section id="diversity-analysis-shannon-index" class="level3">
<h3 class="anchored" data-anchor-id="diversity-analysis-shannon-index">Diversity Analysis (Shannon Index)</h3>
<p>We assessed changes in urban fabric diversity using the Shannon Index across the studied years. The results are summarized in the table below, indicating slight fluctuations, with the most notable increase in diversity occurring in 2019:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Year</th>
<th>Shannon Index</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2016</td>
<td>1.868</td>
</tr>
<tr class="even">
<td>2017</td>
<td>1.863</td>
</tr>
<tr class="odd">
<td>2018</td>
<td>1.868</td>
</tr>
<tr class="even">
<td>2019</td>
<td>2.007</td>
</tr>
<tr class="odd">
<td>2020</td>
<td>1.873</td>
</tr>
<tr class="even">
<td>2021</td>
<td>1.872</td>
</tr>
</tbody>
</table>
<p>The marked increase in the Shannon Index in 2019 suggests an increase in class diversity during that year, followed by a subsequent return to previous levels. This could indicate some differences in the imagery caused by weather conditions or sensor characteristics specific to 2019.</p>
</section>
<section id="spatial-patterns-of-change" class="level3">
<h3 class="anchored" data-anchor-id="spatial-patterns-of-change">Spatial Patterns of Change</h3>
<p>To make predictions across years, we used the trained model and fed in all GB tiles to get predictions for the 12 cleasses.</p>
<p>Spatial analysis identified areas across England with frequent class transitions (map below), particularly around major urban centers and suburban zones. These frequent transitions may either represent genuine urban transformations or result from classifier uncertainty, especially in ambiguous zones between visually similar Spatial Signatures classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/frequency_map.png" class="nostretch figure-img" height="420"></p>
<figcaption>Frequency Map</figcaption>
</figure>
</div>
<p>Class-specific analysis further highlighted particular Spatial Signatures types prone to transitions. The figure below shows which classes experienced frequent transitions:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/class_transitions.png" class="nostretch figure-img" height="420"></p>
<figcaption>Class Transitions</figcaption>
</figure>
</div>
<p>To better interpret these transitions, we calculated transition probabilities and organized them into a structured confusion matrix. This matrix clearly shows the urban fabric classes most likely to interchange over the studied period:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/class_transitions_reasonable.png" class="nostretch figure-img" height="420"></p>
<figcaption>‘Reasonable’ confusion matrix</figcaption>
</figure>
</div>
<p>Lastly, we analyzed the inverse-probability-based distances of urban fabric classes over time. Shorter distances represent a higher likelihood of transitioning or changing classes from year to year, whereas longer distances indicate greater stability:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/1d_transitions.png" class="img-fluid figure-img"></p>
<figcaption>Probability of change across classes</figcaption>
</figure>
</div>
<p>This analysis shows urban fabric classes such as <em>Urbanity</em>, <em>Warehouse/Park land</em>, <em>Dense urban neighbourhoods</em>, and <em>Connected residential neighbourhoods</em> have shorter inverse distances, indicating higher levels of dynamic change or redevelopment. Conversely, classes such as <em>Wild countryside</em> and <em>Countryside agriculture</em> have longer distances, suggesting greater temporal stability. Although these results align with expectations about urban and rural dynamics, they may also reflect the classifier’s varying uncertainty across these visually distinct environments.</p>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<section id="key-findings" class="level2">
<h2 class="anchored" data-anchor-id="key-findings">Key Findings</h2>
<p>The analysis demonstrates that urban fabric classifications exhibit distinct temporal and spatial dynamics, reflecting varying levels of stability and diversity over time. Notably, urban classes such as <em>Dense urban neighbourhoods</em> and <em>Connected residential neighbourhoods</em> displayed higher probabilities of transition, indicating active urban transformation. Similarly, suburban classes, such as <em>Accessible suburbia</em>, <em>Disconnected suburbia</em>, and <em>Urban buffer</em>, also show a higher probability of change between these classes. Conversely, rural classes showed significant stability. It’s important to note that these observed changes may be more related to classifier uncertainty than actual environmental changes.</p>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h2>
<ul>
<li><p><strong>Scale</strong>: Urban fabric classes, like spatial signatures, have fuzzy boundaries. Pixel-level classifications provide the highest possible spatial resolution, which is beneficial for detailed analysis. However, pixels often lack clear visual cues indicating exact boundaries between classes, making pixel-level predictions challenging for the model. Patch-level classification, although lower in spatial resolution depending on patch size, provided clearer visual context and resulted in better overall performance.</p></li>
<li><p><strong>Embeddings vs.&nbsp;Fine-Tuned Foundation Model</strong>: Fine-tuning foundation models involves significant complexity and requires careful design decisions. In our case, the limited number of training examples was insufficient to achieve noticeable improvements through fine-tuning. The effort required for fine-tuning did not outweigh the simpler alternative of using off-the-shelf embeddings.</p></li>
<li><p><strong>Regional Trends</strong>: Including regional contextual information substantially improved the classifier’s accuracy. Nonetheless, we found it essential that embeddings themselves already capture enough visual detail for accurate classification, ensuring that predictions remain robust even without regional context (and simply do not just rely on the spatial information to make predictions).</p></li>
<li><p><strong>Data Augmentation</strong>: The sliding window augmentation approach effectively addressed class imbalances, significantly improving model performance by increasing representation of previously underrepresented urban fabric classes. This approach could boost classifier performance by an additional 10-20% in terms of accuracy.</p></li>
</ul>
</section>
<section id="potential-research-directions" class="level2">
<h2 class="anchored" data-anchor-id="potential-research-directions">Potential Research Directions</h2>
<p>There are several promising directions for future research:</p>
<ul>
<li><p><strong>Handling Misclassifications</strong>: Misclassifications typically occur between visually similar urban fabric classes, indicating inherent uncertainty in predictions. Incorporating prediction probabilities into a secondary model could help address this issue. By explicitly using probability scores from the initial classification as input for a refinement model — as previously shown by Fleischmann and Arribas-Bel<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> — we could better distinguish between ambiguous cases. This approach may “smooth” predictions, reducing noise and improving overall classification accuracy. Future work should explore how prediction confidence scores can be systematically utilized, either by employing spatial smoothing algorithms or by applying secondary machine learning models trained specifically to correct uncertain predictions.</p></li>
<li><p><strong>Generalizability Testing</strong>: Evaluating the generalizability of this methodological framework is crucial for its wider applicability. Future research should test this modeling approach in different European regions, assessing whether the chosen methods, including data preprocessing, augmentation strategies, spatial embeddings, and classifier architectures, perform consistently outside Great Britain. This would involve exploring variations in urban form and regional urban planning contexts across Europe. Understanding these factors will help identify potential adjustments needed to ensure reliable predictions when extending the model beyond the original study area.</p></li>
</ul>
</section>
</section>
<section id="software-and-example-datasets" class="level1">
<h1>Software and Example Datasets</h1>
<p>All analyses presented here are supported by openly accessible software hosted on <a href="https://github.com/eurofab-project/eo/tree/main">GitHub</a>. The AI prediction pipeline, including preprocessing, embedding generation, and prediction of spatial signatures, is fully documented and accessible in the EO repository.</p>
<section id="software-ai-method-for-urban-fabric-classification-and-morphometric-characterization" class="level2">
<h2 class="anchored" data-anchor-id="software-ai-method-for-urban-fabric-classification-and-morphometric-characterization">Software: AI Method for Urban Fabric Classification and Morphometric Characterization</h2>
<p>All the work supporting this analysis can be found on GitHub. The main prediction pipeline, which includes data preprocessing, embedding creation, and spatial signature prediction, can be used as follows:</p>
<pre><code># Run the pipeline
pipeline.spatial_sig_prediction(
    geo_path= "../spatial_signatures/eo/data/example/london_25_25_grid_clipped.geojson", ## Vector file (geojson or parquet) of analysis area (grid).
    vrt_file= "../satellite_demoland/data/mosaic_cube/vrt_allbands/2017_combined.vrt", ## Vrt file of the satellite composite
    xgb_weights = "../spatial_signatures/classifier/k12_h5_slided_gb_xgb_model.bin", ## Model weights for XGBoost classifier
    model_weights = "../satellite_demoland/models/satlas/weights/satlas-model-v1-lowres.pth", ## Model weights for embedding model (Satlas)
    output_path= "../vjgo8416-demoland/spatial_signatures/eo/data/predictions/test_london_h6.parquet", ## Output file with predictions, prediction probabilities and geometries
    h3_resolution=5 ## h3 resolution to be added to analysis (spatial context)
)</code></pre>
<p>More details and documentation on how to run the pipeline can be found in the example on the <a href="https://github.com/eurofab-project/eo/blob/main/notebooks/run_pipeline.ipynb">EO repository</a>.</p>
</section>
<section id="example-datasets-generated-during-verification-exercises" class="level2">
<h2 class="anchored" data-anchor-id="example-datasets-generated-during-verification-exercises">Example Datasets Generated During Verification Exercises</h2>
<p>The final datacube including predictions for the years 2016 to 2021 for 7 and 12 classes can be found in the repository.</p>
<p>Here are some example visualizations showing London and Liverpool from the dataset:</p>
</section>
<section id="london" class="level2">
<h2 class="anchored" data-anchor-id="london">London</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/london_k7.png" class="nostretch figure-img" height="400"></p>
<figcaption>London, 7 classes</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/london_k12.png" class="nostretch figure-img" height="400"></p>
<figcaption>London, 12 classes</figcaption>
</figure>
</div>
</section>
<section id="liverpool" class="level2">
<h2 class="anchored" data-anchor-id="liverpool">Liverpool</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/liverpool_k7.png" class="nostretch figure-img" height="400"></p>
<figcaption>Liverpool, 7 classes</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/liverpool_k12.png" class="nostretch figure-img" height="400"></p>
<figcaption>Liverpool, 12 classes</figcaption>
</figure>
</div>
</section>
</section>
<section id="references" class="level1 unnumbered">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-arribas2022spatial" class="csl-entry" role="listitem">
Arribas-Bel, Daniel, and Martin Fleischmann. 2022. <span>“Spatial Signatures-Understanding (Urban) Spaces Through Form and Function.”</span> <em>Habitat International</em> 128: 102641.
</div>
<div id="ref-corbane2020global" class="csl-entry" role="listitem">
Corbane, C., P. Politis, P. Kempeneers, D. Simonetti, P. Soille, A. Burger, M. Pesaresi, F. Sabo, V. Syrris, and T. Kemper. 2020. <span>“A Global Cloud Free Pixel- Based Image Composite from <span>Sentinel</span>-2 Data.”</span> <em>Data in Brief</em> 31 (August): 105737. <a href="https://doi.org/10.1016/j.dib.2020.105737">https://doi.org/10.1016/j.dib.2020.105737</a>.
</div>
<div id="ref-fleischmann2022geographical" class="csl-entry" role="listitem">
Fleischmann, Martin, and Daniel Arribas-Bel. 2022. <span>“Geographical Characterisation of British Urban Form and Function Using the Spatial Signatures Framework.”</span> <em>Scientific Data</em> 9 (1): 546.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Fleischmann and Arribas-Bel, 2024. Decoding (urban) form and function using spatially explicit deep learning. <em>Computers, Environment and Urban Systems</em>, 31, p.105737.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>