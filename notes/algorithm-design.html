<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krasen Samardzhiev, Barbara Metzler, Martin Fleischmann, Dani Arribas-Bel">

<title>Algorithm Design and Theoretical Basis Description – Technical notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-f25f27ff4b7ba7102a05c66407501d67.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&amp;display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive summary</a></li>
  <li><a href="#theoretical-basis" id="toc-theoretical-basis" class="nav-link" data-scroll-target="#theoretical-basis">Theoretical basis</a>
  <ul class="collapse">
  <li><a href="#morphometric-classification-homogenisation" id="toc-morphometric-classification-homogenisation" class="nav-link" data-scroll-target="#morphometric-classification-homogenisation">Morphometric Classification Homogenisation</a></li>
  <li><a href="#ai-modelling-using-satellite-imagery" id="toc-ai-modelling-using-satellite-imagery" class="nav-link" data-scroll-target="#ai-modelling-using-satellite-imagery">AI Modelling using Satellite Imagery</a></li>
  </ul></li>
  <li><a href="#algorithm-design" id="toc-algorithm-design" class="nav-link" data-scroll-target="#algorithm-design">Algorithm Design</a>
  <ul class="collapse">
  <li><a href="#morphometric-classification-homogenisation-1" id="toc-morphometric-classification-homogenisation-1" class="nav-link" data-scroll-target="#morphometric-classification-homogenisation-1">Morphometric Classification Homogenisation</a>
  <ul class="collapse">
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model architecture</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data preprocessing</a></li>
  <li><a href="#morphometric-characterisation" id="toc-morphometric-characterisation" class="nav-link" data-scroll-target="#morphometric-characterisation">Morphometric characterisation</a></li>
  <li><a href="#target-labels" id="toc-target-labels" class="nav-link" data-scroll-target="#target-labels">Target labels</a></li>
  <li><a href="#prediction-modelling-and-traintest-split" id="toc-prediction-modelling-and-traintest-split" class="nav-link" data-scroll-target="#prediction-modelling-and-traintest-split">Prediction modelling and train/test split</a></li>
  <li><a href="#training-and-evaluation" id="toc-training-and-evaluation" class="nav-link" data-scroll-target="#training-and-evaluation">Training and evaluation</a></li>
  <li><a href="#preliminary-results" id="toc-preliminary-results" class="nav-link" data-scroll-target="#preliminary-results">Preliminary results</a></li>
  <li><a href="#limitations-potential-problems" id="toc-limitations-potential-problems" class="nav-link" data-scroll-target="#limitations-potential-problems">Limitations &amp; Potential problems</a></li>
  </ul></li>
  <li><a href="#ai-modelling-using-satellite-imagery-1" id="toc-ai-modelling-using-satellite-imagery-1" class="nav-link" data-scroll-target="#ai-modelling-using-satellite-imagery-1">AI Modelling using Satellite Imagery</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing-1" id="toc-data-preprocessing-1" class="nav-link" data-scroll-target="#data-preprocessing-1">Data preprocessing</a></li>
  <li><a href="#model-architectures" id="toc-model-architectures" class="nav-link" data-scroll-target="#model-architectures">Model architectures</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#preliminary-results-1" id="toc-preliminary-results-1" class="nav-link" data-scroll-target="#preliminary-results-1">Preliminary results</a></li>
  <li><a href="#x25-grid-classification-pipeline" id="toc-x25-grid-classification-pipeline" class="nav-link" data-scroll-target="#x25-grid-classification-pipeline">25x25 grid classification pipeline</a></li>
  <li><a href="#sampling-experiments" id="toc-sampling-experiments" class="nav-link" data-scroll-target="#sampling-experiments">Sampling experiments</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="algorithm-design.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Algorithm Design and Theoretical Basis Description</h1>
<p class="subtitle lead">Technical Note D2</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Krasen Samardzhiev, Barbara Metzler, Martin Fleischmann, Dani Arribas-Bel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Charles University; The Alan Turing Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<!-- This tries to closely match what was promised in the project proposal. -->
<section id="executive-summary" class="level1">
<h1>Executive summary</h1>
<p>The detailed classification of urban form can provide valuable insights into the structure of cities and towns, guide targeted policy applications, and form the backbone of urban planning. However, classification of this sort are scarcely available and even fewer are detailed, scalable, and consistent while reflecting the nature of the local data all at the same time. We often see conceptual classification, which has a tendency to oversimplify the structure when “zooming in”, loosing the interest of planners and policy-makers. The classifications developed by key members of the EuroFab project team overcome these limitations, but are dependent on the availability of high quality data capturing urban form, e.g.&nbsp;individual buildings and related street networks. The issue is that these are not always readily available, even within the context of generally data-rich countries of the European region. Therefore, there is a need to derive such classification from suboptimal data that do not have the same qualities as, for example, cadastre would have, but have consistency and continental coverage. This Technical Note outlines the theoretical and methodological bases for two models - one based on satellite-derived building footprints; and a second, based on a direct use of Sentinel-2 visible bands. The models aim to adress the data issues presented above, while maintaining the quality of urban classification needed by practioners.</p>
<p>The first model, which we can call morphological, uses the Microsoft Open Buildings footprints dataset of a remote-sensing origin and predicts the classes provided by an authoritative target classification based on cadastral data. It measures morphological variables based on building footprints, linked to street network coming from the Overture Maps project and uses the measurements to predict the authoritative classification using non-linear tree-based ensemble models. The evaluated models are trained using spatially explicit splits and are extensively tested for a robustness of prediction in an out-of-sample context. We further use the results of the models as an iterative feedback loop allowing gap-filling of the original taxonomy when the model detects urban fabric it has not seen before and is therefore not able to reliably classify.</p>
<p>The second model, which we can call the AI model, works on top of Sentinel-2 visible bands at 10 meters per pixel resolution, and aims to provide a prediction using the foundational computer vision models, eventually allowing deployment of a predicted time series followed-up by yearly updates. The related work package compares three different architectures based on a direct extraction of model embeddings, segmentation, and classification, based on different foundational models (SatlasPretrain for the embeddings, SatlasPretrain, Clay, and IBM/NASA’s Prithvi for segmentation, Clay for classification). To further improve the model performance and acknowledge the effects of spatial heterogeneity, models optionally include explicit spatial dimension encoded via H3 grid locations. Overall, the classification approach with regional information (H3 Level 5) yielded the best performance, achieving both high accuracy and a reasonable balance across classes. Additionally, this approach is computationally efficient: once the image embeddings are generated, the downstream classification process can be completed in just a few minutes.</p>
</section>
<section id="theoretical-basis" class="level1">
<h1>Theoretical basis</h1>
<p>This report describes in detail the data, the data preprocessing, as well as model selection, training and validation schemes. The background section positions the research in the literature and identifies the drawbacks of current approaches, this study aims to address. It also provides some background on the study area - Central Europe. The data section describes the input, output and target data for the model. The methodology section focuses on the model selection, training and validation approaches.</p>
<section id="morphometric-classification-homogenisation" class="level2">
<h2 class="anchored" data-anchor-id="morphometric-classification-homogenisation">Morphometric Classification Homogenisation</h2>
<p>The spatial layout of the physical elements of cities - its urban fabric - affects most activities their residents undertake, from accessing services or jobs to their social and cultural lives. Analysing the interplay of urban form, land use, mobility and other dimensions of human activities provides insights into how cities evolve and what effective developmental policies should look like. Researchers in the field of urban morphology have spend years in identifying, classifying and analysing the variations in urban form across cities from all over the world. A core new development, powered by advancements in spatial data science, computer vision, and open data availability, are the methodologies created to computationally discern and analyse urban fabric <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span>. Taken together, these facts have opened up the possibility of a more systematic and comprehensive approach to the classification of urban morphological patterns, which in turn can drive our understanding of cities <span class="citation" data-cites="calafiore2023inequalities arribas2022spatial">(<a href="#ref-calafiore2023inequalities" role="doc-biblioref">Calafiore et al. 2023</a>; <a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>)</span>.</p>
<p>One factor that limits the wider application of these methods is their dependency on high-quality data, that is generally not available for every city. For example, <span class="citation" data-cites="fleischmann2022methodological arribas2022spatial">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>; <a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>)</span> use building polygons from the respective municipal and national mapping agencies. However, such data is not generally available even for most high-income countries. And where available, it can come in various formats and the data itself is not necessarily homogenous, which makes its processing difficult. For example, official Czechia cadastral building polygons (https://services.cuzk.cz/) come as a GeoPackage, with no information about building age, height, type or use. In contrast, official German building polygons, come separately for every state , typically but not always as GML data from a WFS service, and sometimes have extra building information. However, the definition of building is different to Czechia and the polygon set may include parts of tunnels, overpasses or tram lines. These definitions are even sometimes different in the cases of cities within the same country - i.e.&nbsp;the data for Berlin and Hamburg is different from the data available for Bremen.</p>
<p>EuroFab aims to address the data availability and processing issues, as well as broaden the applicability of urban morphometrics. It does this through the development of a predictive urban morphometric model which uses widely available building footprints to infer structure of urban form. The model input is calculated directly on satellite-derived building footprints, which gives it a global scope and thus eliminates the need the data fusion problems described above. Furthermore, it is trained on a detailed morphometric classification derived from official cadastre data in a large study area that covers multiple countries - Poland, Austria, Czechia, Germany and Slovakia, i.e.&nbsp;Central Europe. The heterogeneity of urban form and planning regimes present in the study area enables the model to distinguish a rich variety of urban patterns and improves it generalisability.</p>
<p>The specific approach taken has four stages. In the first stage morphological elements and their characteristics are calculated using the widely available Microsoft building and Overture Maps Transportation data. These elements are the predictor variables that the model will use. To achieve this we develop a highly-scalable polygon and street data pipeline, capable of calculating an exhaustive list of morphometric characteristics. In the second stage, each element, calculated in the first stage, is assigned a target classification label based on spatial overlap with morphological elements from <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span>. <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> is a classification of urban fabrics in Central Europe based on the highest-detail available cadastre data. Third, the data is then split into five training and testing subsets, so that every country and every combination of the other four countries are used as test and training data respectively. Lastly, a series of non-linear models are trained on each subset to predict the morphological classification of individual elements, using a custom cross-validation scheme in order to account for spatial leakage. Our training and testing setup emphasises evaluating the model’s ability to deal with realistic scenarios and with previously unseen data and new urban fabric types. The best performing model is chosen as the final production model, and retrained on the whole dataset.</p>
<!--Satellite derived building footprints intro-->
<p>Satellite derived building footprints are becoming more widely adopted, however, the data does not come without issues. For example, in dense urban centres entire blocks can be delineated as individual buildings. Given that morphology calculations rely on precise local topological relations between neighbours, such as two buildings touching, this problem renders a whole number of possible measurements described in <span class="citation" data-cites="fleischmann2021measuring">(<a href="#ref-fleischmann2021measuring" role="doc-biblioref">Fleischmann, Romice, and Porta 2021</a>)</span> meaningless. Furthermore, this issue affects even simpler calculations such as counting the number of buildings within a radius or topological neighbourhood. Other issues are that computer vision techniques sometimes miss entire buildings or misidentify building boundaries. Therefore, any approach that uses satellite-derived building footprints should be able to account for these three and potentially other problems.</p>
<p>Implementing these stages, and further realising the potential of the model for global application, requires the creation of novel, scalable data processing pipelines capable of analysing hundreds of millions of morphological elements - buildings, streets and their derivatives. Due to their scalability, these pipelines can further be reused for other work, outside of the immediate scope of this projects. To ensure maximum impact and high standards, all work for this project follows open science principles and is open-sourced on GitHub.</p>
</section>
<section id="ai-modelling-using-satellite-imagery" class="level2">
<h2 class="anchored" data-anchor-id="ai-modelling-using-satellite-imagery">AI Modelling using Satellite Imagery</h2>
<p>Advancements in satellite imagery and artificial intelligence (AI) have significantly enhanced our ability to analyze complex geospatial datasets. Satellite imagery provides detailed information at varying spatial and temporal scales, enabling applications that range from analyzing individual buildings to studying entire continents. By extracting visual and spectral data, satellite images offer insights into ground conditions such as land cover and land use classification <span class="citation" data-cites="esch2010how ma2019deep ibrahim2020understanding huang2018urban">(<a href="#ref-esch2010how" role="doc-biblioref">Esch et al. 2010</a>; <a href="#ref-ma2019deep" role="doc-biblioref">Ma et al. 2019</a>; <a href="#ref-ibrahim2020understanding" role="doc-biblioref">Ibrahim, Haworth, and Cheng 2020</a>; <a href="#ref-huang2018urban" role="doc-biblioref">Huang, Zhao, and Song 2018</a>)</span>. AI algorithms transform these data into actionable insights by automating the analysis of vast amounts of pixel-level information. However, many conventional AI models are designed for specific tasks and require significant fine-tuning for new applications.</p>
<p>The emergence of foundation models in geospatial AI (GeoAI) marks a transformative shift. Unlike traditional task-specific models, foundation models are pre-trained on large, diverse datasets and can be adapted to multiple tasks with minimal additional training <span class="citation" data-cites="rolf2021generalizable">(<a href="#ref-rolf2021generalizable" role="doc-biblioref">Rolf et al. 2021</a>)</span>. This adaptability is particularly useful in remote sensing, where data are often complex, integrating multi-spectral and multi-temporal information <span class="citation" data-cites="lu2024ai">(<a href="#ref-lu2024ai" role="doc-biblioref">Lu et al. 2024</a>)</span>.</p>
<p>Foundation models have recently demonstrated superior performance, especially in scenarios with limited labeled data. Transformer-based architectures, pre-trained on extensive multispectral datasets, have become central to GeoAI research. Notable examples include Prithvi <span class="citation" data-cites="jakubik2023foundation">(<a href="#ref-jakubik2023foundation" role="doc-biblioref">Jakubik et al. 2023</a>)</span> and SatlasPretrain <span class="citation" data-cites="bastani2023satlaspretrain">(<a href="#ref-bastani2023satlaspretrain" role="doc-biblioref">Bastani et al. 2023</a>)</span>, which were trained on terabytes of imagery and millions of labeled instances. These models have achieved state-of-the-art results in tasks such as image classification and multi-temporal image segmentation, illustrating their ability to address diverse and complex Earth observation challenges.</p>
</section>
</section>
<section id="algorithm-design" class="level1">
<h1>Algorithm Design</h1>
<section id="morphometric-classification-homogenisation-1" class="level2">
<h2 class="anchored" data-anchor-id="morphometric-classification-homogenisation-1">Morphometric Classification Homogenisation</h2>
<section id="model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture">Model architecture</h3>
<p>The model architecture consists of the two main components: 1) derivation of predictive variables, and 2) development of the predictive model. Given the target of the prediction is morphological classification, we use morphometric measurements based on the sub-standard satellite-derived data as predictive variables, as they are conceptually related - the original target classification is a result of unsupervised learning on top of morphometric measurements based on cadastral data. The model’s main job is then to capture the shift of meaning of individual characters from the original, when measured on precise geometries, to the derived one, measured on imperfect representation of urban form. Given the model will eventually perform a prediction out of the sample, we further build in a logic identifying the types of urban form it has not seen previously and ingesting then manually in the original taxonomy, forming an iterative feedback loop. The whole system is illustrated in Figure 1.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/workflow.png" class="nostretch figure-img" height="400"></p>
<figcaption>Project workflow</figcaption>
</figure>
</div>
<p>The data preparation and model training consists of four stages. The first and second stages cover the morphometric characterisation, which acts as the model’s predictor variables, and the target variables generation. The third stage splits splits the whole study area into five subsets, so that every country and every combintion of the four other countries in the area, act as test and training data respectively. The final stage is the model training and evaluation, aiming to minimise spatial leakeage in the training and evaluation data, and testing the model’s performance on realistic scenarios. The full model training and evaluation framework will be implemented using scikit-learn pipelines.</p>
</section>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h3>
<section id="building-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="building-preprocessing">Building preprocessing</h4>
<p>All available Microsoft building footprints for the study area are used for the analysis. Typically, building polygons required for morphological studies have to be of very high quality. For example, building polygons overlapping by a thousandth of a millimetre break topological contiguity and therefore affect the calculation of morphological properties, such as the ratio of shared walls, or the number of adjacent buildings. Furthermore, even the highest quality available software suffers from numerical precision issues which exasperate the above problem. Another potential issue is the inclusion of artefacts we are not directly interested in such as sheds or market stalls. Most importantly, polygons shall represent individual buildings, not compounds of buildings that are adjacent. The footprints used for this study fall short of these standards. Therefore, our approach aims to accommodate less than ideal data and methods, first by processing the building data, and second, by adapting the morphological calculations to account for numerical issues.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/building_comparison.png" class="nostretch figure-img" height="400"></p>
<figcaption>Comparison between MS buildings and cadastre level buildings in central Prague</figcaption>
</figure>
</div>
<p>Before dealing with any morphological assessment, the polygons need to undergo basic topological preprocessing.</p>
<p>The first step in the building data processing is to split up multi-polygons and make the geometries valid. The second step, is to simplify the polygons in order to accurately represent the corners of buildings and other shape related characteristics. Next, to filter out any buildings that have an area larger than 200,000 sq.m. This is done since some artefacts such as construction sites, mines or tunnels might be included in the data as buildings. The next step is to merge overlapping buildings that either: overlap for at least 10 percent of their areas, or one of them has less than 50 sq.m. in total area. This is done to merge buildings and building parts, since cadastre definitions of these two polygon types are inconsistent and sometimes buildings are assigned as building parts or vice versa. This step merges the buildings and its parts into one polygon. Finally, the preprocessing pipeline snaps nearby buildings together and fills gaps in the polygons that are less than 10 square cm. These two steps aim to address some common topological issues, such as missing slivers with almost zero areas between multiple or inside individual building polygons. Nevertheless, even after the preprocessing numerous topological issues remain and therefore we take this into account in subsequent analysis steps.</p>
</section>
<section id="overture-streets" class="level4">
<h4 class="anchored" data-anchor-id="overture-streets">Overture Streets</h4>
<p>The street network is a direct download from Overture Maps Transportation theme, a processed subset of data from OpenStreetMap, which has global coverage and high quality data. Since the dataset includes multiple segments types, including footpaths, the types of segments used in the analysis are limited to … . Another type of segment that is filtered out are tunnels - the analysis strictly focuses on two dimensions and therefore undergrounds structures adversely affect the calculation of boundaries and characters.</p>
<p>The second major stage of the street processing is the simplification of the entire street network for each subregion. The network coming from Overture Maps, similarly to nearly any other common source, focuses on representation of street network for transportation purposes. That means it tends to include multiple geometries for wide boulevards where each captures a single carriageway, complex representation of junctions or even the smallest artefacts of transportation-based focus. However, such a network is not directly usable for morphological analysis as it does not capture morphological perception of street network which is usually captured via street centrelines, omitting transportation detail. For this reason, we apply the simplification method based on the detection of the problematic parts of the network <span class="citation" data-cites="fleischmann2024Shapebased">(<a href="#ref-fleischmann2024Shapebased" role="doc-biblioref">Fleischmann and Vybornova 2024</a>)</span> by <span class="citation" data-cites="sgeop">Fleischmann, Vybornova, and Gaboardi (<a href="#ref-sgeop" role="doc-biblioref">Forthcoming</a>)</span>. This ensures automatised algorithmic cleaning of street networks resulting in a morphological representation derived from the transportation one. <!--- TODO this needs more explanation--></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/street_processing.png" class="nostretch figure-img" height="400"></p>
<figcaption>Streets in central Prague</figcaption>
</figure>
</div>
</section>
</section>
<section id="morphometric-characterisation" class="level3">
<h3 class="anchored" data-anchor-id="morphometric-characterisation">Morphometric characterisation</h3>
<p>The morphometric characterisation is directly derived from the method of <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> as closely as possible to ensure that we minimise the conceptual differences between the methodological backbone using the derivation of the target classification and the the data used within our model.</p>
<section id="subregions-split" class="level4">
<h4 class="anchored" data-anchor-id="subregions-split">Subregions split</h4>
<p>Since the study area of interest is large and contains tens of millions of buildings, it is divided into subregions to carry out all computation. The separation is done based on distances between buildings - buildings are split into subregions such that the building from one region and its closest neighbour (another building) from another region are at least 400 meters apart. This custom separation, rather than official administrative divisions, ensures that all elements that may affect morphological calculations are in the same set (subregion) and not split across political boundaries. All processing and character calculations are done for each region independently and in parallel.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/subregions.png" class="nostretch figure-img" height="400"></p>
<figcaption>All subregions in the study area</figcaption>
</figure>
</div>
</section>
<section id="elements-and-units" class="level4">
<h4 class="anchored" data-anchor-id="elements-and-units">Elements and units</h4>
<p>There are five morphological elements used for the morphometric characterisation - two base ones - buildings and streets and three derived ones - enclosures, nodes, enclosed tessellation cells (ETC). Buildings and streets are the two elements from which all other units are derived. The core unit of analysis in the study is the enclosed tessellation cell, which breaks down the whole study area into small-scale units, which when taken together fully cover the area.</p>
<section id="nodes" class="level5">
<h5 class="anchored" data-anchor-id="nodes">Nodes</h5>
<p>The first type of derived element in the study are street nodes, which are defined as the intersection points between different streets. They are used to calculate characteristics of the street network that capture relationships between streets such as number of intersections, as well as relationships between neighbouring enclosures and ETCs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/nodes.png" class="nostretch figure-img" height="400"></p>
<figcaption>Nodes in central Prague</figcaption>
</figure>
</div>
</section>
<section id="enclosures" class="level5">
<h5 class="anchored" data-anchor-id="enclosures">Enclosures</h5>
<p>Enclosures capture the characteristics of plots of land that contain from none to (usually) multiple buildings. They are operationalised as land delineations, surrounded by streets and other physical barriers, which can vary in size depending on the geographic context. If an area is in the city centre, each enclosure would approximate a single block and multiple building units, however if it was in an industrial area it would potentially encompass a single, or very few large buildings. In this study, only the street network is used for barriers to minimise the data dependency. Furthermore, enclosures are used to delineate the boundaries of enclosed tessellation cells to the surrounding streets - i.e.&nbsp;representing physical barriers.</p>
<p>In this study, enclosure delineation is further modified by introducing a variable, individual bandwidth for every building, as opposed to the global one used by <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> or none using in generic enclosure delineation. This is done to limit the boundaries effects around the edges of cities and towns - i.e.&nbsp;cells on the edges of cities in <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> are always large because there are no surrounding buildings and their cells resemble those of large buildings with lots of empty space around them. The limits used here are calculated through a Gabriel graph-based filtration of the subregion, which takes into account the surrounding neighbours structure around every ETC. For example, in row housing the buffer will be relatively small, in single family housing estates the buffer will be larger, and in industrial areas larger still; regardless of whether or not these buildings are in the middle of cities or around their edges. The detailed technical implementation is out of scope of this Technical Note.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/enclosures.png" class="nostretch figure-img" height="400"></p>
<figcaption>Enclosures in central Prague</figcaption>
</figure>
</div>
</section>
<section id="enclosed-tessellation-cells" class="level5">
<h5 class="anchored" data-anchor-id="enclosed-tessellation-cells">Enclosed Tessellation Cells</h5>
<p>Enclosed Tessellation Cells are the core unit used for the analysis and the one used to combine aspects of all of the other four elements. To operationalise it, the study follows the definition by <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> - “the portion of space that results from growing a morphological tessellation within an enclosure delineated by a series of natural or built barriers identified from the literature on urban form, function and perception”, where the morphological tessellation is a delineation of the space based on Voronoi polygons centred around buildings. The boundaries of ETCs also represent the closest area of land to each building, than to any other building within an enclosure. Because of this feature, ETCs intersect with all other elements and are the unit that links together the characteristics of the four other elements. In some cases, if there are no buildings within the enclosure the whole enclosure is treated as an ‘empty’ tessellation cell.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/tessellations.png" class="nostretch figure-img" height="400"></p>
<figcaption>Enclosed tessellation cells in central Prague</figcaption>
</figure>
</div>
</section>
<section id="morphometric-characters" class="level5">
<h5 class="anchored" data-anchor-id="morphometric-characters">Morphometric Characters</h5>
<p>Characteristics describing the interactions of these elements, and the elements themselves are calculated at three scales: small - covering only aspects of the element; medium - covering aspects of the element and neighbouring elements and large - covering neighbouring elements up to five topological neighbours. In total there are 52 morphometric characters calculated described in the list below, which come directly from the list of characters used to derive the target classification.</p>
<ol type="1">
<li><strong>Area of a building</strong> is denoted as</li>
</ol>
<ol class="example" type="1">
<li><span class="math inline">\(a_{blg}\)</span></li>
</ol>
<p>and defined as an area covered by a building footprint in m<sup>2</sup> .</p>
<ol start="2" type="1">
<li><strong>Perimeter of a building</strong> is denoted as</li>
</ol>
<ol start="2" class="example" type="1">
<li><span class="math inline">\(p_{blg}\)</span></li>
</ol>
<p>and defined as the sum of lengths of the building exterior walls in m.</p>
<ol start="3" type="1">
<li><strong>Courtyard area of a building</strong> is denoted as</li>
</ol>
<ol start="3" class="example" type="1">
<li><span class="math inline">\(a_{blg_c}\)</span></li>
</ol>
<p>and defined as the sum of areas of interior holes in footprint polygons in m<sup>2</sup>.</p>
<ol start="4" type="1">
<li><strong>Circular compactness of a building</strong> is denoted as</li>
</ol>
<ol start="4" class="example" type="1">
<li><span class="math inline">\(CCo_{blg} = \frac{a_{blg}}{a_{blgC}}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{blgC}\)</span> is an area of minimal enclosing circle. It captures the relation of building footprint shape to its minimal enclosing circle, illustrating the similarity of shape and circle <span class="citation" data-cites="dibble2015">(<a href="#ref-dibble2015" role="doc-biblioref">Dibble et al. 2015</a>)</span>.</p>
<ol start="5" type="1">
<li><strong>Corners of a building</strong> is denoted as</li>
</ol>
<ol start="5" class="example" type="1">
<li><span class="math inline">\(Cor_{blg} = \sum_{i=1}^{n}{c_{blg}}\)</span></li>
</ol>
<p>where <span class="math inline">\(c_{blg}\)</span> is defined as a vertex of building exterior shape with an angle between adjacent line segments <span class="math inline">\(\leq\)</span> 170 degrees. It uses only external shape (<code>shapely.geometry.exterior</code>), courtyards are not included. Character is adapted from <span class="citation" data-cites="steiniger2008">(<a href="#ref-steiniger2008" role="doc-biblioref">Steiniger et al. 2008</a>)</span> to exclude non-corner-like vertices.</p>
<ol start="6" type="1">
<li><strong>Squareness of a building</strong> is denoted as</li>
</ol>
<ol start="6" class="example" type="1">
<li><span class="math inline">\(Squ_{blg} =  \frac{\sum_{i=1}^{n} D_{c_{blg_i}}}{n}\)</span></li>
</ol>
<p>where <span class="math inline">\(D\)</span> is the deviation of angle of corner <span class="math inline">\(c_{blg_i}\)</span> from 90 degrees and <span class="math inline">\(n\)</span> is a number of corners.</p>
<ol start="7" type="1">
<li><strong>Equivalent rectangular index of a building</strong> is denoted as</li>
</ol>
<ol start="7" class="example" type="1">
<li><span class="math inline">\(ERI_{blg} =  \sqrt{{a_{blg}} \over {a_{blgB}}} * {p_{blgB} \over p_{blg}}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{blgB}\)</span> is an area of a minimal rotated bounding rectangle of a building (MBR) footprint and <span class="math inline">\(p_{blgB}\)</span> its perimeter of MBR. It is a measure of shape complexity identified by <span class="citation" data-cites="basaraner2017">Basaraner and Cetinkaya (<a href="#ref-basaraner2017" role="doc-biblioref">2017</a>)</span> as the shape characters with the best performance.</p>
<ol start="8" type="1">
<li><strong>Elongation of a building</strong> is denoted as</li>
</ol>
<ol start="8" class="example" type="1">
<li><span class="math inline">\(Elo_{blg} =  \frac{l_{blgB}}{w_{blgB}}\)</span></li>
</ol>
<p>where <span class="math inline">\(l_{blgB}\)</span> is length of MBR and <span class="math inline">\(w_{blgB}\)</span> is width of MBR. It captures the ratio of shorter to the longer dimension of MBR to indirectly capture the deviation of the shape from a square <span class="citation" data-cites="schirmer2015">(<a href="#ref-schirmer2015" role="doc-biblioref">Schirmer and Axhausen 2015</a>)</span>.</p>
<ol start="9" type="1">
<li><strong>Centroid - corner distance deviation of a building</strong> is denoted as</li>
</ol>
<ol start="9" class="example" type="1">
<li><span class="math inline">\(CCD_{blg} =  \sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(ccd_{i}-\bar{ccd}\right)^{2}}\)</span></li>
</ol>
<p>where <span class="math inline">\(ccd_i\)</span> is a distance between centroid and corner <span class="math inline">\(i\)</span> and <span class="math inline">\(\bar{ccd}\)</span> is mean of all distances. It captures a variety of shape. As a corner is considered vertex with angle &lt; 170º to reflect potential circularity of object and topological imprecision of building polygon.</p>
<ol start="10" type="1">
<li><strong>Centroid - corner mean distance of a building</strong> is denoted as</li>
</ol>
<ol start="10" class="example" type="1">
<li><span class="math inline">\(CCM_{blg} =\frac{1}{n}\left(\sum_{i=1}^{n} ccd_{i}\right)\)</span></li>
</ol>
<p>where <span class="math inline">\(ccd_i\)</span> is a distance between centroid and corner <span class="math inline">\(i\)</span>. It is a character measuring a dimension of the object dependent on its shape <span class="citation" data-cites="schirmer2015">(<a href="#ref-schirmer2015" role="doc-biblioref">Schirmer and Axhausen 2015</a>)</span>.</p>
<ol start="11" type="1">
<li><strong>Longest axis length of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="11" class="example" type="1">
<li><span class="math inline">\(LAL_{cell} = d_{cellC}\)</span></li>
</ol>
<p>where <span class="math inline">\(d_{cellC}\)</span> is a diameter of the minimal circumscribed circle around the tessellation cell polygon. The axis itself does not have to be fully within the polygon. It could be seen as a proxy of plot depth for tessellation-based analysis.</p>
<ol start="12" type="1">
<li><strong>Area of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="12" class="example" type="1">
<li><span class="math inline">\(a_{cell}\)</span></li>
</ol>
<p>and defined as an area covered by a tessellation cell footprint in m<sup>2</sup>.</p>
<ol start="13" type="1">
<li><strong>Circular compactness of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="13" class="example" type="1">
<li><span class="math inline">\(CCo_{cell} = \frac{a_{cell}}{a_{cellC}}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{cellC}\)</span> is an area of minimal enclosing circle. It captures the relation of tessellation cell footprint shape to its minimal enclosing circle, illustrating the similarity of shape and circle.</p>
<ol start="14" type="1">
<li><strong>Equivalent rectangular index of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="14" class="example" type="1">
<li><span class="math inline">\(ERI_{cell} =  \sqrt{{a_{cell}} \over {a_{cellB}}} * {p_{cellB} \over p_{cell}}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{cellB}\)</span> is an area of the minimal rotated bounding rectangle of a tessellation cell (MBR) footprint and <span class="math inline">\(p_{cellB}\)</span> its perimeter of MBR. It is a measure of shape complexity identified by <span class="citation" data-cites="basaraner2017">Basaraner and Cetinkaya (<a href="#ref-basaraner2017" role="doc-biblioref">2017</a>)</span> as a shape character of the best performance.</p>
<ol start="15" type="1">
<li><strong>Coverage area ratio of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="15" class="example" type="1">
<li><span class="math inline">\(CAR_{cell} = \frac{a_{blg}}{a_{cell}}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{blg}\)</span> is an area of a building and <span class="math inline">\(a_{cell}\)</span> is an area of related tessellation cell <span class="citation" data-cites="schirmer2015">(<a href="#ref-schirmer2015" role="doc-biblioref">Schirmer and Axhausen 2015</a>)</span>. Coverage area ratio (CAR) is one of the commonly used characters capturing <em>intensity</em> of development. However, the definitions vary based on the spatial unit.</p>
<ol start="16" type="1">
<li><strong>Floor area ratio of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="16" class="example" type="1">
<li><span class="math inline">\(FAR_{cell} = \frac{fa_{blg}}{a_{cell}}\)</span></li>
</ol>
<p>where <span class="math inline">\(fa_{blg}\)</span> is a floor area of a building and <span class="math inline">\(a_{cell}\)</span> is an area of related tessellation cell. Floor area could be computed based on the number of levels or using an approximation based on building height.</p>
<ol start="17" type="1">
<li><strong>Length of a street segment</strong> is denoted as</li>
</ol>
<ol start="17" class="example" type="1">
<li><span class="math inline">\(l_{edg}\)</span></li>
</ol>
<p>and defined as a length of a <code>LineString</code> geometry in metres <span class="citation" data-cites="dibble2015 gil2012">(<a href="#ref-dibble2015" role="doc-biblioref">Dibble et al. 2015</a>; <a href="#ref-gil2012" role="doc-biblioref">Gil et al. 2012</a>)</span>.</p>
<ol start="18" type="1">
<li><strong>Width of a street profile</strong> is denoted as</li>
</ol>
<ol start="18" class="example" type="1">
<li><span class="math inline">\(w_{sp} = \frac{1}{n}\left(\sum_{i=1}^{n} w_{i}\right)\)</span></li>
</ol>
<p>where <span class="math inline">\(w_{i}\)</span> is width of a street section i. The algorithm generates street sections every 3 meters alongside the street segment, and measures mean value. In the case of the open-ended street, 50 metres is used as a perception-based proximity limit <span class="citation" data-cites="araldi2019">(<a href="#ref-araldi2019" role="doc-biblioref">Araldi and Fusco 2019</a>)</span>.</p>
<ol start="19" type="1">
<li><strong>Openness of a street profile</strong> is denoted as</li>
</ol>
<ol start="19" class="example" type="1">
<li><span class="math inline">\(Ope_{sp} = 1 - \frac{\sum hit}{2\sum sec}\)</span></li>
</ol>
<p>where <span class="math inline">\(\sum hit\)</span> is a sum of section lines (left and right sides separately) intersecting buildings and <span class="math inline">\(\sum sec\)</span> total number of street sections. The algorithm generates street sections every 3 meters alongside the street segment.</p>
<ol start="20" type="1">
<li><strong>Width deviation of a street profile</strong> is denoted as</li>
</ol>
<ol start="20" class="example" type="1">
<li><span class="math inline">\(wDev_{sp} = \sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(w_{i}-w_{sp}\right)^{2}}\)</span></li>
</ol>
<p>where <span class="math inline">\(w_{i}\)</span> is width of a street section i and <span class="math inline">\(w_{sp}\)</span> is mean width. The algorithm generates street sections every 3 meters alongside the street segment.</p>
<ol start="21" type="1">
<li><strong>Linearity of a street segment</strong> is denoted as</li>
</ol>
<ol start="21" class="example" type="1">
<li><span class="math inline">\(Lin_{edg} = \frac{l_{eucl}}{l_{edg}}\)</span></li>
</ol>
<p>where <span class="math inline">\(l_{eucl}\)</span> is Euclidean distance between endpoints of a street segment and <span class="math inline">\(l_{edg}\)</span> is a street segment length. It captures the deviation of a segment shape from a straight line. It is adapted from <span class="citation" data-cites="araldi2019">Araldi and Fusco (<a href="#ref-araldi2019" role="doc-biblioref">2019</a>)</span>.</p>
<ol start="22" type="1">
<li><strong>Area covered by a street segment</strong> is denoted as</li>
</ol>
<ol start="22" class="example" type="1">
<li><span class="math inline">\(a_{edg} = \sum_{i=1}^{n} a_{cell_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{cell_i}\)</span> is an area of tessellation cell <span class="math inline">\(i\)</span> belonging to the street segment. It captures the area which is likely served by each segment.</p>
<ol start="23" type="1">
<li><strong>Buildings per meter of a street segment</strong> is denoted as</li>
</ol>
<ol start="23" class="example" type="1">
<li><span class="math inline">\(BpM_{edg} = \frac{\sum blg}{l_{edg}}\)</span></li>
</ol>
<p>where <span class="math inline">\(\sum blg\)</span> is a number of buildings belonging to a street segment and <span class="math inline">\(l_{edg}\)</span> is a length of a street segment. It reflects the granularity of development along each segment.</p>
<ol start="24" type="1">
<li><strong>Area covered by a street node</strong> is denoted as</li>
</ol>
<ol start="24" class="example" type="1">
<li><span class="math inline">\(a_{node} = \sum_{i=1}^{n} a_{cell_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{cell_i}\)</span> is an area of tessellation cell <span class="math inline">\(i\)</span> belonging to the street node. It captures the area which is likely served by each node.</p>
<ol start="25" type="1">
<li><strong>Shared walls ratio of adjacent buildings</strong> is denoted as</li>
</ol>
<ol start="25" class="example" type="1">
<li><span class="math inline">\(SWR_{blg} = \frac{p_{blg_{shared}}}{p_{blg}}\)</span></li>
</ol>
<p>where <span class="math inline">\(p_{blg_{shared}}\)</span> is a length of a perimeter shared with adjacent buildings and <span class="math inline">\(p_{blg}\)</span> is a perimeter of a building. It captures the amount of wall space facing the open space <span class="citation" data-cites="hamaina2012a">(<a href="#ref-hamaina2012a" role="doc-biblioref">Hamaina, Leduc, and Moreau 2012</a>)</span>.</p>
<ol start="26" type="1">
<li><strong>‌Mean distance to neighbouring buildings</strong> is denoted as</li>
</ol>
<ol start="26" class="example" type="1">
<li><span class="math inline">\(NDi_{blg} = \frac{1}{n} \sum_{i=1}^{n} d_{blg, blg_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(d_{blg, blg_i}\)</span> is a distance between building and building <span class="math inline">\(i\)</span> on a neighbouring tessellation cell. It is adapted from <span class="citation" data-cites="hijazi2016">Hijazi et al. (<a href="#ref-hijazi2016" role="doc-biblioref">2016</a>)</span>. It captures the average proximity to other buildings.</p>
<ol start="27" type="1">
<li><strong>Weighted neighbours of a tessellation cell</strong> is denoted as</li>
</ol>
<ol start="27" class="example" type="1">
<li><span class="math inline">\(WNe_{cell} = \frac{\sum cell_n}{p_{cell}}\)</span></li>
</ol>
<p>where <span class="math inline">\(\sum cell_n\)</span> is a number of cell neighbours and <span class="math inline">\(p_{cell}\)</span> is a perimeter of a cell. It reflects granularity of morphological tessellation.</p>
<ol start="28" type="1">
<li><strong>Area covered by neighbouring cells</strong> is denoted as</li>
</ol>
<ol start="28" class="example" type="1">
<li><span class="math inline">\(a_{cell_n} = \sum_{i=1}^{n} a_{cell_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{cell_i}\)</span> is area of tessellation cell <span class="math inline">\(i\)</span> within topological distance 1. It captures the scale of morphological tessellation.</p>
<ol start="29" type="1">
<li><strong>Reached cells by neighbouring segments</strong> is denoted as</li>
</ol>
<ol start="29" class="example" type="1">
<li><span class="math inline">\(RC_{edg_n} = \sum_{i=1}^{n} cells_{edg_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(cells_{edg_i}\)</span> is number of tessellation cells on segment <span class="math inline">\(i\)</span> within topological distance 1. It captures accessible granularity.</p>
<ol start="30" type="1">
<li><strong>Reached area by neighbouring segments</strong> is denoted as</li>
</ol>
<ol start="30" class="example" type="1">
<li><span class="math inline">\(a_{edg_n} = \sum_{i=1}^{n} a_{edg_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{edg_i}\)</span> is an area covered by a street segment <span class="math inline">\(i\)</span> within topological distance 1. It captures an accessible area.</p>
<ol start="31" type="1">
<li><strong>Degree of a street node</strong> is denoted as</li>
</ol>
<ol start="31" class="example" type="1">
<li><span class="math inline">\(deg_{node_i} = \sum_{j} edg_{i j}\)</span></li>
</ol>
<p>where <span class="math inline">\(edg_{i j}\)</span> is an edge of a street network between node <span class="math inline">\(i\)</span> and node <span class="math inline">\(j\)</span>. It reflects the basic degree centrality.</p>
<ol start="32" type="1">
<li><strong>Mean distance to neighbouring nodes from a street node</strong> is denoted as</li>
</ol>
<ol start="32" class="example" type="1">
<li><span class="math inline">\(MDi_{node} = \frac{1}{n} \sum_{i=1}^{n} d_{node, node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(d_{node, node_i}\)</span> is a distance between node and node <span class="math inline">\(i\)</span> within topological distance 1. It captures the average proximity to other nodes.</p>
<ol start="33" type="1">
<li><strong>Reached cells by neighbouring nodes</strong> is denoted as</li>
</ol>
<ol start="33" class="example" type="1">
<li><span class="math inline">\(RC_{node_n} = \sum_{i=1}^{n} cells_{node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(cells_{node_i}\)</span> is number of tessellation cells on node <span class="math inline">\(i\)</span> within topological distance 1. It captures accessible granularity.</p>
<ol start="34" type="1">
<li><strong>Reached area by neighbouring nodes</strong> is denoted as</li>
</ol>
<ol start="34" class="example" type="1">
<li><span class="math inline">\(a_{node_n} = \sum_{i=1}^{n} a_{node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{node_i}\)</span> is an area covered by a street node <span class="math inline">\(i\)</span> within topological distance 1. It captures an accessible area.</p>
<ol start="35" type="1">
<li><strong>Number of courtyards of adjacent buildings</strong> is denoted as</li>
</ol>
<ol start="35" class="example" type="1">
<li><span class="math inline">\(NCo_{blg_{adj}}\)</span></li>
</ol>
<p>where <span class="math inline">\(NCo_{blg_{adj}}\)</span> is a number of interior rings of a polygon composed of footprints of adjacent buildings <span class="citation" data-cites="schirmer2015">(<a href="#ref-schirmer2015" role="doc-biblioref">Schirmer and Axhausen 2015</a>)</span>.</p>
<ol start="36" type="1">
<li><strong>Perimeter wall length of adjacent buildings</strong> is denoted as</li>
</ol>
<ol start="36" class="example" type="1">
<li><span class="math inline">\(p_{blg_{adj}}\)</span></li>
</ol>
<p>where <span class="math inline">\(p_{blg_{adj}}\)</span> is a length of an exterior ring of a polygon composed of footprints of adjacent buildings.</p>
<ol start="37" type="1">
<li><strong>Mean inter-building distance between neighbouring buildings</strong> is denoted as</li>
</ol>
<ol start="37" class="example" type="1">
<li><span class="math inline">\(IBD_{blg} = \frac{1}{n} \sum_{i=1}^{n} d_{blg, blg_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(d_{blg, blg_i}\)</span> is a distance between building and building <span class="math inline">\(i\)</span> on a tessellation cell within topological distance 3. It is adapted from <span class="citation" data-cites="caruso2017">Caruso, Hilal, and Thomas (<a href="#ref-caruso2017" role="doc-biblioref">2017</a>)</span>. It captures the average proximity between buildings.</p>
<ol start="38" type="1">
<li><strong>‌Building adjacency of neighbouring buildings</strong> is denoted as</li>
</ol>
<ol start="38" class="example" type="1">
<li><span class="math inline">\(BuA_{blg} = \frac{\sum blg_{adj}}{\sum blg}\)</span></li>
</ol>
<p>where <span class="math inline">\(\sum blg_{adj}\)</span> is a number of joined built-up structures within topological distance three and <span class="math inline">\(\sum blg\)</span> is a number of buildings within topological distance 3. It is adapted from <span class="citation" data-cites="vanderhaegen2017">Vanderhaegen and Canters (<a href="#ref-vanderhaegen2017" role="doc-biblioref">2017</a>)</span>.</p>
<ol start="39" type="1">
<li><strong>Weighted reached blocks of neighbouring tessellation cells</strong> is denoted as</li>
</ol>
<ol start="39" class="example" type="1">
<li><span class="math inline">\(WRB_{cell} = \frac{\sum blk}{\sum_{i=1}^{n} a_{cell_i}}\)</span></li>
</ol>
<p>where <span class="math inline">\(\sum blk\)</span> is a number of blocks within topological distance three and <span class="math inline">\(a_{cell_i}\)</span> is an area of tessellation cell <span class="math inline">\(i\)</span> within topological distance three.</p>
<ol start="40" type="1">
<li><strong>Local meshedness of a street network</strong> is denoted as</li>
</ol>
<ol start="40" class="example" type="1">
<li><span class="math inline">\(Mes_{node}= \frac{e-v+1}{2 v-5}\)</span></li>
</ol>
<p>where <span class="math inline">\(e\)</span> is a number of edges in a subgraph, and <span class="math inline">\(v\)</span> is the number of nodes in a subgraph <span class="citation" data-cites="feliciotti2018">(<a href="#ref-feliciotti2018" role="doc-biblioref">Feliciotti 2018</a>)</span>. A subgraph is defined as a network within topological distance five around a node.</p>
<ol start="41" type="1">
<li><strong>Mean segment length of a street network</strong> is denoted as</li>
</ol>
<ol start="41" class="example" type="1">
<li><span class="math inline">\(MSL_{edg} = \frac{1}{n} \sum_{i=1}^{n} l_{edg_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(l_{edg_i}\)</span> is a length of a street segment <span class="math inline">\(i\)</span> within a topological distance 3 around a segment.</p>
<ol start="42" type="1">
<li><strong>Cul-de-sac length of a street network</strong> is denoted as</li>
</ol>
<ol start="42" class="example" type="1">
<li><span class="math inline">\(CDL_{node} = \sum_{i=1}^{n} l_{edg_i}, \text { if }edg_i \text { is cul-de-sac}\)</span></li>
</ol>
<p>where <span class="math inline">\(l_{edg_i}\)</span> is a length of a street segment <span class="math inline">\(i\)</span> within a topological distance 3 around a node.</p>
<ol start="43" type="1">
<li><strong>Reached cells by street network segments</strong> is denoted as</li>
</ol>
<ol start="43" class="example" type="1">
<li><span class="math inline">\(RC_{edg} = \sum_{i=1}^{n} cells_{edg_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(cells_{edg_i}\)</span> is number of tessellation cells on segment <span class="math inline">\(i\)</span> within topological distance 3. It captures accessible granularity.</p>
<ol start="44" type="1">
<li><strong>Node density of a street network</strong> is denoted as</li>
</ol>
<ol start="44" class="example" type="1">
<li><span class="math inline">\(D_{node} = \frac{\sum node}{\sum_{i=1}^{n} l_{edg_i}}\)</span></li>
</ol>
<p>where <span class="math inline">\(\sum node\)</span> is a number of nodes within a subgraph and <span class="math inline">\(l_{edg_i}\)</span> is a length of a segment <span class="math inline">\(i\)</span> within a subgraph. A subgraph is defined as a network within topological distance five around a node.</p>
<ol start="45" type="1">
<li><strong>Reached cells by street network nodes</strong> is denoted as</li>
</ol>
<ol start="45" class="example" type="1">
<li><span class="math inline">\(RC_{node_{net}} = \sum_{i=1}^{n} cells_{node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(cells_{node_i}\)</span> is number of tessellation cells on node <span class="math inline">\(i\)</span> within topological distance 3. It captures accessible granularity.</p>
<ol start="46" type="1">
<li><strong>Reached area by street network nodes</strong> is denoted as</li>
</ol>
<ol start="46" class="example" type="1">
<li><span class="math inline">\(a_{node_{net}} = \sum_{i=1}^{n} a_{node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(a_{node_i}\)</span> is an area covered by a street node <span class="math inline">\(i\)</span> within topological distance 3. It captures an accessible area.</p>
<ol start="47" type="1">
<li><strong>Proportion of cul-de-sacs within a street network</strong> is denoted as</li>
</ol>
<ol start="47" class="example" type="1">
<li><span class="math inline">\(pCD_{node} = \frac{\sum_{i=1}^{n} node_i, \text { if }deg_{node_i} = 1}{\sum_{i=1}^{n} node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(node_i\)</span> is a node whiting topological distance five around a node. Adapted from <span class="citation" data-cites="boeing2017a">(<a href="#ref-boeing2017a" role="doc-biblioref">Boeing 2017</a>)</span>.</p>
<ol start="48" type="1">
<li><strong>Proportion of 3-way intersections within a street network</strong> is denoted as</li>
</ol>
<ol start="48" class="example" type="1">
<li><span class="math inline">\(p3W_{node} = \frac{\sum_{i=1}^{n} node_i, \text { if }deg_{node_i} = 3}{\sum_{i=1}^{n} node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(node_i\)</span> is a node whiting topological distance five around a node. Adapted from <span class="citation" data-cites="boeing2017a">(<a href="#ref-boeing2017a" role="doc-biblioref">Boeing 2017</a>)</span>.</p>
<ol start="49" type="1">
<li><strong>Proportion of 4-way intersections within a street network</strong> is denoted as</li>
</ol>
<ol start="49" class="example" type="1">
<li><span class="math inline">\(p4W_{node} = \frac{\sum_{i=1}^{n} node_i, \text { if }deg_{node_i} = 4}{\sum_{i=1}^{n} node_i}\)</span></li>
</ol>
<p>where <span class="math inline">\(node_i\)</span> is a node whiting topological distance five around a node. Adapted from <span class="citation" data-cites="boeing2017a">(<a href="#ref-boeing2017a" role="doc-biblioref">Boeing 2017</a>)</span>.</p>
<ol start="50" type="1">
<li><strong>Weighted node density of a street network</strong> is denoted as</li>
</ol>
<ol start="50" class="example" type="1">
<li><span class="math inline">\(wD_{node} = \frac{\sum_{i=1}^{n} deg_{node_i} - 1}{\sum_{i=1}^{n} l_{edg_i}}\)</span></li>
</ol>
<p>where <span class="math inline">\(deg_{node_i}\)</span> is a degree of a node <span class="math inline">\(i\)</span> within a subgraph and <span class="math inline">\(l_{edg_i}\)</span> is a length of a segment <span class="math inline">\(i\)</span> within a subgraph. A subgraph is defined as a network within topological distance five around a node.</p>
<ol start="51" type="1">
<li><strong>Local closeness centrality of a street network</strong> is denoted as</li>
</ol>
<ol start="51" class="example" type="1">
<li><span class="math inline">\(lCC_{node} = \frac{n - 1}{\sum_{v=1}^{n-1} d(v, u)}\)</span></li>
</ol>
<p>where <span class="math inline">\(d(v, u)\)</span> is the shortest-path distance between <span class="math inline">\(v\)</span> and <span class="math inline">\(u\)</span>, and <span class="math inline">\(n\)</span> is the number of nodes within a subgraph. A subgraph is defined as a network within topological distance five around a node.</p>
<ol start="52" type="1">
<li><strong>Square clustering of a street network</strong> is denoted as</li>
</ol>
<ol start="52" class="example" type="1">
<li><span class="math inline">\(sCl_{node} = \frac{\sum_{u=1}^{k_{v}} \sum_{w=u+1}^{k_{v}} q_{v}(u, w)}{\sum_{u=1}^{k_{v}} \sum_{w=u+1}^{k_{v}}\left[a_{v}(u, w)+q_{v}(u, w)\right]}\)</span></li>
</ol>
<p>where <span class="math inline">\(q_v(u,w)\)</span> are the number of common neighbours of <span class="math inline">\(u\)</span> and <span class="math inline">\(w\)</span> other than <span class="math inline">\(v\)</span> (ie squares), and <span class="math inline">\(a_v(u,w) = (k_u - (1+q_v(u,w)+\theta_{uv}))(k_w - (1+q_v(u,w)+\theta_{uw}))\)</span>, where <span class="math inline">\(\theta_{uw} = 1\)</span> if <span class="math inline">\(u\)</span> and <span class="math inline">\(w\)</span> are connected and 0 otherwise <span class="citation" data-cites="PhysRevE.72.056127">(<a href="#ref-PhysRevE.72.056127" role="doc-biblioref">Lind, González, and Herrmann 2005</a>)</span>.</p>
</section>
</section>
</section>
<section id="target-labels" class="level3">
<h3 class="anchored" data-anchor-id="target-labels">Target labels</h3>
<p>For the second stage, we assign a target classification label to every ETC derived using the satellite-derived polygons. This is done based on spatial intersection between EuroFab and <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> ETCs. In cases where there are multiple detailed tessellation cells that fall within the range of a single EuroFab ETC, the label is decided based on majority.</p>
<p>Since the final output of <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> is a hierarchy, rather than a flat clustering there are several options how to pick the specific target labels. Generally, clusters lower in the hierarchy represent classifications of urban fabrics at more granular scales. For example, depending on the hierarchy cutoff point historical urban areas can be one cluster, or can be separated into two - medieval and industrial-era urban fabrics.</p>
<p>The specific selection of cutoff points will follow <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span>. The first set of urban fabrics we will aim to predict, broadly differentiates - different types of houses; from heterogenous historical urbanised areas; from recent modern urban developments such as apartment blocks and commercial areas; from large industrial areas.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/prague_600.png" class="nostretch figure-img" height="400"></p>
<figcaption>High-level urban fabrics in Prague</figcaption>
</figure>
</div>
<p>The second set breaks down each of the first sets into multiple subsets. It goes into more detail and splits the houses into more classes, based on features such as size and proximity to cities; it also splits the historical areas based on origin - medieval, industrial-era and others; the modern urban developments into subclasses such as different types of modernist apartment blocks, commercial areas, offices and others; and the several industrial area types. By analysing the model performance across two different hierarchical levels, we will understand what is the highest resolution detail the model can predict, given the shortcomings of the data and which factors affect predictions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/prague_400.png" class="nostretch figure-img" height="400"></p>
<figcaption>More detailed urban fabrics in Prague</figcaption>
</figure>
</div>
<!--evaluation setup TODO:align with image-->
</section>
<section id="prediction-modelling-and-traintest-split" class="level3">
<h3 class="anchored" data-anchor-id="prediction-modelling-and-traintest-split">Prediction modelling and train/test split</h3>
<!--why classification and tree based methods-->
<p>The main aim of the modelling task is to generate a classification of morphological elements of similar quality to <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> given the data quality limitations, albeit flat, not hierarchical. To achieve this we create an evaluation framework for the selection of non-linear tree-based models like a random forest classifier or an XGBoost model. We use the satellite-derived buildings, their ETCs and their characteristics as input data and the clusters from <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> as target labels for a classification task. The choice of tree-based learning models is due to their readily available implementations, high scalability and ability to quickly offer interpretation insights. Furthermore, they handle well high dimensional data, non-linear interactions and require minimal hyper-parameter tuning. The flexibility of the models and the specific training/testing framework setup will allow us to not just produce a predictive model but also to identify potential areas for improvement in the original data preprocessing.</p>
<!--out of sample importance-->
<p>Since we want the final production model to be general and applicable to large areas i.e.&nbsp;whole continents, it needs to be able to handle previously unseen urban fabric types. For example, an urban morphology type that is present in the test data, or in another study area, might not be present in the training data and in that case the model should flag its predicted label as uncertain. This is another area where tree models have an advantage, since they are ensemble methods and this can help reduces their tendency to overfit. They also readily provide a confidence score for each prediction which can be used to flag unseen data. Furthermore, we take extra care to evaluate the final production models performance in realistic scenarios and the relationship between its accuracy on test data and whole countries that are not part of the model training or test data.</p>
<!--evaluation setup-->
<p>To achieve this we split the study area into five subsets and train five independent iterations of each model. This is done so that that every country and every combination of countries is used as final hold-out test data and training/validation pipeline respectively. For example, one iteration will use Germany, Poland, Czechia and Austria as part of its training/validation pipeline, whereas Slovakia will be used as the hold-out data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/eval_split.png" class="nostretch figure-img" height="400"></p>
<figcaption>Model evaluation and training setup</figcaption>
</figure>
</div>
<p>This strategy acts as an extra check against overfitting and ultimately enable us to see how the final production model will perform in realistic scenarios - applying it to whole countries which are not used for the training or testing at all. This comes with at least two advantages over simply reporting a test score on a random sample. First, it is a test of model performance on a dataset that does not have any spatial leakage with the training or testing data. Second, it ensures that we evaluate model performance on unseen urban fabric types from other countries. We can afford to do this in part due to the large size of the data we are working with. In every permutation there will be a rich variety of urban fabrics and tens of millions of ETCs used in the model training.</p>
</section>
<section id="training-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="training-and-evaluation">Training and evaluation</h3>
<p>Lastly, after splitting the study area into five subsets, we create a schema that will dictate how to split the training data of each subset for the classification task. We use five-fold cross validation for hyper parameter tuning, based on spatial contiguity. Random subsetting does not work for this study, since we need to account for spatial dependency and the related data leakage between train and evaluation data. The spatial leakage comes from both the nature of the data - spatial contiguity is one of the core aspects of morphological elements - but also from the way characters are calculated based on various nearest topological neighbours.</p>
<p>To account for this, we aggregate nearby ETCs into higher granularity spatial units - level 7 H3 cells - and randomly split these units into five groups, to carry out the cross-validation training. This ensures that the majority of the ETCs and their neighbours in one set are not present in the other sets, and therefore spatial leakage is minimised. We use level 7 H3 cells, which represent a delineation of the globe into hexagons with an area of approximately 5 sq. km. , rather than enclosures or ETC contiguity, to ensure that contiguous subsets of test data cover areas of heterogenous elements and present the model with a realistic validation scenario.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/train_test_prague.png" class="nostretch figure-img" height="400"></p>
<figcaption>Example iteration in a five-fold cross validation split. Only one group is highligted in blue, with its ETCs plotted in green, which will be used as a hold-out data for accuracy evaluation and hyper parameter tuning.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/train_test_prague_zoom2.png" class="nostretch figure-img" height="400"></p>
<figcaption>Example hexagon in the validation split</figcaption>
</figure>
</div>
<p>The model training and evaluation will follow standard best practices - model coefficients and hyper parameter tuning, such as the decision threshold will be optimised based on the training subset, and all of the data in the hold-out country will be used to give a final model accuracy score. Specifically, the models will use balanced accuracy as the optimisation metric in order to account for imbalances in the distribution of urban fabric classes. The extra validation steps we carry out with the hold-out countries will be used to used in three ways. First, to contextualise the final models’ accuracy on the test data; second, to indicate how the model will perform on other countries; and third to see how it handles urban fabric types not seen in the training in a realistic scenario.</p>
<p>The final production model is trained on the whole dataset, using the same hyper parameter grid search configuration and training/test spatial split.</p>
</section>
<section id="preliminary-results" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-results">Preliminary results</h3>
<p>We have implemented a preliminary pipeline that carries out full data preprocessing - generating morphological elements and characters, assigning target labels and some exploratory modelling. The core functionality for all of this was made available within open-source packages - <code>momepy</code>, <code>libpysal</code>, <code>sgeop</code> (the name of which may eventually change).</p>
<p>Based on the preliminary results, there are 56,845,150 Microsoft building footprints for our study area, which are split into 474 subregions. This is significantly less than the available cadastre data, which has around 88 million buildings and are separated into 828 regions. The number of downloaded, unprocessed streets is similar to those in <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref">Fleischmann and Samardzhiev Forthcoming</a>)</span> - 23,332,865 - since they cover the same study area and come from the same source - Overture Maps, which is a processed subset of OpenStreepMap. However, the number of tessellation cells is the same as the buildings and therefore less than the cadastre data-based classification. Furthermore, the street simplification algorithm is affected by the available buildings, and therefore in turn also affects the tessellation cell boundaries.</p>
<p>These results highlight the effect of the satellite derived building footprints that have been discussed in the Technical Note D3 . There are significantly less subregions in the study area primarily due to the effect of the threshold of 10k buildings required for a region. As the adjacent buildings tend to be merged, the algorithm needs to cover larger area before reaching the threshold, resulting in the lower number of regions. However, as the region split is purely procedural step allowing efficient processing of data, this is not an issue of any sort.</p>
<!--preliminary modelling-->
<p>As a first modelling step we tried a random forest (RF) classifier on a subset of the data covering the region surrounding Prague. The goal was to evaluate the project workflow and the feasibility of the proposed model architecture, rather than the specific model’s performance. We trained and tested the same simple RF model on the data within the same region, split in different ways - one was based on stratified spatial k-fold train/test splits (our proposed setup) and another based on random train/test splits. The latter model with random sampling had an accuracy of 0.95 versus an accuracy of 0.68 for the former model with spatial stratification. The difference in accuracy highlights the extent of spatial leakage of information and the need for the proposed spatially explicit train/test/validation split of the data. Otherwise, the performance of the production model would be significantly lower than what the training data suggest. In any case, the relatively high accuracy score of the models hint towards the viability of predicting urban fabric types.</p>
<p>In total, the results point towards two things. First, they further show need for a non-linear classification model, cable of accounting for the discrepancies between satellite-derived building footprint and cadastral data. Second, the utility of our designed framework to account for spatial leakage and evaluate model performance in more realistic scenarios.</p>
</section>
<section id="limitations-potential-problems" class="level3">
<h3 class="anchored" data-anchor-id="limitations-potential-problems">Limitations &amp; Potential problems</h3>
<p>One limitation of the approach is its reliance on primarily European data for training and evaluation. Therefore, the model may struggle to provide meaningful classifications in other contexts. There is no equivalent in the training data for the vast urban sprawl patterns present in the United States, for example. Nevertheless, there are urban fabric types present in the model which are ubiqutous in a lot of non-european contexts, such as communist planned areas in Asia, historical city centres, single housing areas and others.</p>
<p>A potential problem is class disbalance when training the model and afterwas when calculating its accuracy. If such an issue is present in the country split experiments, we aim to address it by undersampling the target classes. We can afford to undersample, rather than oversample, since our data covers a large geograpical area and each class has tens of thousands of instances at the very least.</p>
</section>
</section>
<section id="ai-modelling-using-satellite-imagery-1" class="level2">
<h2 class="anchored" data-anchor-id="ai-modelling-using-satellite-imagery-1">AI Modelling using Satellite Imagery</h2>
<p>In satellite image analysis, classification and segmentation address spatial labelling at different levels of granularity, with classification assigning a single label to an image tile or cell, while segmentation provides pixel-level detail. In our study, the label dataset does not always correspond directly to identifiable features in the imagery, making classification a potentially more suitable approach as it generalises each tile’s dominant land cover type without requiring exact pixel alignment. However, we explore both approaches: classification for a tile-based analysis and segmentation for finer, boundary-specific mapping. This dual approach enables us to evaluate how each method performs given the scale and nature of the dataset.</p>
<section id="data-preprocessing-1" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-1">Data preprocessing</h3>
<p>For our analysis, we employ two distinct datasets of image tiles at varying scales. These datasets enable us to evaluate both segmentation and classification tasks for urban fabric prediction. We choose a larger tile size for the segmentation task since most segmentation models work better with conventional image sizes (such as 224 x 224 pixels) and they are also a lot more efficient since the dataset is not as large. For classification, the tile size represents the scale of the analysis and for that reason we chose a smaller tile size of 56x56 pixels.</p>
<ul>
<li><strong>Segmentation Dataset</strong>: Comprising 26,753 tiles, each 224 x 224 pixels (covering 2240 x 2240 meters). Of these, 21,402 tiles are allocated for training, and 5,351 for testing.</li>
<li><strong>Classification Dataset</strong>: Comprising 403,722 tiles, each 56 x 56 pixels (covering 560 x 560 meters). The training set consists of 342,648 tiles, with the remaining 61,074 reserved for testing.</li>
</ul>
<p>To ensure consistency across both tasks, we exclusively use tiles that fully overlap with the spatial signature labels. This alignment facilitates robust pixel-level comparisons of classification and segmentation outcomes while maintaining compatibility with our urban fabric typology as shown below.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../figures/algo_design/sampling.png" class="nostretch quarto-figure quarto-figure-left figure-img" height="400"></p>
</figure>
</div>
<section id="unbalanced-dataset" class="level4">
<h4 class="anchored" data-anchor-id="unbalanced-dataset">Unbalanced dataset</h4>
<p>A significant challenge in our dataset is class imbalance, where certain urban fabric types are much more prevalent than others. This imbalance required careful consideration in model design and loss function selection, prompting us to explore approaches that could better handle uneven class distributions.</p>
<p>The figure below visualizes the class distribution of spatial signatures, highlighting the imbalance across different urban fabric types. Notably, the countryside agriculture and wild countryside classes are more dominant compared to the more urban-centric classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/unbalanced.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
<section id="traintest-split" class="level4">
<h4 class="anchored" data-anchor-id="traintest-split">Train/test split</h4>
<p>The dataset is divided into 80% for training and 20% for testing across both tasks. The segmentation and classification datasets share the same test samples, which helps make the results more comparable and allows us to evaluate performance across both tasks at the same time.</p>
<p>The figures below show how the training and testing datasets sampled across the whole study area:</p>
<p><img src="../figures/algo_design/train_df.png" class="nostretch quarto-figure quarto-figure-left" height="400"> <img src="../figures/algo_design/test_df.png" class="nostretch quarto-figure quarto-figure-right" height="400"></p>
<p>We used standard, pre-configured neural network setups without tuning the hyperparameters, due to the constraints of the project. As a result, we did not include a validation split in these experiments.</p>
</section>
</section>
<section id="model-architectures" class="level3">
<h3 class="anchored" data-anchor-id="model-architectures">Model architectures</h3>
<p>To assess the performance of different AI models for urban fabric classification and segmentation, we designed three distinct experimental approaches. Each approach leverages different combinations of pre-trained models and fine-tuning strategies to evaluate their ability to accurately classify and segment urban fabric types. The following experiments were carried out to explore the effectiveness of both image embeddings and geospatial foundation models in addressing the challenges posed by urban fabric analysis:</p>
<p>We conducted three main experiments as part of the AI model design to analyze urban fabric classification and segmentation.</p>
<ul>
<li><p>Approach A (<em>Embedding approach</em>): We start with a baseline experiment, where we generate image embeddings using the SatlasPretrain model. These embeddings are then fed into an XGBoost classifier to predict urban fabric classes.</p></li>
<li><p>Approach B (<em>Segmentation approach</em>): Next, we fine-tune three different geospatial foundation models—SatlasPretrain, Clay, and IBM/NASA’s Prithvi model—specifically for segmentation tasks.</p></li>
<li><p>Approach C (<em>Classification approach</em>): Finally, we take the best-performing geospatial foundation model from the segmentation experiments (Clay) and fine-tune it for the classification task.</p></li>
</ul>
<p>To evaluate and compare the results of these approaches, we report weighted pixel-level accuracy, F1 score, and Intersection over Union (IoU) metrics.</p>
<section id="baseline-embedding-approach-approach-a" class="level4">
<h4 class="anchored" data-anchor-id="baseline-embedding-approach-approach-a">Baseline embedding approach (Approach A)</h4>
<p>In the first experiment, we implement a baseline approach using image embeddings created by a geospatial foundation model, followed by classification with an XGBoost model. This approach is computationally efficient and easy to implement, making it a good starting point for comparison. Once the embeddings are generated, they can be directly input into a machine learning (ML) model for classification.</p>
<p>The tiles are processed by the SatlasPretrain model <span class="citation" data-cites="bastani2023satlaspretrain">(<a href="#ref-bastani2023satlaspretrain" role="doc-biblioref">Bastani et al. 2023</a>)</span>, a geospatial foundation model pretrained on more than 302 million labels from remote sensing and computer vision tasks. We chose this model because it was specifically trained on Sentinel-2 images, making it a good fit for our dataset.</p>
<p>The model works in two steps:</p>
<ul>
<li>Foundation Model: We use a Vision Transformer (ViT) with a Feature Pyramid Network (FPN) and a pooling layer to generate image embeddings. These embeddings are lower-dimensional representations of the images.</li>
<li>Machine Learning Classifier: The generated embeddings are then passed into an XGBoost classifier, which predicts the urban fabric classes across England.</li>
</ul>
<p>The diagram below illustrates this baseline approach: <img src="../figures/algo_design/baseline.png" class="nostretch quarto-figure quarto-figure-center" height="400"></p>
<p><em>Baseline approach (ordinal)</em></p>
<p>In addition to the basic classification task, we also explored an ordinal regression approach. Since the urban fabric classes represent a continuum rather than strictly categorical data, this approach accounts for the ordering between the classes. The following ordinal mapping was applied to model the spatial signatures:</p>
<p><code>ordinal_mapping = {     'Wild countryside': 0,     'Countryside agriculture': 1,     'Urban buffer': 2,     'Open sprawl': 3,     'Disconnected suburbia': 4,     'Accessible suburbia': 5,     'Warehouse/Park land': 6,     'Gridded residential quarters': 7,     'Connected residential neighbourhoods': 8,     'Dense residential neighbourhoods': 9,     'Dense urban neighbourhoods': 10,     'Urbanity': 11, }</code></p>
<p>Using this ordinal mapping, the model achieved a Mean Absolute Error (MAE) and Mean Squared Error (MSE) of 0.28, with an R² score of 0.62. The Sankey diagram below shows the main misclassifications, which typically occur between similar urban fabric types.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/sankey.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
<p>For pixel-level comparison, we round the predicted values to the closest class and report them in the overview in the Preliminary results section.</p>
<p><em>Baseline approach + spatial context</em></p>
<p>To further improve model performance, we added spatial context by including regional geographical information to the predictive model. We thus added the regional H3 resolution 5 code as categorical variable to the machine learning models. The visualisation below shows the hexagons plotted on top of England.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/hex_level5.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
<section id="segmentation-approach-b" class="level4">
<h4 class="anchored" data-anchor-id="segmentation-approach-b">Segmentation (Approach B)</h4>
<p>In this section, we explore segmentation using a fine-tuned geospatial foundation model. We trained three state-of-the-art models on 224x224x3 image tiles to classify urban fabric types at a pixel level. Each model was fine-tuned for 10 epochs, and we evaluated their performance using key metrics. The models we tested vary in architecture and dataset size, as summarized below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Architecture</th>
<th>Dataset Size</th>
<th>Image Sources</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Satlas <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></td>
<td>SwinT</td>
<td>302M labels</td>
<td>Sentinel-2</td>
</tr>
<tr class="even">
<td>Clay <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></td>
<td>MAE/ViT</td>
<td>70M labels</td>
<td>Multiple+</td>
</tr>
<tr class="odd">
<td>Prithvi <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></td>
<td>MAE/ViT</td>
<td>250 PB</td>
<td>Sentinel-2/Landsat</td>
</tr>
</tbody>
</table>
<p>+Multiple sources include Sentinel-2, Landsat, NAIP, and LINZ</p>
<p>These models differ mainly in their backbone architecture and the datasets they were pretrained on, which impacts their ability to capture different spatial and spectral features from the input images.</p>
<p>The following visualisations show the varying model configurations for the three different approaches tested for the segmentation task. The main difference is the varying backbone.</p>
</section>
<section id="model-1-satlas" class="level4">
<h4 class="anchored" data-anchor-id="model-1-satlas">Model 1: Satlas</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/satlas_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
</section>
<section id="model-2-clay" class="level4">
<h4 class="anchored" data-anchor-id="model-2-clay">Model 2: Clay</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/clay_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
</section>
<section id="model-3-prithvi" class="level4">
<h4 class="anchored" data-anchor-id="model-3-prithvi">Model 3: Prithvi</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/prithvi_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<p>After fine-tuning each model for 10 epochs, we compared their performance based on weighted accuracy, Intersection over Union (IoU), and F1 score, among other metrics. The table below summarizes the results of the segementation model comparison:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Satlas</th>
<th>Clay</th>
<th>Prithvi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weighted Accuracy</td>
<td>0.57</td>
<td><strong>0.72</strong></td>
<td>0.62</td>
</tr>
<tr class="even">
<td>Weighted IoU</td>
<td>0.33</td>
<td><strong>0.58</strong></td>
<td>0.41</td>
</tr>
<tr class="odd">
<td>Weighted F1</td>
<td>0.41</td>
<td><strong>0.69</strong></td>
<td>0.58</td>
</tr>
<tr class="even">
<td>Training Time/Epoch</td>
<td>9 mins</td>
<td>8 mins</td>
<td>20 mins</td>
</tr>
<tr class="odd">
<td>Parameters</td>
<td>90M</td>
<td>86M</td>
<td>120M</td>
</tr>
<tr class="even">
<td>Implementation Score</td>
<td>5/10</td>
<td>6/10</td>
<td>7/10</td>
</tr>
</tbody>
</table>
<p>The Clay model outperformed the others across all metrics, demonstrating the best performance in terms of weighted accuracy, IoU, and F1 score, while maintaining reasonable training times and computational efficiency.</p>
<p>The choice of loss function played a crucial role in the performance of the models. We found that focal loss was particularly effective in handling class imbalance, a common challenge in geospatial datasets. When applied with the Clay model, this loss function led to significant improvements in segmentation accuracy, especially for underrepresented urban fabric classes.</p>
</section>
<section id="classification-approach-c" class="level4">
<h4 class="anchored" data-anchor-id="classification-approach-c">Classification (Approach C)</h4>
<p>In Approach C, we focused on fine-tuning a geospatial foundation model for a classification task. For this, we used the smaller 56x56x3 image tiles as input. Based on the promising results from the segmentation experiments (Approach B), we chose to use the Clay model as the backbone for this classification task, as it consistently outperformed the other models across key metrics.</p>
<p>The figure below compares the predicted urban fabric classes from the fine-tuned geospatial foundation model in both the segmentation (Approach B) and classification (Approach C) tasks. This visual comparison highlights the differences in the model’s performance and the class predictions between these two approaches.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/comparison_B_C.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<p>While the classification approach (Approach C) tends to overpredict the dominant class, the segmentation output from Approach B faces challenges in representing useful shapes for classes with fewer examples. These differences highlight the trade-offs in model performance across tasks with varying data distributions. Additionally, this could suggest that some spatial signatures lack clear boundaries on the ground, making it difficult for the segmentation algorithm to accurately detect borders between classes. This insight underscores the complexities of applying segmentation techniques to spatial data with ambiguous or overlapping class boundaries.</p>
</section>
</section>
<section id="evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h3>
<p>To comprehensively evaluate the performance of our models, we used several key metrics that capture different aspects of model performance:</p>
<ul>
<li><p>Intersection over Union (IoU): This metric quantifies the overlap between predicted and ground truth segmentations. It ranges from 0 (no overlap) to 1 (perfect overlap). IoU is calculated by dividing the area of intersection by the area of the union between the predicted and actual segmentation masks.</p></li>
<li><p>Weighted F1 Score: The F1 score is the harmonic mean of precision and recall, offering a balanced measure of both. The weighted F1 score adjusts for class imbalances by giving more importance to classes with fewer examples. Precision measures how many of the predicted positives are correct, while recall indicates how many of the actual positives were correctly identified.</p></li>
<li><p>Weighted Accuracy: This metric measures the overall proportion of correct predictions, adjusted by class frequencies to address class imbalance. It provides a more representative performance measure by considering the prevalence of each class in the dataset.</p></li>
</ul>
</section>
<section id="preliminary-results-1" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-results-1">Preliminary results</h3>
<p>Comparing model results directly can be challenging due to differences in image tile sizes and overlap (e.g., 56px vs.&nbsp;224px). To ensure a fair comparison, we calculate pixel-level accuracy scores for each approach. Specifically, we predict the full map for the test set, compare overlapping tiles (as described in the sampling method), and compute the following metrics on a per-pixel basis.</p>
<section id="overall-model-performance-comparison-pixel-level" class="level4">
<h4 class="anchored" data-anchor-id="overall-model-performance-comparison-pixel-level">Overall model performance comparison (Pixel-level)</h4>
<p>Our evaluation across the different approaches showed varying levels of performance. Below is a summary of the performance metrics for each approach:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 17%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Global Accuracy</th>
<th>Macro Accuracy</th>
<th>F1 Score</th>
<th>IoU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A: Classification (embeddings)</td>
<td>0.76 (0.66)</td>
<td>0.22 (0.13)</td>
<td>0.23</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>A: Classification + H3 level 5</td>
<td><strong>0.87</strong> (0.82)</td>
<td><strong>0.42</strong> (0.35)</td>
<td><strong>0.45</strong></td>
<td><strong>0.79</strong></td>
</tr>
<tr class="odd">
<td>A: Classification + H3 ordinal</td>
<td>0.80 (0.80)</td>
<td>0.26 (0.26)</td>
<td>0.26</td>
<td>0.69</td>
</tr>
<tr class="even">
<td>B: Segmentation (Clay)</td>
<td>0.73</td>
<td>0.31</td>
<td>0.30</td>
<td>0.58</td>
</tr>
<tr class="odd">
<td>C: Classification (Clay)</td>
<td>0.59 (0.68)</td>
<td>0.09</td>
<td>0.12</td>
<td>0.38</td>
</tr>
</tbody>
</table>
<p>The results in brackets represent the tile-level accuracy, which is typically reported in classification tasks. However, to facilitate more meaningful comparisons across approaches, we use pixel-level accuracy for all experiments.</p>
<p><strong>Key observations</strong></p>
<ul>
<li><p>The baseline classification approaches showed varied results:</p>
<ul>
<li>The basic embedding classification approach achieved a global accuracy of 76% (22% balanced), reflecting a strong initial performance.</li>
<li>When incorporating regional trends, performance improved significantly, with a global accuracy of 87% (42% balanced), suggesting that regional context plays a critical role in improving classification accuracy.</li>
<li>The H3 Level 5 ordinal classification approach also performed well, with an accuracy of 80% (26% balanced), but it lagged behind in balancing the performance across classes.</li>
</ul></li>
<li><p>The fine-tuned geospatial foundation model performed better than the fine-tuned classification models, achieving an accuracy of 0.73 compared to 0.56 for the classification model (Clay model).</p></li>
<li><p>Overall, the classification approach with regional information (H3 Level 5) yielded the best performance, achieving both high accuracy and a reasonable balance across classes. Additionally, this approach is computationally efficient: once the image embeddings are generated, the downstream classification process can be completed in just a few minutes.</p></li>
</ul>
</section>
<section id="prediction-example-london" class="level4">
<h4 class="anchored" data-anchor-id="prediction-example-london">Prediction example: London</h4>
<p>To showcase the practical application of the model, we used it to make predictions on a map of London. This example uses the model from Approach A, which incorporates regional trends through H3 categories, and generates predictions across the entire country. The figure below presents a sample prediction for the London area, where each color represents a different spatial signature, and the background color corresponds to the ground truth.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/results_eurofab.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
</section>
<section id="x25-grid-classification-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="x25-grid-classification-pipeline">25x25 grid classification pipeline</h3>
<p>Following the initial analysis, which demonstrated that the embeddings-based approach yielded the most favorable results, we extended the classification pipeline to include testing with a finer grid resolution. Through team discussions, we determined that a grid size of 25x25 pixels (corresponding to 250x250 meters on the ground) is particularly well-suited for downstream planning applications. This smaller grid size not only aligns better with practical use cases but also led to improvements in key performance metrics such as the overall weighted F1 score and macro accuracy. However, the overall accuracy is a bit lower, which is for our application not as important.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 35%">
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Tile size</strong></th>
<th><strong>Model</strong></th>
<th><strong>Global Accuracy</strong></th>
<th><strong>MACRO Accuracy</strong></th>
<th><strong>F1 Score (balanced)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>56x56x3</strong></td>
<td>Classification (embeddings)</td>
<td>0.76 (0.66)</td>
<td>0.22 (0.13)</td>
<td>0.23</td>
</tr>
<tr class="even">
<td><strong>56x56x3</strong></td>
<td>Classification (embeddings) + H3 level 5 (cat)</td>
<td><strong>0.87 (0.82)</strong></td>
<td><strong>0.42 (0.35)</strong></td>
<td><strong>0.45</strong></td>
</tr>
<tr class="odd">
<td><strong>56x56x3</strong></td>
<td>Classification (embeddings) + H3 level 5 (lat/lon)</td>
<td><strong>0.87 (0.81)</strong></td>
<td>0.39 (0.31)</td>
<td>0.42</td>
</tr>
<tr class="even">
<td><strong>56x56x3</strong></td>
<td>Classification (embeddings) + H3 level 5 ordinal</td>
<td>0.80 (0.80)</td>
<td>0.26 (0.26)</td>
<td>0.26</td>
</tr>
<tr class="odd">
<td><strong>25x25x3</strong></td>
<td>Classification (embeddings)</td>
<td>0.73</td>
<td>0.31</td>
<td>0.30</td>
</tr>
<tr class="even">
<td><strong>25x25x3</strong></td>
<td>Classification (embeddings) + H3 level 5 (lat/lon)</td>
<td><strong>0.81</strong></td>
<td>0.46</td>
<td><strong>0.53</strong></td>
</tr>
<tr class="odd">
<td><strong>25x25x3</strong></td>
<td>Classification (embeddings) + lat/lon</td>
<td>0.89</td>
<td>0.71</td>
<td>0.78</td>
</tr>
<tr class="even">
<td><strong>25x25x3</strong></td>
<td>Lat/lon</td>
<td>0.91</td>
<td>0.78</td>
<td>0.83</td>
</tr>
</tbody>
</table>
</section>
<section id="sampling-experiments" class="level3">
<h3 class="anchored" data-anchor-id="sampling-experiments">Sampling experiments</h3>
<p>We also evaluated random sampling and H3 resolution 3 regional sampling to assess their impact on spatial generalization and F1-score performance. While random sampling ensures diversity and captures localized patterns, it risks spatial leakage, potentially inflating performance metrics. In contrast, H3-based regional sampling reduces spatial leakage and offers a more realistic evaluation of generalization but can suffer from unfair penalization due to the heterogeneity of regions.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Random</th>
<th>H3 split (resolution 3)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ensures that the training and testing datasets include diverse samples from all regions, including smaller, localized patterns that might not appear in every larger region.</td>
<td>Could lead to under-sampling or over-representation of certain spatial signature types if these types are not evenly distributed across regions.</td>
</tr>
<tr class="even">
<td>Increased risk of spatial leakage: test samples may be geographically close to training samples, leading to overestimated performance because the model effectively sees similar data in training and testing.</td>
<td>Minimizes spatial leakage by ensuring that test regions are distinct from training regions. This gives a more realistic estimate of how the model will generalize to new, unseen regions.</td>
</tr>
<tr class="odd">
<td>Random sampling (diversity) benefits the training process but risks overestimating performance due to leakage.</td>
<td>Regional splitting (independence) gives a clearer picture of spatial generalization but could penalize the model unfairly if regions are too internally heterogeneous.</td>
</tr>
<tr class="even">
<td></td>
<td>Would need to be repeated across k-folds for possibly ‘fairer’ evaluation.</td>
</tr>
</tbody>
</table>
<section id="model-choice-based-on-objective" class="level4">
<h4 class="anchored" data-anchor-id="model-choice-based-on-objective">Model choice based on objective</h4>
<p>Goal: - If the goal is to predict locally, then random sampling might align better with your objectives, as it focuses on learning detailed local variations. - If the goal is to predict regionally or globally, regional splitting is more suitable because it ensures the model learns broader generalization patterns.</p>
<p>–&gt; Deployment on all data in the end; pipeline will look the same in the end (sampling only for reporting)</p>
</section>
<section id="sampling-results" class="level4">
<h4 class="anchored" data-anchor-id="sampling-results">Sampling results</h4>
<p>H3 regional sampling showed slightly lower performance, hinting to some spatial leakage through random sampling.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 5%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Approach</strong></th>
<th><strong>Res.</strong></th>
<th><strong>Sampling</strong></th>
<th><strong>Clas/seg</strong></th>
<th><strong>Regional info</strong></th>
<th><strong>Global Acc</strong></th>
<th><strong>Macro Acc</strong></th>
<th><strong>F1(macro)</strong></th>
<th><strong>IoU</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td></td>
<td>0.76 (0.66)</td>
<td>0.22 (0.13)</td>
<td>0.23</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td>H3 res 5 (cat)</td>
<td><strong>0.87 (0.82)</strong></td>
<td><strong>0.42 (0.35)</strong></td>
<td><strong>0.45</strong></td>
<td><strong>0.79</strong></td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>classification</td>
<td>H3 res 5 (lat/lon)</td>
<td><strong>0.87 (0.81)</strong></td>
<td>0.39 (0.31)</td>
<td>0.42</td>
<td>0.78</td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>56x56</td>
<td>random</td>
<td>regression (ordinal)</td>
<td>H3 res 5 (lat/lon)</td>
<td>0.80 (0.80)</td>
<td>0.26 (0.26)</td>
<td>0.26</td>
<td>0.69</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>random</td>
<td>classification</td>
<td></td>
<td>(0.73)</td>
<td>(0.31)</td>
<td>(0.3)</td>
<td></td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>25x25</td>
<td>random</td>
<td>classification</td>
<td></td>
<td><strong>(0.81)</strong></td>
<td><strong>(0.46)</strong></td>
<td><strong>(0.53)</strong></td>
<td></td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>random</td>
<td>classification</td>
<td>lat/lon</td>
<td>(0.89)</td>
<td>(0.71)</td>
<td>(0.78)</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>25x25</td>
<td>random</td>
<td></td>
<td>lat/lon</td>
<td>(0.91)</td>
<td>(0.78)</td>
<td>(0.83)</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>25x25</td>
<td>H3 res 3 (55,743)</td>
<td>A (embeddings)</td>
<td>H3 res 5 (lat/lon)</td>
<td>(0.58)</td>
<td>(0.15)</td>
<td>(0.15)</td>
<td></td>
</tr>
<tr class="even">
<td>A (embeddings)</td>
<td>25x25</td>
<td>H3 res 5 (2,125)</td>
<td>A (embeddings)</td>
<td>H3 res 5 (lat/lon)</td>
<td>(0.65)</td>
<td>(0.2)</td>
<td>(0.21)</td>
<td></td>
</tr>
<tr class="odd">
<td>A (embeddings)</td>
<td>25x25</td>
<td>H3 res 6 (335)</td>
<td>A (embeddings)</td>
<td>H3 res 5 (lat/lon)</td>
<td><strong>(0.72)</strong></td>
<td><strong>(0.29)</strong></td>
<td><strong>(0.32)</strong></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="limitations" class="level4">
<h4 class="anchored" data-anchor-id="limitations">Limitations</h4>
<ul>
<li>Data
<ul>
<li>class imbalance:
<ul>
<li>classification with models with oversampling/undersampling</li>
<li>we fit a weighted (add weights based on distribution) XGBoost model did not give better results</li>
<li>potential way forward if continues to be a problem: join classes together</li>
</ul></li>
</ul></li>
<li>Model constraints and generalisability
<ul>
<li>generalisability to other European countries
<ul>
<li>final model will be fit on urban form –&gt; hopefully improve the model
<ul>
<li>more data points of classes with small number of instances</li>
</ul></li>
<li>sampling across different countries, the final model will not just be trained on England but on a range of sample countries, so we are not anticipating any issues with generalisability</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
</section>
</section>
<section id="references" class="level1 unnumbered">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-araldi2019" class="csl-entry" role="listitem">
Araldi, Alessandro, and Giovanni Fusco. 2019. <span>“From the Street to the Metropolitan Region: <span>Pedestrian</span> Perspective in Urban Fabric Analysis:”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 46 (7): 1243–63. <a href="https://doi.org/10.1177/2399808319832612">https://doi.org/10.1177/2399808319832612</a>.
</div>
<div id="ref-arribas2022spatial" class="csl-entry" role="listitem">
Arribas-Bel, Daniel, and Martin Fleischmann. 2022. <span>“Spatial Signatures-Understanding (Urban) Spaces Through Form and Function.”</span> <em>Habitat International</em> 128: 102641.
</div>
<div id="ref-basaraner2017" class="csl-entry" role="listitem">
Basaraner, Melih, and Sinan Cetinkaya. 2017. <span>“Performance of Shape Indices and Classification Schemes for Characterising Perceptual Shape Complexity of Building Footprints in <span>GIS</span>.”</span> <em>International Journal of Geographical Information Science</em> 31 (10): 1952–77. <a href="https://doi.org/10.1080/13658816.2017.1346257">https://doi.org/10.1080/13658816.2017.1346257</a>.
</div>
<div id="ref-bastani2023satlaspretrain" class="csl-entry" role="listitem">
Bastani, Favyen, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. 2023. <span>“<span>SatlasPretrain</span>: <span>A</span> <span>Large</span>-<span>Scale</span> <span>Dataset</span> for <span>Remote</span> <span>Sensing</span> <span>Image</span> <span>Understanding</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2211.15660">http://arxiv.org/abs/2211.15660</a>.
</div>
<div id="ref-boeing2017a" class="csl-entry" role="listitem">
Boeing, Geoff. 2017. <span>“<span>OSMnx</span>: <span>New</span> Methods for Acquiring, Constructing, Analyzing, and Visualizing Complex Street Networks.”</span> <em>Computers, Environment and Urban Systems</em> 65 (September): 126–39. <a href="https://doi.org/10/gbvjxq">https://doi.org/10/gbvjxq</a>.
</div>
<div id="ref-calafiore2023inequalities" class="csl-entry" role="listitem">
Calafiore, Alessia, Krasen Samardzhiev, Francisco Rowe, Martin Fleischmann, and Daniel Arribas-Bel. 2023. <span>“Inequalities in Experiencing Urban Functions. An Exploration of Human Digital (Geo-) Footprints.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em>, 23998083231208507.
</div>
<div id="ref-caruso2017" class="csl-entry" role="listitem">
Caruso, Geoffrey, Mohamed Hilal, and Isabelle Thomas. 2017. <span>“Measuring Urban Forms from Inter-Building Distances: <span>Combining MST</span> Graphs with a <span>Local Index</span> of <span>Spatial Association</span>.”</span> <em>Landscape and Urban Planning</em> 163 (July): 80–89. <a href="https://doi.org/10.1016/j.landurbplan.2017.03.003">https://doi.org/10.1016/j.landurbplan.2017.03.003</a>.
</div>
<div id="ref-dibble2015" class="csl-entry" role="listitem">
Dibble, Jacob, Alexios Prelorendjos, Ombretta Romice, Mattia Zanella, Emanuele Strano, Mark Pagel, and Sergio Porta. 2015. <span>“Urban <span>Morphometrics</span>: <span>Towards</span> a <span>Science</span> of <span>Urban Evolution</span>.”</span> <em>arXiv.org</em> physics.soc-ph (June).
</div>
<div id="ref-esch2010how" class="csl-entry" role="listitem">
Esch, Thomas, Hannes Taubenböck, Wieke Heldens, Michael Thiel, Michael Wurm, and Stefan Dech. 2010. <span>“How Can Earth Observation Support the Sustainable Development of Urban Environments?”</span> <em>Urban Remote Sensing</em>.
</div>
<div id="ref-feliciotti2018" class="csl-entry" role="listitem">
Feliciotti, Alessandra. 2018. <span>“<span>RESILIENCE AND URBAN DESIGN</span>: <span>A SYSTEMS APPROACH TO THE STUDY OF RESILIENCE IN URBAN FORM</span>.”</span> PhD thesis, <span>Glasgow</span>: University of Strathclyde.
</div>
<div id="ref-fleischmann2022methodological" class="csl-entry" role="listitem">
Fleischmann, Martin, Alessandra Feliciotti, Ombretta Romice, and Sergio Porta. 2022. <span>“Methodological Foundation of a Numerical Taxonomy of Urban Form.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 49 (4): 1283–99.
</div>
<div id="ref-fleischmann2021measuring" class="csl-entry" role="listitem">
Fleischmann, Martin, Ombretta Romice, and Sergio Porta. 2021. <span>“Measuring Urban Form: Overcoming Terminological Inconsistencies for a Quantitative and Comprehensive Morphologic Analysis of Cities.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 48 (8): 2133–50.
</div>
<div id="ref-primus" class="csl-entry" role="listitem">
Fleischmann, Martin, and Krasen Samardzhiev. Forthcoming. <span>“Numerical Taxonomy of Urban Fabric in Central Europe,”</span> Forthcoming.
</div>
<div id="ref-fleischmann2024Shapebased" class="csl-entry" role="listitem">
Fleischmann, Martin, and Anastassia Vybornova. 2024. <span>“A Shape-Based Heuristic for the Detection of Urban Block Artifacts in Street Networks.”</span> <em>Journal of Spatial Information Science</em> 28 (June): 75–102. <a href="https://doi.org/10.5311/JOSIS.2024.28.319">https://doi.org/10.5311/JOSIS.2024.28.319</a>.
</div>
<div id="ref-sgeop" class="csl-entry" role="listitem">
Fleischmann, Martin, Anastassia Vybornova, and James Gaboardi. Forthcoming. <span>“Continuity-Preserving Simplification of Street Networks,”</span> Forthcoming.
</div>
<div id="ref-gil2012" class="csl-entry" role="listitem">
Gil, Jorge, Nuno Montenegro, J N Beirão, and J P Duarte. 2012. <span>“On the <span>Discovery</span> of <span>Urban Typologies</span>: <span>Data Mining</span> the <span>Multi</span>-Dimensional <span>Character</span> of <span>Neighbourhoods</span>.”</span> <em>Urban Morphology</em> 16 (1): 27–40.
</div>
<div id="ref-hamaina2012a" class="csl-entry" role="listitem">
Hamaina, Rachid, Thomas Leduc, and Guillaume Moreau. 2012. <span>“Towards <span>Urban Fabrics Characterization Based</span> on <span>Buildings Footprints</span>.”</span> In <em>Bridging the <span>Geographic Information Sciences</span></em>, 2:327–46. <span>Berlin, Heidelberg</span>: <span>Springer, Berlin, Heidelberg</span>. <a href="https://doi.org/10.1007/978-3-642-29063-3_18">https://doi.org/10.1007/978-3-642-29063-3_18</a>.
</div>
<div id="ref-hijazi2016" class="csl-entry" role="listitem">
Hijazi, Ihab, Xin Li, Reinhard Koenig, G Schmit, R El Meouche, Zhihan Lv, and M Abunemeh. 2016. <span>“Measuring the Homogeneity of Urban Fabric Using <span>2D</span> Geometry Data.”</span> <em>Environment and Planning B: Planning and Design</em>, January, 1–25. <a href="https://doi.org/10.1177/0265813516659070">https://doi.org/10.1177/0265813516659070</a>.
</div>
<div id="ref-huang2018urban" class="csl-entry" role="listitem">
Huang, Bo, Bei Zhao, and Yimeng Song. 2018. <span>“Urban Land-Use Mapping Using a Deep Convolutional Neural Network with High Spatial Resolution Multispectral Remote Sensing Imagery.”</span> <em>Remote Sensing of Environment</em> 214 (September): 73–86. <a href="https://doi.org/10.1016/j.rse.2018.04.050">https://doi.org/10.1016/j.rse.2018.04.050</a>.
</div>
<div id="ref-ibrahim2020understanding" class="csl-entry" role="listitem">
Ibrahim, Mohamed R., James Haworth, and Tao Cheng. 2020. <span>“Understanding Cities with Machine Eyes: <span>A</span> Review of Deep Computer Vision in Urban Analytics.”</span> <em>Cities</em> 96 (January): 102481. <a href="https://doi.org/10.1016/j.cities.2019.102481">https://doi.org/10.1016/j.cities.2019.102481</a>.
</div>
<div id="ref-jakubik2023foundation" class="csl-entry" role="listitem">
Jakubik, Johannes, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, et al. 2023. <span>“Foundation <span>Models</span> for <span>Generalist</span> <span>Geospatial</span> <span>Artificial</span> <span>Intelligence</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2310.18660">https://doi.org/10.48550/arXiv.2310.18660</a>.
</div>
<div id="ref-PhysRevE.72.056127" class="csl-entry" role="listitem">
Lind, Pedro G., Marta C. González, and Hans J. Herrmann. 2005. <span>“Cycles and Clustering in Bipartite Networks.”</span> <em>Physical Review E</em> 72 (5): 056127. <a href="https://doi.org/10/c6m9xd">https://doi.org/10/c6m9xd</a>.
</div>
<div id="ref-lu2024ai" class="csl-entry" role="listitem">
Lu, Siqi, Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, and Yuankai Huo. 2024. <span>“<span>AI</span> <span>Foundation</span> <span>Models</span> in <span>Remote</span> <span>Sensing</span>: <span>A</span> <span>Survey</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2408.03464">https://doi.org/10.48550/arXiv.2408.03464</a>.
</div>
<div id="ref-ma2019deep" class="csl-entry" role="listitem">
Ma, Lei, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan Johnson. 2019. <span>“Deep Learning in Remote Sensing Applications: <span>A</span> Meta-Analysis and Review.”</span> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 152 (June): 166–77. <a href="https://doi.org/10.1016/j.isprsjprs.2019.04.015">https://doi.org/10.1016/j.isprsjprs.2019.04.015</a>.
</div>
<div id="ref-rolf2021generalizable" class="csl-entry" role="listitem">
Rolf, Esther, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. 2021. <span>“A Generalizable and Accessible Approach to Machine Learning with Global Satellite Imagery.”</span> <em>Nature Communications</em> 12 (1). <a href="https://doi.org/10.1038/s41467-021-24638-z">https://doi.org/10.1038/s41467-021-24638-z</a>.
</div>
<div id="ref-schirmer2015" class="csl-entry" role="listitem">
Schirmer, Patrick Michael, and Kay W Axhausen. 2015. <span>“A Multiscale Classification of Urban Morphology.”</span> <em>Journal of Transport and Land Use</em> 9 (1): 101–30. <a href="https://doi.org/10.5198/jtlu.2015.667">https://doi.org/10.5198/jtlu.2015.667</a>.
</div>
<div id="ref-steiniger2008" class="csl-entry" role="listitem">
Steiniger, Stefan, Tilman Lange, Dirk Burghardt, and Robert Weibel. 2008. <span>“An <span>Approach</span> for the <span>Classification</span> of <span>Urban Building Structures Based</span> on <span>Discriminant Analysis Techniques</span>.”</span> <em>Transactions in GIS</em> 12 (1): 31–59. <a href="https://doi.org/10.1111/j.1467-9671.2008.01085.x">https://doi.org/10.1111/j.1467-9671.2008.01085.x</a>.
</div>
<div id="ref-vanderhaegen2017" class="csl-entry" role="listitem">
Vanderhaegen, Sven, and Frank Canters. 2017. <span>“Mapping Urban Form and Function at City Block Level Using Spatial Metrics.”</span> <em>Landscape and Urban Planning</em> 167 (November): 399–409. <a href="https://doi.org/10.1016/j.landurbplan.2017.05.023">https://doi.org/10.1016/j.landurbplan.2017.05.023</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>https://huggingface.co/allenai/satlas-pretrain<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://huggingface.co/made-with-clay/Clay<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>