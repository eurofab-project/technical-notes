<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krasen Samardzhiev, Barbara Metzler, Martin Fleischmann, Dani Arribas-Bel">

<title>Algorithm Design and Theoretical Basis Description – Technical notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive summary</a></li>
  <li><a href="#theoretical-basis" id="toc-theoretical-basis" class="nav-link" data-scroll-target="#theoretical-basis">Theoretical basis</a>
  <ul class="collapse">
  <li><a href="#morphometric-classification-homogenisation" id="toc-morphometric-classification-homogenisation" class="nav-link" data-scroll-target="#morphometric-classification-homogenisation">Morphometric Classification Homogenisation</a></li>
  <li><a href="#ai-modelling-using-satellite-imagery" id="toc-ai-modelling-using-satellite-imagery" class="nav-link" data-scroll-target="#ai-modelling-using-satellite-imagery">AI Modelling using Satellite Imagery</a></li>
  </ul></li>
  <li><a href="#algorithm-design" id="toc-algorithm-design" class="nav-link" data-scroll-target="#algorithm-design">Algorithm Design</a>
  <ul class="collapse">
  <li><a href="#morphometric-classification-homogenisation-1" id="toc-morphometric-classification-homogenisation-1" class="nav-link" data-scroll-target="#morphometric-classification-homogenisation-1">Morphometric Classification Homogenisation</a>
  <ul class="collapse">
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model architecture</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data preprocessing</a></li>
  <li><a href="#morphometric-characterisation" id="toc-morphometric-characterisation" class="nav-link" data-scroll-target="#morphometric-characterisation">Morphometric characterisation</a></li>
  <li><a href="#target-labels" id="toc-target-labels" class="nav-link" data-scroll-target="#target-labels">Target labels</a></li>
  <li><a href="#traintestvalidation-split" id="toc-traintestvalidation-split" class="nav-link" data-scroll-target="#traintestvalidation-split">Train/test/validation split</a></li>
  <li><a href="#training-and-validation" id="toc-training-and-validation" class="nav-link" data-scroll-target="#training-and-validation">Training and validation</a></li>
  <li><a href="#preliminary-results" id="toc-preliminary-results" class="nav-link" data-scroll-target="#preliminary-results">Preliminary results</a></li>
  </ul></li>
  <li><a href="#ai-modelling-using-satellite-imagery-1" id="toc-ai-modelling-using-satellite-imagery-1" class="nav-link" data-scroll-target="#ai-modelling-using-satellite-imagery-1">AI Modelling using Satellite Imagery</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing-1" id="toc-data-preprocessing-1" class="nav-link" data-scroll-target="#data-preprocessing-1">Data preprocessing</a></li>
  <li><a href="#traintest-split" id="toc-traintest-split" class="nav-link" data-scroll-target="#traintest-split">Train/test split</a></li>
  <li><a href="#unbalanced-dataset" id="toc-unbalanced-dataset" class="nav-link" data-scroll-target="#unbalanced-dataset">Unbalanced dataset</a></li>
  <li><a href="#model-architectures" id="toc-model-architectures" class="nav-link" data-scroll-target="#model-architectures">Model architectures</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#preliminary-results-1" id="toc-preliminary-results-1" class="nav-link" data-scroll-target="#preliminary-results-1">Preliminary results</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#final-remarks-and-next-steps" id="toc-final-remarks-and-next-steps" class="nav-link" data-scroll-target="#final-remarks-and-next-steps">Final remarks and next steps</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="algorithm-design.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Algorithm Design and Theoretical Basis Description</h1>
<p class="subtitle lead">Technical Note D2</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Krasen Samardzhiev, Barbara Metzler, Martin Fleischmann, Dani Arribas-Bel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Charles University; The Alan Turing Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<!-- This tries to closely match what was promised in the project proposal. -->
<section id="executive-summary" class="level1">
<h1>Executive summary</h1>
<p>Summary goes here.</p>
</section>
<section id="theoretical-basis" class="level1">
<h1>Theoretical basis</h1>
<p>Kind of lit review I suppose? Probably could be adapted stuff we have in the proposal.</p>
<p>This report describes in detail the data, the data preprocessing, as well as model selection, training and validation schemes. The background section positions the research in the literature and identifies the drawbacks of current approaches, this study aims to address. It also provides some background on the study area - Central Europe. The data section describes the input, output and target data for the model. The methodology section focuses on the model selection, training and validation approaches.</p>
<section id="morphometric-classification-homogenisation" class="level2">
<h2 class="anchored" data-anchor-id="morphometric-classification-homogenisation">Morphometric Classification Homogenisation</h2>
<p>The spatial layout of the physical elements of cities - its urban fabric - affects most activities their residents undertake, from accessing services or jobs to their social and cultural lives. Analysing the interplay of urban form, land use, mobility and other dimensions of human activities provides insights into how cities evolve and what effective developmental policies should look like. Researchers in the field of urban morphology have spend years in identifying, classifying and analysing the variations in urban form across cities from all over the world. A core new development, powered by advancements in spatial data science, computer vision, and open data availability, are the methodologies created to computationally discern and analyse urban fabric <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span>. Taken together, these facts have opened up the possibility of a more systematic and comprehensive approach to the classification of urban morphological patterns, which in turn can drive our understanding of cities <span class="citation" data-cites="calafiore2023inequalities arribas2022spatial">(<a href="#ref-calafiore2023inequalities" role="doc-biblioref">Calafiore et al. 2023</a>; <a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>)</span>.</p>
<p>One factor that limits the wider application of these methods is their dependency on high-quality data, that is generally not available for every city. For example, <span class="citation" data-cites="fleischmann2022methodological arribas2022spatial">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>; <a href="#ref-arribas2022spatial" role="doc-biblioref">Arribas-Bel and Fleischmann 2022</a>)</span> use building polygons from the respective municipal and national mapping agencies. However, such data is not generally available even for most high-income countries. And where available, it can come in various formats and the data itself is not necessarily homogenous, which makes its processing difficult. For example, official Czechia cadastral building polygons (https://services.cuzk.cz/) come as a GeoPackage, with no information about building age, height, type or use. In contrast, official German building polygons, come separately for every state , typically but not always as GML data from a WFS service, and sometimes have extra building information. However, the definition of building is different to Czechia and the polygon set may include parts of tunnels, overpasses or tram lines. These definitions are even sometimes different in the cases of cities within the same country - i.e.&nbsp;the data for Berlin and Hamburg is different from the data available for Bremen.</p>
<p>EuroFab aims to address the data availability and processing issues, as well as broaden the applicability of urban morphometrics. It does this through the development of a predictive urban morphometric model which uses widely available building footprints to infer structure of urban form. The model input is calculated directly on satellite-derived building footprints, which gives it a global scope and thus eliminates the need the data fusion problems described above. Furthermore, it is trained on a detailed morphometric classification derived from official cadastre data in a large study area that covers multiple countries - Poland, Austria, Czechia, Germany and Slovakia, i.e.&nbsp;Central Europe. The heterogeneity of urban form and planning regimes present in the study area enables the model to distinguish a rich variety of urban patterns and improves it generalisability.</p>
<p>The specific approach taken has four stages. In the first stage morphological elements and their characteristics are calculated using the widely available Microsoft building and Overture Maps Transportation data. These elements are the predictor variables that the model will use. To achieve this we develop a highly-scalable polygon and street data pipeline, capable of calculating an exhaustive list of morphometric characteristics. In the second stage, each element, calculated in the first stage, is assigned a target classification label based on spatial overlap with morphological elements from <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span>. <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> is a classification of urban fabrics in Central Europe based on the highest-detail available cadastre data. Third, the data is then split into training and testing subsets based on spatial contiguity, in order to account for spatial leakage. Lastly, a series of non-linear models is trained to predict the morphological classification of individual elements, using a custom validation schema. This is required in order to emphasise the model’s ability to deal with previously unseen data and new urban fabric types in the evaluation. The best performing model is chosen as the final production model, and retrained on the whole dataset.</p>
<!--Satellite derived building footprints intro-->
<p>Satellite derived building footprints are becoming more widely adopted, however, the data does not come without issues. For example, in dense urban centres entire blocks can be delineated as individual buildings. Given that morphology calculations rely on precise local topological relations between neighbours, such as two buildings touching, this problem renders a whole number of possible measurements described in <span class="citation" data-cites="fleischmann2021measuring">(<a href="#ref-fleischmann2021measuring" role="doc-biblioref">Fleischmann, Romice, and Porta 2021</a>)</span> meaningless. Furthermore, this issue affects even simpler calculations such as counting the number of buildings within a radius or topological neighbourhood. Other issues are that computer vision techniques sometimes miss entire buildings or misidentify building boundaries. Therefore, any approach that uses satellite-derived building footprints should be able to account for these three and potentially other problems.</p>
<p>Implementing these stages, and further realising the potential of the model for global application, requires the creation of novel, scalable data processing pipelines capable of analysing hundreds of millions of morphological elements - buildings, streets and their derivatives. Due to their scalability, these pipelines can further be reused for other work, outside of the immediate scope of this projects. To ensure maximum impact and high standards, all work for this project follows open science principles and is open-sourced on GitHub.</p>
</section>
<section id="ai-modelling-using-satellite-imagery" class="level2">
<h2 class="anchored" data-anchor-id="ai-modelling-using-satellite-imagery">AI Modelling using Satellite Imagery</h2>
<p>A brief theoretical background.</p>
</section>
</section>
<section id="algorithm-design" class="level1">
<h1>Algorithm Design</h1>
<section id="morphometric-classification-homogenisation-1" class="level2">
<h2 class="anchored" data-anchor-id="morphometric-classification-homogenisation-1">Morphometric Classification Homogenisation</h2>
<section id="model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture">Model architecture</h3>
<p>The model architecture consists of the two main components: 1) derivation of predictive variables, and 2) development of the predictive model. Given the target of the prediction is morphological classification, we use morphometric measurements based on the sub-standard satellite-derived data as predictive variables, as they are conceptually related - the original target classification is a result of unsupervised learning on top of morphometric measurements based on cadastral data. The model’s main job is then to capture the shift of meaning of individual characters from the original, when measured on precise geometries, to the derived one, measured on imperfect representation of urban form. Given the model will eventually perform a prediction out of the sample, we further build in a logic identifying the types of urban form it has not seen previously and ingesting then manually in the original taxonomy, forming an iterative feedback loop. The whole system is illustrated in a Figure XXX (TODO).</p>
<p>TODO: ADD A DIAGRAM OF THE WHOLE THING, INCLUDING THE FEEDBACK LOOP.</p>
<p>The data preparation and model training consists of four stages. The first and second stages cover the morphometric characterisation, which acts as the model’s predictor variables, and the target variables generation. The third stage randomly splits the whole study area into model training and testing data taking into account spatial contiguity. The final stage is the model training and evaluation, based on a framework in which we empahsize the ability to flag unseen urban fabric types in five different scenarios. The full model training and evaluation framework will be implemented using scikit-learn pipelines.</p>
</section>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h3>
<section id="building-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="building-preprocessing">Building preprocessing</h4>
<p>All available Microsoft building footprints for the study area are used for the analysis. Typically, building polygons required for morphological studies have to be of very high quality. For example, building polygons overlapping by a thousandth of a millimetre break topological contiguity and therefore affect the calculation of morphological properties, such as the ratio of shared walls, or the number of adjacent buildings. Furthermore, even the highest quality available software suffers from numerical precision issues which exasperate the above problem. Another potential issue is the inclusion of artefacts we are not directly interested in such as sheds or market stalls. Most importantly, polygons shall represent individual buildings, not compounds of buildings that are adjacent. The footprints used for this study fall short of these standards. Therefore, our approach aims to accommodate less than ideal data and methods, first by processing the building data, and second, by adapting the morphological calculations to account for numerical issues.</p>
<p>TODO: ADD A FIGURE SHOWING MS BUILDINGS NEXT TO CADASTRE TO PROVIDE A VISUAL CUE</p>
<p>Before dealing with any morphological assessment, the polygons need to undergo basic topological preprocessing.</p>
<p>The first step in the building data processing is to split up multi-polygons and make the geometries valid. The second step, is to simplify the polygons in order to accurately represent the corners of buildings and other shape related characteristics. Next, to filter out any buildings that have an area larger than 200,000 sq.m. This is done since some artefacts such as construction sites, mines or tunnels might be included in the data as buildings. The next step is to merge overlapping buildings that either: overlap for at least 10 percent of their areas, or one of them has less than 50 sq.m. in total area. This is done to merge buildings and building parts, since cadastre definitions of these two polygon types are inconsistent and sometimes buildings are assigned as building parts or vice versa. This step merges the buildings and its parts into one polygon. Finally, the preprocessing pipeline snaps nearby buildings together and fills gaps in the polygons that are less than 10 square cm. These two steps aim to address some common topological issues, such as missing slivers with almost zero areas between multiple or inside individual building polygons. Nevertheless, even after the preprocessing numerous topological issues remain and therefore we take this into account in subsequent analysis steps.</p>
</section>
<section id="overture-streets" class="level4">
<h4 class="anchored" data-anchor-id="overture-streets">Overture Streets</h4>
<p>The street network is a direct download from Overture Maps Transportation theme, a processed subset of data from OpenStreetMap, which has global coverage and high quality data. Since the dataset includes multiple segments types, including footpaths, the types of segments used in the analysis are limited to … . Another type of segment that is filtered out are tunnels - the analysis strictly focuses on two dimensions and therefore undergrounds structures adversely affect the calculation of boundaries and characters.</p>
<p>The second major stage of the street processing is the simplification of the entire street network for each subregion. The actual physical layout of the street is not important to the study - the number of lanes for example, or the exact structure of roundabouts. Rather, the focus is on the topological properties of the street network such as connections, as well as general physical characteristics such as lengths and widths. Therefore the overture street network is further processed by eliminating false nodes, combining lanes and simplifying roundabouts. <!--- TODO this needs more explanation--></p>
<p>TODO: INCLUDE VISUAL REPRESENTATION OF INPUT AND SIMPLIFIED STREETS</p>
</section>
</section>
<section id="morphometric-characterisation" class="level3">
<h3 class="anchored" data-anchor-id="morphometric-characterisation">Morphometric characterisation</h3>
<p>The morphometric characterisation is directly derived from the method of <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> as closely as possible to ensure that we minimise the conceptual differences between the methodological backbone using the derivation of the target classification and the the data used within our model.</p>
<section id="subregions-split" class="level4">
<h4 class="anchored" data-anchor-id="subregions-split">Subregions split</h4>
<p>Since the study area of interest is large and contains tens of millions of buildings, it is divided into subregions to carry out all computation. The separation is done based on distances between buildings - buildings are split into subregions such that the building from one region and its closest neighbour (another building) from another region are at least 400 meters apart. This custom separation, rather than official administrative divisions, ensures that all elements that may affect morphological calculations are in the same set (subregion) and not split across political boundaries. All processing and character calculations are done for each region independently and in parallel.</p>
<p>TODO: ADD A FIGURE OF REGIONS</p>
</section>
<section id="elements-and-units" class="level4">
<h4 class="anchored" data-anchor-id="elements-and-units">Elements and units</h4>
<p>There are five morphological elements used for the morphometric characterisation - two base ones - buildings and streets and three derived ones - enclosures, nodes, enclosed tessellation cells (ETC). Buildings and streets are the two elements from which all other units are derived. The core unit of analysis in the study is the enclosed tessellation cell, which breaks down the whole study area into small-scale units, which when taken together fully cover the area.</p>
<section id="nodes" class="level5">
<h5 class="anchored" data-anchor-id="nodes">Nodes</h5>
<p>The first type of derived element in the study are street nodes, which are defined as the intersection points between different streets. They are used to calculate characteristics of the street network that capture relationships between streets such as number of intersections, as well as relationships between neighbouring enclosures and ETCs.</p>
<p>TODO: ADD A FIGURE OF NODES</p>
</section>
<section id="enclosures" class="level5">
<h5 class="anchored" data-anchor-id="enclosures">Enclosures</h5>
<p>Enclosures capture the characteristics of plots of land that contain from none to (usually) multiple buildings. They are operationalised as land delineations, surrounded by streets and other physical barriers, which can vary in size depending on the geographic context. If an area is in the city centre, each enclosure would approximate a single block and multiple building units, however if it was in an industrial area it would potentially encompass a single, or very few large buildings. In this study, only the street network is used for barriers to minimise the data dependency. Furthermore, enclosures are used to delineate the boundaries of enclosed tessellation cells to the surrounding streets - i.e.&nbsp;representing physical barriers.</p>
<p>In this study, enclosure delineation is further modified by introducing a variable, individual bandwidth for every building, as opposed to the global one used by <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> or none using in generic enclosure delineation. This is done to limit the boundaries effects around the edges of cities and towns - i.e.&nbsp;cells on the edges of cities in <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> are always large because there are no surrounding buildings and their cells resemble those of large buildings with lots of empty space around them. The limits used here are calculated through a Gabriel graph-based filtration of the subregion, which takes into account the surrounding neighbours structure around every ETC. For example, in row housing the buffer will be relatively small, in single family housing estates the buffer will be larger, and in industrial areas larger still; regardless of whether or not these buildings are in the middle of cities or around their edges. The detailed technical implementation is out of scope of this Technical Note.</p>
<p>TODO: ADD A FIGURE OF ENCLOSURES</p>
</section>
<section id="enclosed-tessellation-cells" class="level5">
<h5 class="anchored" data-anchor-id="enclosed-tessellation-cells">Enclosed Tessellation Cells</h5>
<p>Enclosed Tessellation Cells are the core unit used for the analysis and the one used to combine aspects of all of the other four elements. To operationalise it, the study follows the definition by <span class="citation" data-cites="fleischmann2022methodological">(<a href="#ref-fleischmann2022methodological" role="doc-biblioref">Fleischmann et al. 2022</a>)</span> - “the portion of space that results from growing a morphological tessellation within an enclosure delineated by a series of natural or built barriers identified from the literature on urban form, function and perception”, where the morphological tessellation is a delineation of the space based on Voronoi polygons centred around buildings. The boundaries of ETCs also represent the closest area of land to each building, than to any other building within an enclosure. Because of this feature, ETCs intersect with all other elements and are the unit that links together the characteristics of the four other elements. In some cases, if there are no buildings within the enclosure the whole enclosure is treated as an ‘empty’ tessellation cell.</p>
<p>TODO: ADD A FIGURE OF ETC</p>
</section>
<section id="morphometric-characters" class="level5">
<h5 class="anchored" data-anchor-id="morphometric-characters">Morphometric Characters</h5>
<p>Characteristics describing the interactions of these elements, and the elements themselves are calculated at three scales: small - covering only aspects of the element; medium - covering aspects of the element and neighbouring elements and large - covering neighbouring elements up to five topological neighbours. In total there are 63 (TODO: CHECK) morphometric characters calculated described in Table XXX, which come directly from the list of characters used to derive the target classification.</p>
<p>TODO: INCLUDE A TABLE OF CHARACTERS</p>
</section>
</section>
</section>
<section id="target-labels" class="level3">
<h3 class="anchored" data-anchor-id="target-labels">Target labels</h3>
<p>For the second stage, we assign a target classification label to every ETC derived using the satellite-derived polygons. This is done based on spatial intersection between EuroFab and <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> ETCs. In cases where there are multiple detailed tessellation cells that fall within the range of a single EuroFab ETC, the label is decided based on majority.</p>
<p>Since the final output of <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> is a hierarchy, rather than a flat clustering there are several options how to pick the specific target labels. Generally, clusters lower in the hierarchy represent classifications of urban fabrics at more granular scales. For example, depending on the hierarchy cutoff point historical urban areas can be one cluster, or can be separated into two - medieval and industrial-era urban fabrics.</p>
<p>The specific selection of cutoff points will follow <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span>. The first set of urban fabrics we will aim to predict, broadly differentiates - different types of houses; from heterogenous historical urbanised areas; from recent modern urban developments - apartment blocks and industrial areas. The second set breaks down each of the first sets into multiple subsets. It goes into more detail and splits the houses into more classes, based on features such as size and proximity to cities; it also splits the historical areas based on origin - medieval, industrial-era and others; and splits the modern urban developments into subclasses such as modernist apartment blocks, industrial areas, offices and others. By analysing the model performance across two different hierarchical levels, we will understand what is the highest resolution detail the model can predict, given the shortcomings of the data and which factors affect predictions.</p>
<p>TODO: FIGURE OF CLASSIFICATION IN AT LEAST TWO HIGH-LEVEL CUTS</p>
</section>
<section id="traintestvalidation-split" class="level3">
<h3 class="anchored" data-anchor-id="traintestvalidation-split">Train/test/validation split</h3>
<p>After assigning the target labels, we create a schema that will dictate how to split the data into a training and testing subsets for the classification models. Random subsetting does not work for this study, since we need to account for spatial dependency and the related data leakage between train and test data. The spatial leakage comes from both the nature of the data - spatial contiguity is one of the core aspects of morphological elements - but also from the way characters are calculated based on various nearest topological neighbours.</p>
<p>To account for this, we aggregate nearby ETCs into higher granularity spatial units - level 5 H3 cells - and randomly split the these units into 80% train and 20% test data, based on contiguity. This ensures that the majority of the test set ETCs and their neighbours are not present in the training data and no morphological characters related to them affect the training set. We use level 5 H3 cells, which represent a delineation of the globe into hexagons with approximately 250 sq. km. area, rather than enclosures or ETC contiguity, to ensure that contiguous subsets of test data cover areas of heterogenous elements and present the model with a realistic validation scenario. This process is done once for the whole study area, before training any models.</p>
<p>TODO: FIGURE OF TRAIN/TEST SPLIT BASED ON H3 ON A SINGLE CITY. IDEALLY MAPPED TO ETCS</p>
</section>
<section id="training-and-validation" class="level3">
<h3 class="anchored" data-anchor-id="training-and-validation">Training and validation</h3>
<!--why classification and tree based methods-->
<p>The main aim of the modelling task is to generate a classification of morphological elements of similar quality to <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> given the data quality limitations, albeit flat, not hierarchical. To achieve this we create an evaluation framework for the selection of non-linear tree-based models like a random forest classifier or an XGBoost model. We use the satellite-derived ETCs and their characteristics as input data and the clusters from <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> as target labels for a classification task. The choice of tree-based learning models is due to their readily available implementations, high scalability and ability to quickly offer interpretation insights. Furthermore, they handle well high dimensional data, non-linear interactions and require minimal hyper-parameter tuning. The flexibility of the models and the specific training/testing framework setup will allow us to not just produce a predictive model but also to identify potential areas for improvement in the original data preprocessing.</p>
<!--out of sample importance-->
<p>Since we want the final production model to be general and applicable to large areas i.e.&nbsp;whole continents, it needs to be able to handle previously unseen urban fabric types. For example, an urban morphology type that is present in the test data, or in another study area, might not be present in the training data and in that case the model should flag its predicted label as uncertain. This is another area where tree models have an advantage, since they are ensemble methods and this can help reduces their tendency to overfit. They also readily provide a confidence score for each prediction which can be used to flag unseen data. Furthermore, we take extra care to evaluate the final production models performance in realistic scenarios and the relationship between its accuracy on test data and whole countries that are not part of the model training or test data.</p>
<!--evaluation setup-->
<p>To achieve this we train five iterations of each model on different subsets of the study area - so that that every country and every combination of countries is used as final hold-out validation data a and train/test pipeline respectively. For example, one iteration will use Germany, Poland, Czechia and Austria as part of its train/test pipeline, whereas Slovakia will be used as the hold-out data , in order to see the relationship between the final test score from the train/test pipeline and the models realistic performance on a whole unseen country’s urban fabric. The data from the four countries in each train/test pipeline will be separated into training and test sets based on the spatial strategy described previously. The model training and evaluation will follow standard best practices - model coefficients and hyper parameter tuning, such as the decision threshold will be optimised based on the training subset, and the test subset will be used to give a final model accuracy score. Specifically, the models will use balanced accuracy as the optimisation metric in order to account for imbalances in the distribution of urban fabric classes. Our framework further extends the best practices with an additional layer of testing using the hold-out country for each train/test pipeline. All of the data in the hold-out country is used to score the model’s performance and the spatial train/test split will be ignored for that particular iteration.</p>
<p>TODO: DIAGRAM SHOWING THE EVAL SETUP</p>
<p>This acts as an extra check against overfitting and ultimately enable us to see how the final production model will perform in realistic scenarios - applying it to whole countries which are not used for the training or testing at all. This comes with at least two advantages over simply reporting a test score on a random sample. First, it is a test of model performance on a dataset that does not have any spatial leakage with the training or testing data. Second, it ensures that we evaluate model performance on unseen urban fabric types from other countries. We can afford to do this in part due to the large size of the data we are working with. In every permutation there will be a rich variety of urban fabrics and tens of millions of ETCs used in the training part of each train/test pipeline.</p>
<p>The final production model is trained on the whole dataset, using the same hyper parameter grid search configuration and training/test spatial split. The extra validation steps we carry out with the hold-out countries will be used to used in three ways. First, to contextualise the final models’ accuracy on the test data; second, to indicate how the model will perform on other countries; and third to see how it handles urban fabric types not seen in the training or test data in a realistic scenario.</p>
</section>
<section id="preliminary-results" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-results">Preliminary results</h3>
<p>There are 56,845,150 Microsoft building footprints for our study area, which are split into 474 subregions. This is significantly less than the available cadastre data, which has around 88 million buildings and are separated into 828 regions TODO: EXPLAIN WHY. The number of downloaded, unprocessed streets is similar to those in <span class="citation" data-cites="primus">(<a href="#ref-primus" role="doc-biblioref"><strong>primus?</strong></a>)</span> - 23,332,865 - since they cover the same study area and come from the same source - Overture Maps, which is a processed subset of OpenStreepMap. However, the number of tessellation cells is the same as the buildings and therefore less than the cadastre data-based classification. Furthermore, the street simplification algorithm is affected by the available buildings, and therefore in turn also affects the tessellation cell boundaries. These results highlight the effect of the satellite derived building footprints that have been discussed in the Technical Note D3 - the missing or wrongly merged building polygons. In turn these affect the third discussed issue - the interpretation of the values of the calculated morphological characters. Many characters from the original classification also completely lose their meaning. For example, the number of adjacent buildings in the EuroFab dataset is much smaller and therefore all characters based on adjacency have a much more limited utility. This also affects, to a relatively smaller, but nevertheless important extent, characters that rely on inter-building distances and counts.</p>
<p>These results further point towards the need for a non-linear classification model, cable of accounting for these data discrepancies.</p>
</section>
</section>
<section id="ai-modelling-using-satellite-imagery-1" class="level2">
<h2 class="anchored" data-anchor-id="ai-modelling-using-satellite-imagery-1">AI Modelling using Satellite Imagery</h2>
<p>Most of the stuff from <code>technical_part_turing.qmd</code> except the data description.</p>
<section id="data-preprocessing-1" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-1">Data preprocessing</h3>
<p>For our analysis, we use two datasets of image tiles at different scales: larger tiles (224 x 224 pixels, covering 2240 x 2240 meters) for segmentation tasks, and smaller tiles (56 x 56 pixels, covering 560 x 560 meters) for classification tasks. The segmentation dataset includes 26,753 tiles (21,402 for training and 5,351 for testing), while the classification dataset consists of 403,722 tiles (342,648 for training and 61,074 for testing). For consistent sampling and comparison, we only use tiles that fully overlap with the spatial signatures, ensuring that each tile aligns with the urban form and function typology in our labeling framework. This alignment supports robust comparison of classification and segmentation outcomes on a pixel-level.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../figures/algo_design/sampling.png" class="nostretch quarto-figure quarto-figure-left figure-img" height="400"></p>
</figure>
</div>
<p>A significant challenge in our dataset is class imbalance, where certain urban fabric types are substantially more represented than others. This imbalance influenced our decisions regarding model architecture and loss function selection, leading us to explore specialized approaches for handling uneven class distributions.</p>
</section>
<section id="traintest-split" class="level3">
<h3 class="anchored" data-anchor-id="traintest-split">Train/test split</h3>
<p>We split the dataset into 80% train and 20% test data. The test datasets for segmentation and classification overlap.</p>
<p><img src="../figures/algo_design/train_df.png" class="nostretch quarto-figure quarto-figure-left" height="400"> <img src="../figures/algo_design/test_df.png" class="nostretch quarto-figure quarto-figure-right" height="400"></p>
</section>
<section id="unbalanced-dataset" class="level3">
<h3 class="anchored" data-anchor-id="unbalanced-dataset">Unbalanced dataset</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/unbalanced.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
<section id="model-architectures" class="level3">
<h3 class="anchored" data-anchor-id="model-architectures">Model architectures</h3>
<p>As part of the AI model design, we tested three main experiments to analyze urban fabric classification and segmentation. First, we conduct a baseline experiment using image embeddings from the SatlasPretrain model <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, which we fit to an XGBoost classifier to predict urban fabric classes (Approach A). Second, we fine-tune three different geospatial foundation models—SatlasPretrain, Clay, and IBM/NASA’s Prithvi model—to perform segmentation tasks (Approach B). Third, we take the best-performing geospatial foundation model from the segmentation experiments (Clay) and fine-tune it specifically for a classification task (Approach C). To evaluate and compare the results, we report weighted pixel-level accuracy, F1 score, and Intersection over Union (IoU) metrics across the experiments.</p>
<ul>
<li>Approach A: Image embeddings + XGBoost model</li>
<li>Approach B: Fine-tuned geospatial foundation model (segmentation)</li>
<li>Approach C: Fine-tuned geospatial foundation model (classification)</li>
</ul>
<section id="baseline-approach-approach-a" class="level4">
<h4 class="anchored" data-anchor-id="baseline-approach-approach-a">Baseline approach (Approach A)</h4>
<p>The tiles are fed into the geospatial foundation model SatlasPretrain <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that has been pretrained with more than 302 million labels on a range of remote sensing and computer vision tasks.</p>
<p>The model operates in two main steps:</p>
<ol type="1">
<li>Foundation Model: A vision transformer model with a feature pyramid network (FPN) and a pooling layer is used to derive image embeddings—lower-dimensional representations of the images (Fig. 2).</li>
<li>Machine Learning Classifier: The image embeddings are then input into an XGBoost classifier to predict urban fabric classes across England.</li>
</ol>
<p>Our baseline model achieved a moderate prediction accuracy of approximately 61% with varying accuracy across the various spatial signature classes as seen in the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/baseline.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="400"></p>
</figure>
</div>
</section>
<section id="results-across-spatial-signatures" class="level4">
<h4 class="anchored" data-anchor-id="results-across-spatial-signatures">Results across spatial signatures</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/baseline_tile_level.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
<p><em>Baseline approach (ordinal)</em></p>
<p>In addition to the general classification task, we explored an ordinal regression task to account for the continuous nature of the spatial signatures, which are not strictly categorical. We applied the following ordinal mapping:</p>
<p><code>ordinal_mapping = {     'Wild countryside': 0,     'Countryside agriculture': 1,     'Urban buffer': 2,     'Open sprawl': 3,     'Disconnected suburbia': 4,     'Accessible suburbia': 5,     'Warehouse/Park land': 6,     'Gridded residential quarters': 7,     'Connected residential neighbourhoods': 8,     'Dense residential neighbourhoods': 9,     'Dense urban neighbourhoods': 10,     'Urbanity': 11, }</code></p>
<p>This ordinal approach produced a Mean Absolute Error (MAE) and Mean Squared Error (MSE) of 0.28, along with an R² score of 0.62. The Sankey diagram below highlights the primary misclassifications, which tend to occur between similar classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/sankey.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
<p><em>Baseline approach + spatial context</em></p>
<p>To enhance model performance, we incorporated spatial context, enabling the model to account for regional location. We included H3 Level 5 hexagon identifiers as a categorical variable, where each hexagon (approximately 560x560 meters) encompasses around 80 tiles.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/hex_level5.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
<section id="segmentation-approach-b" class="level4">
<h4 class="anchored" data-anchor-id="segmentation-approach-b">Segmentation (Approach B)</h4>
<p>In Approach B, we fine-tuned a state-of-the-art geospatial foundation model for a segmentation task. We used the 224x224x3 image tiles as input. We evaluated three state-of-the-art foundation models, each with unique characteristics as listed below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Architecture</th>
<th>Dataset Size</th>
<th>Image Sources</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Satlas [^1]</td>
<td>SwinT</td>
<td>302M labels</td>
<td>Sentinel-2</td>
</tr>
<tr class="even">
<td>Clay <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></td>
<td>MAE/ViT</td>
<td>70M labels</td>
<td>Multiple+</td>
</tr>
<tr class="odd">
<td>Prithvi <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></td>
<td>MAE/ViT</td>
<td>250 PB</td>
<td>Sentinel-2/Landsat</td>
</tr>
</tbody>
</table>
<p>+Multiple sources include Sentinel-2, Landsat, NAIP, and LINZ</p>
<p>The following visualisations show the varying model configurations for the three different approaches tested for the segmentation task. The main difference is the varying backbone.</p>
</section>
<section id="model-a-satlas" class="level4">
<h4 class="anchored" data-anchor-id="model-a-satlas">Model A: Satlas</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/satlas_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
</section>
<section id="model-b-clay" class="level4">
<h4 class="anchored" data-anchor-id="model-b-clay">Model B: Clay</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/clay_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
</section>
<section id="model-c-prithvi" class="level4">
<h4 class="anchored" data-anchor-id="model-c-prithvi">Model C: Prithvi</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/prithvi_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<p>After fine-tuning each foundation model for 10 epochs, we observed the following performance metrics:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Satlas</th>
<th>Clay</th>
<th>Prithvi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weighted Accuracy</td>
<td>0.57</td>
<td><strong>0.72</strong></td>
<td>0.62</td>
</tr>
<tr class="even">
<td>Weighted IoU</td>
<td>0.33</td>
<td><strong>0.58</strong></td>
<td>0.41</td>
</tr>
<tr class="odd">
<td>Weighted F1</td>
<td>0.41</td>
<td><strong>0.69</strong></td>
<td>0.58</td>
</tr>
<tr class="even">
<td>Training Time/Epoch</td>
<td>9 mins</td>
<td>8 mins</td>
<td>20 mins</td>
</tr>
<tr class="odd">
<td>Parameters</td>
<td>90M</td>
<td>86M</td>
<td>120M</td>
</tr>
<tr class="even">
<td>Implementation Score</td>
<td>5/10</td>
<td>6/10</td>
<td>7/10</td>
</tr>
</tbody>
</table>
<p>The Clay model consistently outperformed other foundation models across all metrics, while also maintaining reasonable training times and computational requirements.</p>
<p><strong>Loss Function Impact:</strong> The choice of loss function significantly influenced model performance. Focal loss proved particularly effective in handling class imbalance, especially when combined with the Clay model architecture.</p>
</section>
<section id="classification-approach-c" class="level4">
<h4 class="anchored" data-anchor-id="classification-approach-c">Classification (Approach C)</h4>
<p>Finally, in Approach C, we fine-tuned a geospatial foundation model for a classification task. We used the 56x56x3 image tiles as input.</p>
<p>For this approach we only used the Clay model as backbone, since it performed the best in the previous experiments.</p>
<p>The accuracy varied across the different classes as seen in the figure below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/class_acc.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="430"></p>
</figure>
</div>
<p>This figure shows a comparison between the predicted classes for the fine-tuned geospatial foundation model with segmentation approach (B) and the classification (C).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/comparison_B_C.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="430"></p>
</figure>
</div>
</section>
</section>
<section id="evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h3>
<p>We employed multiple complementary metrics to evaluate model performance, as follows:</p>
<ol type="1">
<li><p><strong>Intersection over Union (IoU):</strong> This metric measures the overlap between predicted and ground truth segmentations, ranging from 0 (no overlap) to 1 (perfect overlap). IoU is calculated as the area of intersection divided by the area of union between the predicted and actual segmentation masks.</p></li>
<li><p><strong>Weighted F1 Score:</strong> This metric provides a balanced measure of precision and recall, particularly important for imbalanced datasets. It is calculated as the harmonic mean of precision (how many of the predicted positives are correct) and recall (how many of the actual positives were identified), weighted by class frequencies.</p></li>
<li><p><strong>Weighted Accuracy:</strong> This metric calculates the proportion of correct predictions, weighted by class frequencies to account for class imbalance. It provides a more representative measure of model performance across all classes, regardless of their frequency in the dataset.</p></li>
</ol>
</section>
<section id="preliminary-results-1" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-results-1">Preliminary results</h3>
<p>Comparing the results is a non-trivial task because the image tiles do not correspond to each other and do not perfectly overlap (42px vs 224 px). To make a fair comparison we thus calculate the pixel-level accuracy scores across the approaches. For this purpose, we predict the full map of the test set and compare the overlapping tiles (as described in sampling). We then calculate the following metrics on a per-pixel level.</p>
<section id="overall-model-performance-comparison-pixel-level" class="level4">
<h4 class="anchored" data-anchor-id="overall-model-performance-comparison-pixel-level">Overall model performance comparison (Pixel-level)</h4>
<p>Our comprehensive evaluation revealed varying levels of performance across the three different approaches:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 17%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Global Accuracy</th>
<th>Macro Accuracy</th>
<th>F1 Score</th>
<th>IoU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Classification (embeddings)</td>
<td>0.76 (0.66)</td>
<td>0.22 (0.13)</td>
<td>0.23</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>Classification + H3 level 5</td>
<td><strong>0.87</strong> (0.82)</td>
<td><strong>0.42</strong> (0.35)</td>
<td><strong>0.45</strong></td>
<td><strong>0.79</strong></td>
</tr>
<tr class="odd">
<td>Classification + H3 ordinal</td>
<td>0.80 (0.80)</td>
<td>0.26 (0.26)</td>
<td>0.26</td>
<td>0.69</td>
</tr>
<tr class="even">
<td>Classification (Clay)</td>
<td>0.59 (0.68)</td>
<td>0.09</td>
<td>0.12</td>
<td>0.38</td>
</tr>
<tr class="odd">
<td>Segmentation (Clay)</td>
<td>0.73</td>
<td>0.31</td>
<td>0.30</td>
<td>0.58</td>
</tr>
</tbody>
</table>
<p>The baseline classification approaches demonstrated varying levels of success: - Basic embedding classification achieved 76% global accuracy (66% balanced) - Integration with H3 level 5 spatial indexing significantly improved performance to 87% global accuracy (42% balanced) - H3 level 5 ordinal classification reached 80% accuracy (26% balanced)</p>
<p>The fine-tuned geospatial foundation model performed better than the fine-tuned classification, with an accuracy score of 0.56 and 0.73 respectively.</p>
<p>Overall, the baseline approach with regional information performed best. This approach is not only the best performing but also relatively efficient to implement. Once the image embeddings are created, the downstream classification can be done in minutes.</p>
</section>
<section id="prediction-example-london" class="level4">
<h4 class="anchored" data-anchor-id="prediction-example-london">Prediction example: London</h4>
<p>The following figure shows an example of a prediction for the London area using the whole dataset. Each colour represents a different signature. The background colour represents the ground truth.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/results_eurofab.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
</section>
</section>
</section>
<section id="final-remarks-and-next-steps" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Final remarks and next steps</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-arribas2022spatial" class="csl-entry" role="listitem">
Arribas-Bel, Daniel, and Martin Fleischmann. 2022. <span>“Spatial Signatures-Understanding (Urban) Spaces Through Form and Function.”</span> <em>Habitat International</em> 128: 102641.
</div>
<div id="ref-calafiore2023inequalities" class="csl-entry" role="listitem">
Calafiore, Alessia, Krasen Samardzhiev, Francisco Rowe, Martin Fleischmann, and Daniel Arribas-Bel. 2023. <span>“Inequalities in Experiencing Urban Functions. An Exploration of Human Digital (Geo-) Footprints.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em>, 23998083231208507.
</div>
<div id="ref-fleischmann2022methodological" class="csl-entry" role="listitem">
Fleischmann, Martin, Alessandra Feliciotti, Ombretta Romice, and Sergio Porta. 2022. <span>“Methodological Foundation of a Numerical Taxonomy of Urban Form.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 49 (4): 1283–99.
</div>
<div id="ref-fleischmann2021measuring" class="csl-entry" role="listitem">
Fleischmann, Martin, Ombretta Romice, and Sergio Porta. 2021. <span>“Measuring Urban Form: Overcoming Terminological Inconsistencies for a Quantitative and Comprehensive Morphologic Analysis of Cities.”</span> <em>Environment and Planning B: Urban Analytics and City Science</em> 48 (8): 2133–50.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Bastani, F. et al., 2023. Satlaspretrain: A large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.&nbsp;16772-16782.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Bastani, F. et al., 2023. Satlaspretrain: A large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.&nbsp;16772-16782.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://huggingface.co/made-with-clay/Clay<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>