<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krasen Samardzhiev, Barbara Metzler, Martin Fleischmann, Dani Arribas-Bel">

<title>Algorithm Design and Theoretical Basis Description â€“ Technical notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#exective-summary" id="toc-exective-summary" class="nav-link active" data-scroll-target="#exective-summary">Exective summary</a></li>
  <li><a href="#theroetical-basis" id="toc-theroetical-basis" class="nav-link" data-scroll-target="#theroetical-basis">Theroetical basis</a>
  <ul class="collapse">
  <li><a href="#morphometric-classification-homogenisation" id="toc-morphometric-classification-homogenisation" class="nav-link" data-scroll-target="#morphometric-classification-homogenisation">Morphometric Classification Homogenisation</a></li>
  <li><a href="#ai-modelling-using-satellite-imagery" id="toc-ai-modelling-using-satellite-imagery" class="nav-link" data-scroll-target="#ai-modelling-using-satellite-imagery">AI Modelling using Satellite Imagery</a></li>
  </ul></li>
  <li><a href="#algorithm-design" id="toc-algorithm-design" class="nav-link" data-scroll-target="#algorithm-design">Algorithm Design</a>
  <ul class="collapse">
  <li><a href="#morphometric-classification-homogenisation-1" id="toc-morphometric-classification-homogenisation-1" class="nav-link" data-scroll-target="#morphometric-classification-homogenisation-1">Morphometric Classification Homogenisation</a>
  <ul class="collapse">
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model architecture</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data preprocessing</a></li>
  <li><a href="#morphometric-characterisation" id="toc-morphometric-characterisation" class="nav-link" data-scroll-target="#morphometric-characterisation">Morphometric characterisation</a></li>
  <li><a href="#target-labels" id="toc-target-labels" class="nav-link" data-scroll-target="#target-labels">Target labels</a></li>
  <li><a href="#traintestvalidation-split" id="toc-traintestvalidation-split" class="nav-link" data-scroll-target="#traintestvalidation-split">Train/test/validation split</a></li>
  <li><a href="#training-and-validation" id="toc-training-and-validation" class="nav-link" data-scroll-target="#training-and-validation">Training and validation</a></li>
  </ul></li>
  <li><a href="#ai-modelling-using-satellite-imagery-1" id="toc-ai-modelling-using-satellite-imagery-1" class="nav-link" data-scroll-target="#ai-modelling-using-satellite-imagery-1">AI Modelling using Satellite Imagery</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing-1" id="toc-data-preprocessing-1" class="nav-link" data-scroll-target="#data-preprocessing-1">Data preprocessing</a></li>
  <li><a href="#traintest-split" id="toc-traintest-split" class="nav-link" data-scroll-target="#traintest-split">Train/test split</a></li>
  <li><a href="#unbalanced-dataset" id="toc-unbalanced-dataset" class="nav-link" data-scroll-target="#unbalanced-dataset">Unbalanced dataset</a></li>
  <li><a href="#model-architectures" id="toc-model-architectures" class="nav-link" data-scroll-target="#model-architectures">Model architectures</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#preliminary-results" id="toc-preliminary-results" class="nav-link" data-scroll-target="#preliminary-results">Preliminary results</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next steps</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="algorithm-design.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Algorithm Design and Theoretical Basis Description</h1>
<p class="subtitle lead">Technical Note D2</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Krasen Samardzhiev, Barbara Metzler, Martin Fleischmann, Dani Arribas-Bel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Charles University; The Alan Turing Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<!-- This tries to closely match what was promised in the project proposal. -->
<section id="exective-summary" class="level1">
<h1>Exective summary</h1>
<p>Summary goes here.</p>
</section>
<section id="theroetical-basis" class="level1">
<h1>Theroetical basis</h1>
<p>Kind of lit review I suppose? Probably could be adapted stuff we have in the proposal.</p>
<section id="morphometric-classification-homogenisation" class="level2">
<h2 class="anchored" data-anchor-id="morphometric-classification-homogenisation">Morphometric Classification Homogenisation</h2>
<p>A brief theoretical background.</p>
</section>
<section id="ai-modelling-using-satellite-imagery" class="level2">
<h2 class="anchored" data-anchor-id="ai-modelling-using-satellite-imagery">AI Modelling using Satellite Imagery</h2>
<p>A brief theoretical background.</p>
</section>
</section>
<section id="algorithm-design" class="level1">
<h1>Algorithm Design</h1>
<section id="morphometric-classification-homogenisation-1" class="level2">
<h2 class="anchored" data-anchor-id="morphometric-classification-homogenisation-1">Morphometric Classification Homogenisation</h2>
<section id="model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture">Model architecture</h3>
<p>One paragraph summarising the whole thing.</p>
</section>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h3>
</section>
<section id="morphometric-characterisation" class="level3">
<h3 class="anchored" data-anchor-id="morphometric-characterisation">Morphometric characterisation</h3>
</section>
<section id="target-labels" class="level3">
<h3 class="anchored" data-anchor-id="target-labels">Target labels</h3>
</section>
<section id="traintestvalidation-split" class="level3">
<h3 class="anchored" data-anchor-id="traintestvalidation-split">Train/test/validation split</h3>
</section>
<section id="training-and-validation" class="level3">
<h3 class="anchored" data-anchor-id="training-and-validation">Training and validation</h3>
</section>
</section>
<section id="ai-modelling-using-satellite-imagery-1" class="level2">
<h2 class="anchored" data-anchor-id="ai-modelling-using-satellite-imagery-1">AI Modelling using Satellite Imagery</h2>
<p>Most of the stuff from <code>technical_part_turing.qmd</code> except the data description.</p>
<section id="data-preprocessing-1" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-1">Data preprocessing</h3>
<p>For our analysis, we use two datasets of image tiles at different scales: larger tiles (224 x 224 pixels, covering 2240 x 2240 meters) for segmentation tasks, and smaller tiles (56 x 56 pixels, covering 560 x 560 meters) for classification tasks. The segmentation dataset includes 26,753 tiles (21,402 for training and 5,351 for testing), while the classification dataset consists of 403,722 tiles (342,648 for training and 61,074 for testing). For consistent sampling and comparison, we only use tiles that fully overlap with the spatial signatures, ensuring that each tile aligns with the urban form and function typology in our labeling framework. This alignment supports robust comparison of classification and segmentation outcomes on a pixel-level.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../figures/algo_design/sampling.png" class="nostretch quarto-figure quarto-figure-left figure-img" height="400"></p>
</figure>
</div>
<p>A significant challenge in our dataset is class imbalance, where certain urban fabric types are substantially more represented than others. This imbalance influenced our decisions regarding model architecture and loss function selection, leading us to explore specialized approaches for handling uneven class distributions.</p>
</section>
<section id="traintest-split" class="level3">
<h3 class="anchored" data-anchor-id="traintest-split">Train/test split</h3>
<p>We split the dataset into 80% train and 20% test data. The test datasets for segmentation and classification overlap.</p>
<p><img src="../figures/algo_design/train_df.png" class="nostretch quarto-figure quarto-figure-left" height="400"> <img src="../figures/algo_design/test_df.png" class="nostretch quarto-figure quarto-figure-right" height="400"></p>
</section>
<section id="unbalanced-dataset" class="level3">
<h3 class="anchored" data-anchor-id="unbalanced-dataset">Unbalanced dataset</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/unbalanced.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
<section id="model-architectures" class="level3">
<h3 class="anchored" data-anchor-id="model-architectures">Model architectures</h3>
<p>As part of the AI model design, we tested three main experiments to analyze urban fabric classification and segmentation. First, we conduct a baseline experiment using image embeddings from the SatlasPretrain model <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, which we fit to an XGBoost classifier to predict urban fabric classes (Approach A). Second, we fine-tune three different geospatial foundation modelsâ€”SatlasPretrain, Clay, and IBM/NASAâ€™s Prithvi modelâ€”to perform segmentation tasks (Approach B). Third, we take the best-performing geospatial foundation model from the segmentation experiments (Clay) and fine-tune it specifically for a classification task (Approach C). To evaluate and compare the results, we report weighted pixel-level accuracy, F1 score, and Intersection over Union (IoU) metrics across the experiments.</p>
<ul>
<li>Approach A: Image embeddings + XGBoost model</li>
<li>Approach B: Fine-tuned geospatial foundation model (segmentation)</li>
<li>Approach C: Fine-tuned geospatial foundation model (classification)</li>
</ul>
<section id="baseline-approach-approach-a" class="level4">
<h4 class="anchored" data-anchor-id="baseline-approach-approach-a">Baseline approach (Approach A)</h4>
<p>The tiles are fed into the geospatial foundation model SatlasPretrain <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that has been pretrained with more than 302 million labels on a range of remote sensing and computer vision tasks.</p>
<p>The model operates in two main steps:</p>
<ol type="1">
<li>Foundation Model: A vision transformer model with a feature pyramid network (FPN) and a pooling layer is used to derive image embeddingsâ€”lower-dimensional representations of the images (Fig. 2).</li>
<li>Machine Learning Classifier: The image embeddings are then input into an XGBoost classifier to predict urban fabric classes across England.</li>
</ol>
<p>Our baseline model achieved a moderate prediction accuracy of approximately 61% with varying accuracy across the various spatial signature classes as seen in the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/baseline.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="400"></p>
</figure>
</div>
</section>
<section id="results-across-spatial-signatures" class="level4">
<h4 class="anchored" data-anchor-id="results-across-spatial-signatures">Results across spatial signatures</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/baseline_tile_level.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
<p><em>Baseline approach (ordinal)</em></p>
<p>In addition to the general classification task, we explored an ordinal regression task to account for the continuous nature of the spatial signatures, which are not strictly categorical. We applied the following ordinal mapping:</p>
<p><code>ordinal_mapping = {     'Wild countryside': 0,     'Countryside agriculture': 1,     'Urban buffer': 2,     'Open sprawl': 3,     'Disconnected suburbia': 4,     'Accessible suburbia': 5,     'Warehouse/Park land': 6,     'Gridded residential quarters': 7,     'Connected residential neighbourhoods': 8,     'Dense residential neighbourhoods': 9,     'Dense urban neighbourhoods': 10,     'Urbanity': 11, }</code></p>
<p>This ordinal approach produced a Mean Absolute Error (MAE) and Mean Squared Error (MSE) of 0.28, along with an RÂ² score of 0.62. The Sankey diagram below highlights the primary misclassifications, which tend to occur between similar classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/sankey.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
<p><em>Baseline approach + spatial context</em></p>
<p>To enhance model performance, we incorporated spatial context, enabling the model to account for regional location. We included H3 Level 5 hexagon identifiers as a categorical variable, where each hexagon (approximately 560x560 meters) encompasses around 80 tiles.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/hex_level5.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
<section id="segmentation-approach-b" class="level4">
<h4 class="anchored" data-anchor-id="segmentation-approach-b">Segmentation (Approach B)</h4>
<p>In Approach B, we fine-tuned a state-of-the-art geospatial foundation model for a segmentation task. We used the 224x224x3 image tiles as input. We evaluated three state-of-the-art foundation models, each with unique characteristics as listed below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Architecture</th>
<th>Dataset Size</th>
<th>Image Sources</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Satlas [^1]</td>
<td>SwinT</td>
<td>302M labels</td>
<td>Sentinel-2</td>
</tr>
<tr class="even">
<td>Clay <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></td>
<td>MAE/ViT</td>
<td>70M labels</td>
<td>Multiple+</td>
</tr>
<tr class="odd">
<td>Prithvi <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></td>
<td>MAE/ViT</td>
<td>250 PB</td>
<td>Sentinel-2/Landsat</td>
</tr>
</tbody>
</table>
<p>+Multiple sources include Sentinel-2, Landsat, NAIP, and LINZ</p>
<p>The following visualisations show the varying model configurations for the three different approaches tested for the segmentation task. The main difference is the varying backbone.</p>
</section>
<section id="model-a-satlas" class="level4">
<h4 class="anchored" data-anchor-id="model-a-satlas">Model A: Satlas</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/satlas_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
</section>
<section id="model-b-clay" class="level4">
<h4 class="anchored" data-anchor-id="model-b-clay">Model B: Clay</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/clay_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
</section>
<section id="model-c-prithvi" class="level4">
<h4 class="anchored" data-anchor-id="model-c-prithvi">Model C: Prithvi</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/prithvi_model.png" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<p>After fine-tuning each foundation model for 10 epochs, we observed the following performance metrics:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Satlas</th>
<th>Clay</th>
<th>Prithvi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weighted Accuracy</td>
<td>0.57</td>
<td><strong>0.72</strong></td>
<td>0.62</td>
</tr>
<tr class="even">
<td>Weighted IoU</td>
<td>0.33</td>
<td><strong>0.58</strong></td>
<td>0.41</td>
</tr>
<tr class="odd">
<td>Weighted F1</td>
<td>0.41</td>
<td><strong>0.69</strong></td>
<td>0.58</td>
</tr>
<tr class="even">
<td>Training Time/Epoch</td>
<td>9 mins</td>
<td>8 mins</td>
<td>20 mins</td>
</tr>
<tr class="odd">
<td>Parameters</td>
<td>90M</td>
<td>86M</td>
<td>120M</td>
</tr>
<tr class="even">
<td>Implementation Score</td>
<td>5/10</td>
<td>6/10</td>
<td>7/10</td>
</tr>
</tbody>
</table>
<p>The Clay model consistently outperformed other foundation models across all metrics, while also maintaining reasonable training times and computational requirements.</p>
<p><strong>Loss Function Impact:</strong> The choice of loss function significantly influenced model performance. Focal loss proved particularly effective in handling class imbalance, especially when combined with the Clay model architecture.</p>
</section>
<section id="classification-approach-c" class="level4">
<h4 class="anchored" data-anchor-id="classification-approach-c">Classification (Approach C)</h4>
<p>Finally, in Approach C, we fine-tuned a geospatial foundation model for a classification task. We used the 56x56x3 image tiles as input.</p>
<p>For this approach we only used the Clay model as backbone, since it performed the best in the previous experiments.</p>
<p>The accuracy varied across the different classes as seen in the figure below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/class_acc.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="430"></p>
</figure>
</div>
<p>This figure shows a comparison between the predicted classes for the fine-tuned geospatial foundation model with segmentation approach (B) and the classification (C).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/comparison_B_C.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="430"></p>
</figure>
</div>
</section>
</section>
<section id="evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h3>
<p>We employed multiple complementary metrics to evaluate model performance, as follows:</p>
<ol type="1">
<li><p><strong>Intersection over Union (IoU):</strong> This metric measures the overlap between predicted and ground truth segmentations, ranging from 0 (no overlap) to 1 (perfect overlap). IoU is calculated as the area of intersection divided by the area of union between the predicted and actual segmentation masks.</p></li>
<li><p><strong>Weighted F1 Score:</strong> This metric provides a balanced measure of precision and recall, particularly important for imbalanced datasets. It is calculated as the harmonic mean of precision (how many of the predicted positives are correct) and recall (how many of the actual positives were identified), weighted by class frequencies.</p></li>
<li><p><strong>Weighted Accuracy:</strong> This metric calculates the proportion of correct predictions, weighted by class frequencies to account for class imbalance. It provides a more representative measure of model performance across all classes, regardless of their frequency in the dataset.</p></li>
</ol>
</section>
<section id="preliminary-results" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-results">Preliminary results</h3>
<p>Comparing the results is a non-trivial task because the image tiles do not correspond to each other and do not perfectly overlap (42px vs 224 px). To make a fair comparison we thus calculate the pixel-level accuracy scores across the approaches. For this purpose, we predict the full map of the test set and compare the overlapping tiles (as described in sampling). We then calculate the following metrics on a per-pixel level.</p>
<section id="overall-model-performance-comparison-pixel-level" class="level4">
<h4 class="anchored" data-anchor-id="overall-model-performance-comparison-pixel-level">Overall model performance comparison (Pixel-level)</h4>
<p>Our comprehensive evaluation revealed varying levels of performance across the three different approaches:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 17%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Global Accuracy</th>
<th>Macro Accuracy</th>
<th>F1 Score</th>
<th>IoU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Classification (embeddings)</td>
<td>0.76 (0.66)</td>
<td>0.22 (0.13)</td>
<td>0.23</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>Classification + H3 level 5</td>
<td><strong>0.87</strong> (0.82)</td>
<td><strong>0.42</strong> (0.35)</td>
<td><strong>0.45</strong></td>
<td><strong>0.79</strong></td>
</tr>
<tr class="odd">
<td>Classification + H3 ordinal</td>
<td>0.80 (0.80)</td>
<td>0.26 (0.26)</td>
<td>0.26</td>
<td>0.69</td>
</tr>
<tr class="even">
<td>Classification (Clay)</td>
<td>0.59 (0.68)</td>
<td>0.09</td>
<td>0.12</td>
<td>0.38</td>
</tr>
<tr class="odd">
<td>Segmentation (Clay)</td>
<td>0.73</td>
<td>0.31</td>
<td>0.30</td>
<td>0.58</td>
</tr>
</tbody>
</table>
<p>The baseline classification approaches demonstrated varying levels of success: - Basic embedding classification achieved 76% global accuracy (66% balanced) - Integration with H3 level 5 spatial indexing significantly improved performance to 87% global accuracy (42% balanced) - H3 level 5 ordinal classification reached 80% accuracy (26% balanced)</p>
<p>The fine-tuned geospatial foundation model performed better than the fine-tuned classification, with an accuracy score of 0.56 and 0.73 respectively.</p>
<p>Overall, the baseline approach with regional information performed best. This approach is not only the best performing but also relatively efficient to implement. Once the image embeddings are created, the downstream classification can be done in minutes.</p>
</section>
<section id="prediction-example-london" class="level4">
<h4 class="anchored" data-anchor-id="prediction-example-london">Prediction example: London</h4>
<p>The following figure shows an example of a prediction for the London area using the whole dataset. Each colour represents a different signature. The background colour represents the ground truth.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/algo_design/results_eurofab.png" class="nostretch quarto-figure quarto-figure-center figure-img" height="420"></p>
</figure>
</div>
</section>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
</section>
<section id="next-steps" class="level1">
<h1>Next steps</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Bastani, F. et al., 2023. Satlaspretrain: A large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.&nbsp;16772-16782.<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2"><p>Bastani, F. et al., 2023. Satlaspretrain: A large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.&nbsp;16772-16782.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn3"><p>https://huggingface.co/made-with-clay/Clay<a href="#fnref3" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn4"><p>https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M<a href="#fnref4" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>