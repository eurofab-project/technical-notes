<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Barbara Metzler and Dani Arribas-Bel">

<title>AI Model Development for Urban Fabric Segmentation – Technical notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-49bd2ef832d2312f2379eb1ccfdd08c4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#test-and-verification-results-ai-model" id="toc-test-and-verification-results-ai-model" class="nav-link" data-scroll-target="#test-and-verification-results-ai-model">Test and Verification Results: AI Model</a>
  <ul class="collapse">
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a>
  <ul class="collapse">
  <li><a href="#satellite-imagery-input" id="toc-satellite-imagery-input" class="nav-link" data-scroll-target="#satellite-imagery-input">Satellite Imagery (Input)</a></li>
  <li><a href="#urban-fabric-classes-outcome" id="toc-urban-fabric-classes-outcome" class="nav-link" data-scroll-target="#urban-fabric-classes-outcome">Urban Fabric Classes (Outcome)</a></li>
  </ul></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a>
  <ul class="collapse">
  <li><a href="#scale" id="toc-scale" class="nav-link" data-scroll-target="#scale">Scale</a></li>
  <li><a href="#handling-imbalanced-dataset-with-image-augmentation" id="toc-handling-imbalanced-dataset-with-image-augmentation" class="nav-link" data-scroll-target="#handling-imbalanced-dataset-with-image-augmentation">Handling Imbalanced Dataset with Image Augmentation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#classifier-performance" id="toc-classifier-performance" class="nav-link" data-scroll-target="#classifier-performance">Classifier Performance</a></li>
  <li><a href="#changes-over-time-temporal-analysis" id="toc-changes-over-time-temporal-analysis" class="nav-link" data-scroll-target="#changes-over-time-temporal-analysis">Changes Over Time / Temporal Analysis</a>
  <ul class="collapse">
  <li><a href="#diversity-analysis-shannon-index" id="toc-diversity-analysis-shannon-index" class="nav-link" data-scroll-target="#diversity-analysis-shannon-index">Diversity Analysis (Shannon Index)</a></li>
  <li><a href="#spatial-patterns-of-change" id="toc-spatial-patterns-of-change" class="nav-link" data-scroll-target="#spatial-patterns-of-change">Spatial Patterns of Change</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link" data-scroll-target="#key-findings">Key Findings</a></li>
  <li><a href="#potential-research-directions" id="toc-potential-research-directions" class="nav-link" data-scroll-target="#potential-research-directions">Potential Research Directions</a></li>
  </ul></li>
  <li><a href="#software-and-example-datasets" id="toc-software-and-example-datasets" class="nav-link" data-scroll-target="#software-and-example-datasets">Software and Example Datasets</a>
  <ul class="collapse">
  <li><a href="#software-ai-method-for-urban-fabric-classification-and-morphometric-characterization" id="toc-software-ai-method-for-urban-fabric-classification-and-morphometric-characterization" class="nav-link" data-scroll-target="#software-ai-method-for-urban-fabric-classification-and-morphometric-characterization">Software: AI Method for Urban Fabric classification and morphometric characterization</a></li>
  <li><a href="#example-datasets-generated-during-verification-exercises" id="toc-example-datasets-generated-during-verification-exercises" class="nav-link" data-scroll-target="#example-datasets-generated-during-verification-exercises">Example datasets generated during Verification Exercises</a></li>
  <li><a href="#london" id="toc-london" class="nav-link" data-scroll-target="#london">London</a></li>
  <li><a href="#liverpool" id="toc-liverpool" class="nav-link" data-scroll-target="#liverpool">Liverpool</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="results_turing.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AI Model Development for Urban Fabric Segmentation</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Barbara Metzler and Dani Arribas-Bel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Alan Turing Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="executive-summary" class="level1">
<h1>Executive Summary</h1>
</section>
<section id="test-and-verification-results-ai-model" class="level1">
<h1>Test and Verification Results: AI Model</h1>
<p>This section expands on the <a href="https://eurofab.org/technical-notes/notes/algorithm-design.html#ai-modelling-using-satellite-imagery-1">Algorithm Design</a> and <a href="https://eurofab.org/technical-notes/notes/data-selection.html#ai-data">Reference Data Selection</a> sections, detailing the dataset and previous analyses leading to the final model. We discuss model performance in detail and present results of temporal predictions spanning 2016 to 2021.</p>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<section id="satellite-imagery-input" class="level3">
<h3 class="anchored" data-anchor-id="satellite-imagery-input">Satellite Imagery (Input)</h3>
<p>Satellite image data used to train the XGBoost classifier was sourced from the GHS-composite-S2 R2020A dataset<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, as detailed in the <a href="https://eurofab.org/technical-notes/notes/data-selection.html#ai-data">Reference Data Selection</a>. This dataset is a global, cloud-free image composite derived from Sentinel-2 L1C data, covering January 2017 through December 2018. We utilised RGB bands at 10 metres per pixel resolution.</p>
<section id="temporal-change-prediction" class="level4">
<h4 class="anchored" data-anchor-id="temporal-change-prediction">Temporal Change Prediction</h4>
<p>Sentinel-2 satellite imagery covering 2016 to 2021 was acquired using the Google Earth Engine API through automated Python scripts (as described in <a href="https://github.com/urbangrammarai/gee_pipeline">GEE pipeline</a>. Images were selected for low cloud coverage, resulting in composite summary products for each year.</p>
</section>
</section>
<section id="urban-fabric-classes-outcome" class="level3">
<h3 class="anchored" data-anchor-id="urban-fabric-classes-outcome">Urban Fabric Classes (Outcome)</h3>
<p>We employed labels from the Spatial Signatures Framework<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, a typology classifying British urban environments based on form (physical appearance) and function (usage). Although our project specifically targets urban fabric classification based primarily on visible form — which might be sipler to predict — the form-specific classification scheme remains under development (as detailed in the Sections on <em>Morphometric Classification</em>). Consequently, the comprehensive Spatial Signatures Framework currently serves as a proxy aligning closely with our project’s urban characterisation objectives.</p>
<p>We used two versions of this dataset: one with 12 classes and another simplified version with 7 classes. The 12-class scheme combines the various urbanity categories into a single class, maintaining all other Spatial Signatures classes:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>class_labels <span class="op">=</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Accessible suburbia'</span>: <span class="dv">0</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Connected residential neighbourhoods'</span>: <span class="dv">1</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Countryside agriculture'</span>: <span class="dv">2</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Dense residential neighbourhoods'</span>: <span class="dv">3</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Dense urban neighbourhoods'</span>: <span class="dv">4</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Disconnected suburbia'</span>: <span class="dv">5</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Gridded residential quarters'</span>: <span class="dv">6</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Open sprawl'</span>: <span class="dv">7</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urban buffer'</span>: <span class="dv">8</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urbanity'</span>: <span class="dv">9</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Warehouse/Park land'</span>: <span class="dv">10</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Wild countryside'</span>: <span class="dv">11</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the simplified 7-class version, we reclustered underlying data from the Spatial Signatures Framework using K-means clustering (K=7). The resulting classes are:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>class_labels_k7 <span class="op">=</span> {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Countryside agriculture'</span>: <span class="dv">0</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Open sprawl'</span>: <span class="dv">1</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Compact suburbia'</span>: <span class="dv">2</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urban'</span>: <span class="dv">3</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Urban buffer'</span>: <span class="dv">4</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Warehouse/Park land'</span>: <span class="dv">5</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Wild countryside'</span>: <span class="dv">6</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h2>
<section id="scale" class="level3">
<h3 class="anchored" data-anchor-id="scale">Scale</h3>
<p>Based on earlier experiments documented in the technical notes (<a href="https://eurofab.org/technical-notes/notes/algorithm-design.html#ai-modelling-using-satellite-imagery-1">Algorithm Design</a>), we adopted a final analytical scale of 250×250 metres across Great Britain (GB).</p>
<p>A significant issue in our dataset is class imbalance, with specific urban fabric classes being substantially underrepresented. This imbalance informed our decisions on model architecture and loss function selection, motivating exploration of specialised methods to handle imbalanced data. Yet, common modelling approaches were not sufficient, which means we had to resort to data augmentation methods, as described below.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../figures/algo_design/sampling.png" class="nostretch figure-img" height="400"></p>
<figcaption>Sampling Strategy</figcaption>
</figure>
</div>
</section>
<section id="handling-imbalanced-dataset-with-image-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="handling-imbalanced-dataset-with-image-augmentation">Handling Imbalanced Dataset with Image Augmentation</h3>
<p>Due to the significant class imbalance, particularly the underrepresentation of urban classes, we implemented a sliding-window augmentation strategy for classes comprising less than 10% of the dataset (all classes except <em>Countryside agriculture</em> and <em>Wild countryside</em>).</p>
<p>This sliding-window approach systematically shifted the sampling window horizontally and vertically by increments of 50, 100, 150, and 200 metres, significantly increasing the volume of available training data for underrepresented classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/img_aug.jpg" class="nostretch figure-img" height="420"></p>
<figcaption>Sliding Window Augmentation</figcaption>
</figure>
</div>
<p>The images below shows class distributions before (left) and after (right) augmentation:</p>
<p><img src="../figures/results/type_clean_encode.png" class="nostretch quarto-figure quarto-figure-left" height="200" alt="Before Augmentation"> <img src="../figures/results/type_clean_encode_augmented.png" class="nostretch quarto-figure quarto-figure-right" height="200" alt="After Augmentation"></p>
<p>The following table summarises augmentation results, highlighting the substantial increase in training samples for previously underrepresented classes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 48%">
<col style="width: 26%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Class</th>
<th>Before Augmentation</th>
<th>After Augmentation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accessible suburbia</td>
<td>15,054</td>
<td>129,620</td>
</tr>
<tr class="even">
<td>Connected residential neighbourhoods</td>
<td>2,567</td>
<td>21,021</td>
</tr>
<tr class="odd">
<td>Countryside agriculture</td>
<td>1,367,999</td>
<td>1,367,999</td>
</tr>
<tr class="even">
<td>Dense residential neighbourhoods</td>
<td>4,299</td>
<td>34,507</td>
</tr>
<tr class="odd">
<td>Dense urban neighbourhoods</td>
<td>3,636</td>
<td>31,657</td>
</tr>
<tr class="even">
<td>Disconnected suburbia</td>
<td>2,644</td>
<td>20,113</td>
</tr>
<tr class="odd">
<td>Gridded residential quarters</td>
<td>1,518</td>
<td>12,849</td>
</tr>
<tr class="even">
<td>Open sprawl</td>
<td>33,910</td>
<td>292,884</td>
</tr>
<tr class="odd">
<td>Urban buffer</td>
<td>381,283</td>
<td>381,283</td>
</tr>
<tr class="even">
<td>Urbanity</td>
<td>2,495</td>
<td>21,929</td>
</tr>
<tr class="odd">
<td>Warehouse/Park land</td>
<td>21,282</td>
<td>195,105</td>
</tr>
<tr class="even">
<td>Wild countryside</td>
<td>1,395,048</td>
<td>1,395,048</td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="classifier-performance" class="level3">
<h3 class="anchored" data-anchor-id="classifier-performance">Classifier Performance</h3>
<p>The final XGBoost classifier was trained using the augmented dataset and evaluated using three metrics: micro accuracy, macro accuracy (every class has same weighting), and macro F1 score. We validated the model using 5-fold spatial cross-validation at H3 resolution 6, ensuring an 80/20 training-testing split.</p>
<p>The table below summarises classifier performance for two classification schemes (7 and 12 classes) and two spatial contexts (with and without H3 resolution):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 23%">
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Classes (K)</th>
<th>Spatial Context</th>
<th>Accuracy</th>
<th>Macro Accuracy</th>
<th>Macro F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>7</td>
<td>None</td>
<td>0.4924</td>
<td>0.3856</td>
<td>0.3389</td>
</tr>
<tr class="even">
<td>7</td>
<td>H3 (res 5)</td>
<td>0.6959</td>
<td>0.5713</td>
<td><strong>0.5221</strong></td>
</tr>
<tr class="odd">
<td>12</td>
<td>None</td>
<td>0.4617</td>
<td>0.2666</td>
<td>0.2127</td>
</tr>
<tr class="even">
<td>12</td>
<td>H3 (res 5)</td>
<td>0.6654</td>
<td>0.4328</td>
<td>0.3654</td>
</tr>
</tbody>
</table>
<p>Including spatial context (H3 resolution) notably improved classification accuracy and F1 scores. This improvement shows the importance of spatial context in predicting urban fabric classes. Similarily, and as anticipated, the model with a lower number of classes performed better compared to the one with 12 classes.</p>
<section id="per-class-performance" class="level4">
<h4 class="anchored" data-anchor-id="per-class-performance">Per Class Performance</h4>
<p>Examining the 12-class model with spatial context (H3 resolution 5) in greater detail reveals that some classes consistently performed better than others. Specifically, classes such as <em>Wild countryside</em>, <em>Countryside agriculture</em> and <em>Urbanity</em> achieved relatively high accuracy, reflecting their visual distinctiveness in satellite imagery. In contrast, classes like <em>Dense residential neighbourhoods</em> and <em>Connected residential neighbourhoods</em> showed lower accuracy scores. These urban classes heavily depend on road connectivity patterns, which the limited patch size (250×250 metres) does not adequately capture. This limitation might likely contributed to their poorer performance.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/perclass_acc_12h3.png" class="nostretch figure-img" height="420"></p>
<figcaption>Per class accuracy, 12 classes with spatial context</figcaption>
</figure>
</div>
<p>The model without spatial context exhibited similar performance patterns but consistently lower accuracy overall. Interestingly, despite limited training examples, the class <em>Urbanity</em> maintained relatively high accuracy, indicating that its visual characteristics are clearly represented within the embeddings.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/perclassacc_k12.png" class="nostretch figure-img" height="420"></p>
<figcaption>Per class accuracy</figcaption>
</figure>
</div>
<p>We further analysed the relationship between the number of training observations per class and accuracy scores. While a moderate correlation exists — classes with more training samples generally performed better — this does not fully explain the accuracy differences. For example, <em>Urbanity</em>, <em>Gridded residential quarters</em> and <em>Accessible suburbia</em> performed notably better than classes with a comparable number of samples, such as <em>Dense residential neighbourhoods</em> and <em>Connected residential neighbourhoods</em>. This discrepancy likely arises from inherent visual representation of the signatures and distinguishability of these classes, rather than solely due to sample frequency.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/class_acc.png" class="nostretch figure-img" height="420"></p>
<figcaption>Number of observations vs accuracy</figcaption>
</figure>
</div>
</section>
</section>
<section id="changes-over-time-temporal-analysis" class="level2">
<h2 class="anchored" data-anchor-id="changes-over-time-temporal-analysis">Changes Over Time / Temporal Analysis</h2>
<p>We use the trained XGBoost classifier to make predictions across the years 2016 to 2021. The overall overlap between the initial year (2016) and final year (2021) remained high at 88%. This confirms that Spatial Signatures classes remained relatively stable across the study period. However, minor variations may indicate either genuine change or model uncertainty.</p>
<p>We measured urban fabric diversity using the Shannon Index across each year studied. The index showed limited variability over time, typically around 1.86–1.87, except for a noticeable spike in diversity in 2019 (2.007):</p>
<ul>
<li>2016 → 2017: 88%</li>
<li>2017 → 2018: 88%</li>
<li>2018 → 2019: 86%</li>
<li>2019 → 2020: 86%</li>
<li>2020 → 2021: 88%</li>
</ul>
<section id="diversity-analysis-shannon-index" class="level3">
<h3 class="anchored" data-anchor-id="diversity-analysis-shannon-index">Diversity Analysis (Shannon Index)</h3>
<p>We assessed changes in urban fabric diversity using the Shannon Index across the studied years. The results are summarised in the table below, indicating slight fluctuations, with the most notable increase in diversity occurring in 2019:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Year</th>
<th>Shannon Index</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2016</td>
<td>1.868</td>
</tr>
<tr class="even">
<td>2017</td>
<td>1.863</td>
</tr>
<tr class="odd">
<td>2018</td>
<td>1.868</td>
</tr>
<tr class="even">
<td>2019</td>
<td>2.007</td>
</tr>
<tr class="odd">
<td>2020</td>
<td>1.873</td>
</tr>
<tr class="even">
<td>2021</td>
<td>1.872</td>
</tr>
</tbody>
</table>
<p>The marked increase in the Shannon Index in 2019 suggests an increase in class diversity during that year, followed by a subsequent return to previous levels. This could point out some differences in the image as caused by weather or sensor in the year 2019.</p>
</section>
<section id="spatial-patterns-of-change" class="level3">
<h3 class="anchored" data-anchor-id="spatial-patterns-of-change">Spatial Patterns of Change</h3>
<p>Spatial analysis identified areas across England with frequent class transitions (map below), particularly around major urban centres and suburban zones. These frequent transitions may either represent genuine urban transformations or result from classifier uncertainty, especially in ambiguous zones between visually similar Spatial signatures classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/frequency_map.png" class="nostretch figure-img" height="420"></p>
<figcaption>Frequency Map</figcaption>
</figure>
</div>
<p>Class-specific analysis further highlighted particular Spatial signatures types prone to transitions. The figure below shows which classes experienced frequent transitions:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/class_transitions.png" class="nostretch figure-img" height="420"></p>
<figcaption>Class Transitions</figcaption>
</figure>
</div>
<p>To better interpret these transitions, we calculated transition probabilities and organised them into a structured confusion matrix. This matrix clearly shows the urban fabric classes most likely to interchange over the studied period:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/class_transitions_reasonable.png" class="nostretch figure-img" height="420"></p>
<figcaption>‘Reasonable’ confusion matrix</figcaption>
</figure>
</div>
<p>Lastly, we analysed the inverse-probability-based distances of urban fabric classes over time. Shorter distances represent a higher likelihood of transitioning or changing classes from year to year, whereas longer distances indicate greater stability:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://eurofab.org/talks/figures/202502_progress_turing/1d_transitions.png" class="img-fluid figure-img"></p>
<figcaption>Probability of change across classes</figcaption>
</figure>
</div>
<p>This analysis shows urban fabric classes such as <em>Urbanity</em>, <em>Warehouse/Park land</em>, <em>Dense urban neighbourhoods</em>, and <em>Connected residential neighbourhoods</em> have shorter inverse distances, indicating higher levels of dynamic change or redevelopment. Conversely, classes such as <em>Wild countryside</em> and <em>Countryside agriculture</em> have longer distances, suggesting greater temporal stability. Although these results align with expectations about urban and rural dynamics, they may also reflect the classifier’s varying uncertainty across these visually distinct environments.</p>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<p>The analysis shows that urban fabric classifications exhibit distinct temporal and spatial dynamics, reflecting varying levels of stability and diversity over time. Notably, urban classes such as <em>Dense urban neighbourhoods</em>, and <em>Connected residential neighbourhoods</em> displayed higher probabilities of transition, indicating active urban transformation. Similarily, suburban classes, such as <em>Accessible suburbia</em>, <em>Disconnected suburbia</em> and <em>Urban buffer</em>, also show a higher probability of change between said classes. Conversely, rural classes showed significant stability. This is most probably related to the uncertainty in the classifier than connected to actual changes of the environment.</p>
<section id="lessons-learned" class="level4">
<h4 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h4>
<ul>
<li><p><strong>Scale</strong>: Urban fabric classes, like spatial signatures, have fuzzy boundaries. Pixel-level classifications provide the highest possible spatial resolution, which is beneficial for detailed analysis. However, pixels often lack clear visual cues indicating exact boundaries between classes, making pixel-level predictions challenging for the model. Patch-level classification, although lower in spatial resolution depending on patch size, provided clearer visual context and resulted in better overall performance.</p></li>
<li><p><strong>Embeddings vs fine-tuned foundation model</strong>: Fine-tuning foundation models involves significant complexity and requires careful design decisions. In our case, the limited number of training examples was insufficient to achieve noticeable improvements through fine-tuning. The effort required for fine-tuning did not outweigh the simpler alternative of using off-the-shelf embeddings.</p></li>
<li><p><strong>Regional trends</strong>: Including regional contextual information substantially improved the classifier’s accuracy. Nonetheless, we found it essential that embeddings themselves already capture enough visual detail for accurate classification, ensuring that predictions remain robust even without regional context (and simply do not just rely on the spatial information to make predictions).</p></li>
<li><p><strong>Data augmentation</strong>: The sliding window augmentation approach effectively addressed class imbalances, significantly improving model performance by increasing representation of previously underrepresented urban fabric classes. This could boost the performance of the classifier another 10-20% in terms of accuracy.</p></li>
</ul>
</section>
</section>
<section id="potential-research-directions" class="level3">
<h3 class="anchored" data-anchor-id="potential-research-directions">Potential Research Directions</h3>
<p>There are a number of possible directions that would be beneficial to explore. The two main ones are as follows:</p>
<ul>
<li><p>Misclassifications typically occur between visually similar urban fabric classes, indicating inherent uncertainty in predictions. Incorporating prediction probabilities into a secondary model could help address this issue. By explicitly using probability scores from the initial classification as input for a refinement model — as previously shown by Fleischmann and Arribas-Bel<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> — we could better distinguish between ambiguous cases. This approach may “smooth” predictions, reducing noise and improving overall classification accuracy. Future work should explore how prediction confidence scores can be systematically utilised, either by employing spatial smoothing algorithms or by applying secondary machine learning models trained specifically to correct uncertain predictions.</p></li>
<li><p>Evaluating the generalisability of this methodological framework is crucial for its wider applicability. Future research should test this modelling approach in different European regions, assessing whether the chosen methods, including data preprocessing, augmentation strategies, spatial embeddings, and classifier architectures, perform consistently outside Great Britain. This would involve exploring variations in urban form and regional urban planning contexts across Europe. Understanding these factors will help identify potential adjustments needed to ensure reliable predictions when extending the model beyond the original study area.</p></li>
</ul>
</section>
</section>
<section id="software-and-example-datasets" class="level1">
<h1>Software and Example Datasets</h1>
<p>All analyses presented here are supported by openly accessible software hosted on <a href="https://github.com/eurofab-project/eo/tree/main">GitHub</a>. The AI prediction pipeline, including preprocessing, embedding generation, and prediction of spatial signatures, is fully documented and accessible at EO repository.</p>
<section id="software-ai-method-for-urban-fabric-classification-and-morphometric-characterization" class="level2">
<h2 class="anchored" data-anchor-id="software-ai-method-for-urban-fabric-classification-and-morphometric-characterization">Software: AI Method for Urban Fabric classification and morphometric characterization</h2>
<p>All the work supporting this analysis can be found on GitHub. The main prediction pipeline, which includes data preprocessing, embedding creation, and spatial signature prediction, can be used as follows:</p>
<pre><code># Run the pipeline
pipeline.spatial_sig_prediction(
    geo_path= "../spatial_signatures/eo/data/example/london_25_25_grid_clipped.geojson", ## Vector file (geojson or parquet) of analysis area (grid).
    vrt_file= "../satellite_demoland/data/mosaic_cube/vrt_allbands/2017_combined.vrt", ## Vrt file of the satellite composite
    xgb_weights = "../spatial_signatures/classifier/k12_h5_slided_gb_xgb_model.bin", ## Model weights for XGBoost classifier
    model_weights = "../satellite_demoland/models/satlas/weights/satlas-model-v1-lowres.pth", ## Model weights for embedding model (Satlas)
    output_path= "../vjgo8416-demoland/spatial_signatures/eo/data/predictions/test_london_h6.parquet", ## Output file with predictions, prediction probabilities and geometries
    h3_resolution=5 ## h3 resolution to be added to analysis (spatial context)
)`</code></pre>
<p>More details and documentation on how to run the pipeline can be found in the example on the <a href="https://github.com/eurofab-project/eo/blob/main/notebooks/run_pipeline.ipynb">EO repository</a>.</p>
</section>
<section id="example-datasets-generated-during-verification-exercises" class="level2">
<h2 class="anchored" data-anchor-id="example-datasets-generated-during-verification-exercises">Example datasets generated during Verification Exercises</h2>
<p>The final datacube including predictions for the years 2016 to 2021 for 7 and 12 classes can be found on the repository.</p>
<p>Here are some example visualisations showing London and Liverpool from the dataset:</p>
</section>
<section id="london" class="level2">
<h2 class="anchored" data-anchor-id="london">London</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/london_k7.png" class="nostretch figure-img" height="400"></p>
<figcaption>London, 7 classes</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/london_k12.png" class="nostretch figure-img" height="400"></p>
<figcaption>London, 12 classes</figcaption>
</figure>
</div>
</section>
<section id="liverpool" class="level2">
<h2 class="anchored" data-anchor-id="liverpool">Liverpool</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/liverpool_k7.png" class="nostretch figure-img" height="400"></p>
<figcaption>Liverpool, 7 classes</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/results/liverpool_k12.png" class="nostretch figure-img" height="400"></p>
<figcaption>Liverpool, 12 classes</figcaption>
</figure>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Corbane, C. et al., 2020. A global cloud-free pixel-based image composite from Sentinel-2 data. <em>Data in Brief</em>, 31, p.105737.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Fleischmann, M. &amp; Arribas-Bel, D., 2022. Geographical characterisation of British urban form using the spatial signatures framework. <em>Scientific Data</em>, 9(1), p.546.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Fleischmann and Arribas-Bel, 2024. Decoding (urban) form and function using spatially explicit deep learning. <em>Computers, Environment and Urban Systems</em>, 31, p.105737.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>