[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Technical notes",
    "section": "",
    "text": "Technical notes produced during the EuroFab project."
  },
  {
    "objectID": "index.html#list-of-notes",
    "href": "index.html#list-of-notes",
    "title": "Technical notes",
    "section": "List of notes",
    "text": "List of notes\n\nConsolidated Stakeholder Requirements Specification (PDF)\nAlgorithm Design and Theoretical Basis Description (PDF)\nReference Data Selection (PDF)"
  },
  {
    "objectID": "notes/stakeholders.html",
    "href": "notes/stakeholders.html",
    "title": "Consolidated Stakeholder Requirements Specification",
    "section": "",
    "text": "This is some text."
  },
  {
    "objectID": "notes/data-selection.html",
    "href": "notes/data-selection.html",
    "title": "Reference Data Selection",
    "section": "",
    "text": "This is some text."
  },
  {
    "objectID": "notes/algorithm-design.html",
    "href": "notes/algorithm-design.html",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "",
    "text": "This report includes the preliminary research and experiments run to design an AI model for making predictions on the urban fabric of England, as of November 2024."
  },
  {
    "objectID": "notes/algorithm-design.html#data",
    "href": "notes/algorithm-design.html#data",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Data",
    "text": "Data\n\nSatellite imagery (input)\nThe satellite image data used is accessed from the GHS-composite-S2 R2020A dataset 1. The dataset is a global, cloud-free image composite derived from Sentinel-2 L1C data, covering the period from January 2017 to December 2018. We use three bands (RGB) and the images have a resolution of 10 meters per pixel.\n\n\nUrban fabric classes (outcome)\nWe use labels from the Spatial Signatures Framework 2, which provides a typology of British urban environments based on both form (physical appearance) and function (usage). Spatial signatures capture complex urban characterizations, offering insights into how different areas look and operate. While our primary interest is in urban fabric classification focused on form—an approach we expect may be simpler since form is visible in imagery—this classification scheme is still under development. Therefore, we currently use the more comprehensive Spatial Signatures Framework as a proxy, as it aligns with the goals of urban characterization in our project."
  },
  {
    "objectID": "notes/algorithm-design.html#data-preprocessing",
    "href": "notes/algorithm-design.html#data-preprocessing",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nFor our analysis, we use two datasets of image tiles at different scales: larger tiles (224 x 224 pixels, covering 2240 x 2240 meters) for segmentation tasks, and smaller tiles (56 x 56 pixels, covering 560 x 560 meters) for classification tasks. The segmentation dataset includes 26,753 tiles (21,402 for training and 5,351 for testing), while the classification dataset consists of 403,722 tiles (342,648 for training and 61,074 for testing). For consistent sampling and comparison, we only use tiles that fully overlap with the spatial signatures, ensuring that each tile aligns with the urban form and function typology in our labeling framework. This alignment supports robust comparison of classification and segmentation outcomes on a pixel-level.\n\n\n\n\n\nA significant challenge in our dataset is class imbalance, where certain urban fabric types are substantially more represented than others. This imbalance influenced our decisions regarding model architecture and loss function selection, leading us to explore specialized approaches for handling uneven class distributions.\n\nTrain/test split\nWe split the dataset into 80% train and 20% test data. The test datasets for segmentation and classification overlap.\n \n\n\nUnbalanced dataset"
  },
  {
    "objectID": "notes/algorithm-design.html#baseline-approach-approach-a",
    "href": "notes/algorithm-design.html#baseline-approach-approach-a",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Baseline approach (Approach A)",
    "text": "Baseline approach (Approach A)\nThe tiles are fed into the geospatial foundation model SatlasPretrain 4 that has been pretrained with more than 302 million labels on a range of remote sensing and computer vision tasks.\nThe model operates in two main steps:\n\nFoundation Model: A vision transformer model with a feature pyramid network (FPN) and a pooling layer is used to derive image embeddings—lower-dimensional representations of the images (Fig. 2).\nMachine Learning Classifier: The image embeddings are then input into an XGBoost classifier to predict urban fabric classes across England.\n\nOur baseline model achieved a moderate prediction accuracy of approximately 61% with varying accuracy across the various spatial signature classes as seen in the figure below.\n\n\n\n\n\n\nResults across spatial signatures\n\n\n\n\n\nBaseline approach (ordinal)\nIn addition to the general classification task, we explored an ordinal regression task to account for the continuous nature of the spatial signatures, which are not strictly categorical. We applied the following ordinal mapping:\nordinal_mapping = {     'Wild countryside': 0,     'Countryside agriculture': 1,     'Urban buffer': 2,     'Open sprawl': 3,     'Disconnected suburbia': 4,     'Accessible suburbia': 5,     'Warehouse/Park land': 6,     'Gridded residential quarters': 7,     'Connected residential neighbourhoods': 8,     'Dense residential neighbourhoods': 9,     'Dense urban neighbourhoods': 10,     'Urbanity': 11, }\nThis ordinal approach produced a Mean Absolute Error (MAE) and Mean Squared Error (MSE) of 0.28, along with an R² score of 0.62. The Sankey diagram below highlights the primary misclassifications, which tend to occur between similar classes.\n\n\n\n\n\nBaseline approach + spatial context\nTo enhance model performance, we incorporated spatial context, enabling the model to account for regional location. We included H3 Level 5 hexagon identifiers as a categorical variable, where each hexagon (approximately 560x560 meters) encompasses around 80 tiles."
  },
  {
    "objectID": "notes/algorithm-design.html#segmentation-approach-b",
    "href": "notes/algorithm-design.html#segmentation-approach-b",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Segmentation (Approach B)",
    "text": "Segmentation (Approach B)\nIn Approach B, we fine-tuned a state-of-the-art geospatial foundation model for a segmentation task. We used the 224x224x3 image tiles as input. We evaluated three state-of-the-art foundation models, each with unique characteristics as listed below:\n\n\n\nModel\nArchitecture\nDataset Size\nImage Sources\n\n\n\n\nSatlas 5\nSwinT\n302M labels\nSentinel-2\n\n\nClay 6\nMAE/ViT\n70M labels\nMultiple+\n\n\nPrithvi 7\nMAE/ViT\n250 PB\nSentinel-2/Landsat\n\n\n\n+Multiple sources include Sentinel-2, Landsat, NAIP, and LINZ\nThe following visualisations show the varying model configurations for the three different approaches tested for the segmentation task. The main difference is the varying backbone.\n\nModel A: Satlas\n\n\n\n\n\n\n\n\nModel B: Clay\n\n\n\n\n\n\n\n\nModel C: Prithvi\n\n\n\n\n\nAfter fine-tuning each foundation model for 10 epochs, we observed the following performance metrics:\n\n\n\nMetric\nSatlas\nClay\nPrithvi\n\n\n\n\nWeighted Accuracy\n0.57\n0.72\n0.62\n\n\nWeighted IoU\n0.33\n0.58\n0.41\n\n\nWeighted F1\n0.41\n0.69\n0.58\n\n\nTraining Time/Epoch\n9 mins\n8 mins\n20 mins\n\n\nParameters\n90M\n86M\n120M\n\n\nImplementation Score\n5/10\n6/10\n7/10\n\n\n\nThe Clay model consistently outperformed other foundation models across all metrics, while also maintaining reasonable training times and computational requirements.\nLoss Function Impact: The choice of loss function significantly influenced model performance. Focal loss proved particularly effective in handling class imbalance, especially when combined with the Clay model architecture."
  },
  {
    "objectID": "notes/algorithm-design.html#classification-approach-c",
    "href": "notes/algorithm-design.html#classification-approach-c",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Classification (Approach C)",
    "text": "Classification (Approach C)\nFinally, in Approach C, we fine-tuned a geospatial foundation model for a classification task. We used the 56x56x3 image tiles as input.\nFor this approach we only used the Clay model as backbone, since it performed the best in the previous experiments.\nThe accuracy varied across the different classes as seen in the figure below:\n\n\n\n\n\nThis figure shows a comparison between the predicted classes for the fine-tuned geospatial foundation model with segmentation approach (B) and the classification (C).\n\n\n\n\n\n\nEvaluation Metrics\nWe employed multiple complementary metrics to evaluate model performance, as follows:\n\nIntersection over Union (IoU): This metric measures the overlap between predicted and ground truth segmentations, ranging from 0 (no overlap) to 1 (perfect overlap). IoU is calculated as the area of intersection divided by the area of union between the predicted and actual segmentation masks.\nWeighted F1 Score: This metric provides a balanced measure of precision and recall, particularly important for imbalanced datasets. It is calculated as the harmonic mean of precision (how many of the predicted positives are correct) and recall (how many of the actual positives were identified), weighted by class frequencies.\nWeighted Accuracy: This metric calculates the proportion of correct predictions, weighted by class frequencies to account for class imbalance. It provides a more representative measure of model performance across all classes, regardless of their frequency in the dataset."
  },
  {
    "objectID": "notes/algorithm-design.html#overall-model-performance-comparison-pixel-level",
    "href": "notes/algorithm-design.html#overall-model-performance-comparison-pixel-level",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Overall model performance comparison (Pixel-level)",
    "text": "Overall model performance comparison (Pixel-level)\nOur comprehensive evaluation revealed varying levels of performance across the three different approaches:\n\n\n\n\n\n\n\n\n\n\nApproach\nGlobal Accuracy\nMacro Accuracy\nF1 Score\nIoU\n\n\n\n\nClassification (embeddings)\n0.76 (0.66)\n0.22 (0.13)\n0.23\n0.63\n\n\nClassification + H3 level 5\n0.87 (0.82)\n0.42 (0.35)\n0.45\n0.79\n\n\nClassification + H3 ordinal\n0.80 (0.80)\n0.26 (0.26)\n0.26\n0.69\n\n\nClassification (Clay)\n0.59 (0.68)\n0.09\n0.12\n0.38\n\n\nSegmentation (Clay)\n0.73\n0.31\n0.30\n0.58\n\n\n\nThe baseline classification approaches demonstrated varying levels of success: - Basic embedding classification achieved 76% global accuracy (66% balanced) - Integration with H3 level 5 spatial indexing significantly improved performance to 87% global accuracy (42% balanced) - H3 level 5 ordinal classification reached 80% accuracy (26% balanced)\nThe fine-tuned geospatial foundation model performed better than the fine-tuned classification, with an accuracy score of 0.56 and 0.73 respectively.\nOverall, the baseline approach with regional information performed best. This approach is not only the best performing but also relatively efficient to implement. Once the image embeddings are created, the downstream classification can be done in minutes.\n\nPrediction example: London\nThe following figure shows an example of a prediction for the London area using the whole dataset. Each colour represents a different signature. The background colour represents the ground truth."
  },
  {
    "objectID": "notes/algorithm-design.html#key-findings",
    "href": "notes/algorithm-design.html#key-findings",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Key Findings",
    "text": "Key Findings\n\nRegional trends: The integration of regional information into the model significantly improving model performance across all metrics.\nModel performance::"
  },
  {
    "objectID": "notes/algorithm-design.html#challenges-and-limitations",
    "href": "notes/algorithm-design.html#challenges-and-limitations",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Challenges and Limitations",
    "text": "Challenges and Limitations\nOur research encountered several significant challenges:\n\nClass Imbalance: The dataset exhibited substantial variations in class representation, which impacted model performance. While focal loss helped address this issue, some classes remained challenging to predict accurately.\nComputational Resources: Training times varied significantly between models, with Prithvi requiring substantially more computational resources. This presents practical considerations for model deployment and scaling.\nComparison Complexity: The different tile sizes between classification (42px) and segmentation (224px) approaches made direct comparisons challenging, requiring careful consideration of evaluation metrics and methodology."
  },
  {
    "objectID": "notes/algorithm-design.html#footnotes",
    "href": "notes/algorithm-design.html#footnotes",
    "title": "AI Model Development for Urban Fabric Segmentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCorbane, C. et al., 2020. A global cloud-free pixel-based image composite from Sentinel-2 data. Data in brief, 31, p.105737.↩︎\nFleischmann, M. & Arribas-Bel, D., 2022. Geographical characterisation of British urban form using the spatial signatures framework. Scientific Data, 9(1), p.546.↩︎\nBastani, F. et al., 2023. Satlaspretrain: A large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16772-16782.↩︎\nBastani, F. et al., 2023. Satlaspretrain: A large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16772-16782.↩︎\nCorbane, C. et al., 2020. A global cloud-free pixel-based image composite from Sentinel-2 data. Data in brief, 31, p.105737.↩︎\nhttps://huggingface.co/made-with-clay/Clay↩︎\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-100M↩︎"
  }
]